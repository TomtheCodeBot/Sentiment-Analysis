{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"7I-xZzsveUMA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654964888469,"user_tz":-420,"elapsed":3018,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}},"outputId":"64cb9ad4-1647-415d-c6ff-ec9f0f516877"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"S9qzfxSwkTtx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654964898160,"user_tz":-420,"elapsed":9701,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}},"outputId":"a5ea3b77-33c7-4c25-e593-6c4ca1a5118c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: fastNLP in /usr/local/lib/python3.7/dist-packages (0.7.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastNLP) (1.11.0+cu113)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from fastNLP) (2019.12.20)\n","Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.7/dist-packages (from fastNLP) (1.21.6)\n","Requirement already satisfied: prettytable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from fastNLP) (3.3.0)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from fastNLP) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastNLP) (2.23.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable>=0.7.2->fastNLP) (0.2.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable>=0.7.2->fastNLP) (4.11.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->fastNLP) (4.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable>=0.7.2->fastNLP) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastNLP) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastNLP) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastNLP) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastNLP) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (3.4.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (0.0.53)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (3.7.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (21.3)\n","Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (0.9.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (3.17.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (4.64.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (0.1.96)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0) (3.0.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.4.0) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch==1.7.1 (from versions: 0.1.2, 1.0.2)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for pytorch==1.7.1\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: fitlog in /usr/local/lib/python3.7/dist-packages (0.9.13)\n","Requirement already satisfied: gitpython>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from fitlog) (3.1.27)\n","Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from fitlog) (1.1.4)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from fitlog) (1.21.6)\n","Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from fitlog) (0.6.2)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->fitlog) (1.0.1)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->fitlog) (7.1.2)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->fitlog) (1.1.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->fitlog) (2.11.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython>=3.1.2->fitlog) (4.2.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=3.1.2->fitlog) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.2->fitlog) (5.0.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.0.2->fitlog) (2.0.1)\n"]}],"source":["!pip install fastNLP\n","!pip install transformers==3.4.0\n","!pip install pytorch==1.7.1\n","!pip install fitlog"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"G7gO_HpreZuq","executionInfo":{"status":"ok","timestamp":1654958379080,"user_tz":-420,"elapsed":3280,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["import pandas as pd\n","import glob\n","from tqdm import tqdm\n","import re\n","import itertools\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import recall_score,f1_score,precision_score,accuracy_score\n","import torch\n","from torch import nn"]},{"cell_type":"markdown","metadata":{"id":"im-ufjT0g6_L"},"source":["#Laptop-ACOS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oolcZUHk1ClI"},"outputs":[],"source":["def adJusttab(input_file, output_file):\n","    lines_seen = set()  ### holds lines already seen\n","    outfile = open(output_file, \"w\", encoding='latin-1')\n","    for line in open(input_file, \"r\", encoding='latin-1'):\n","        arr = line.split(\"\\t\")\n","        arr[0 : 2] = ['\\t'.join(arr[0 : 2])]\n","        newline =' '.join(arr)\n","        outfile.write(newline)\n","    outfile.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtiXrx492CDt"},"outputs":[],"source":["adJusttab(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_dev.tsv\", \"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_dev_adjusted.tsv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3s5pVxss2eq0"},"outputs":[],"source":["adJusttab(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_train.tsv\", \"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_train_adjusted.tsv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AS9XYEsfDhxi"},"outputs":[],"source":["adJusttab(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_test.tsv\", \"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_test_adjusted.tsv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqZ_evhKg1cu"},"outputs":[],"source":["dev_dataframe = pd.read_csv(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_dev_adjusted.tsv\", sep='\\t',on_bad_lines='skip',names=[\"text\",\"label\"])\n","test_dataframe = pd.read_csv(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_test_adjusted.tsv\", sep='\\t',on_bad_lines='skip',names=[\"text\",\"label\"])\n","train_dataframe = pd.read_csv(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/laptop_quad_train_adjusted.tsv\", sep='\\t',on_bad_lines='skip',names=[\"text\",\"label\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58eOCiIWhNFf"},"outputs":[],"source":["print(train_dataframe)"]},{"cell_type":"markdown","metadata":{"id":"MQwocvOY5EEi"},"source":["#Restaurant-ACOS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_aHU5ae5J_f"},"outputs":[],"source":["adJusttab(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_dev.tsv\", \"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_dev_adjusted.tsv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygAW5Zyb5J_g"},"outputs":[],"source":["adJusttab(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_test.tsv\", \"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_test_adjusted.tsv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6t1c8mPI5J_g"},"outputs":[],"source":["adJusttab(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_train.tsv\", \"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_train_adjusted.tsv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUvv-oG15J_g"},"outputs":[],"source":["dev_dataframe = pd.read_csv(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_dev_adjusted.tsv\", sep='\\t',names=[\"text\",\"label\"])\n","test_dataframe = pd.read_csv(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_test_adjusted.tsv\", sep='\\t',names=[\"text\",\"label\"])\n","train_dataframe = pd.read_csv(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_train_adjusted.tsv\", sep='\\t',names=[\"text\",\"label\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1649953802816,"user":{"displayName":"Duy Hoang","userId":"17457647490907439684"},"user_tz":-420},"id":"p07kqShB5J_h","outputId":"bb99f458-5297-483f-b826-a5525db1df28"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                   text  \\\n","0     judging from previous posts this used to be a ...   \n","1     we , there were four of us , arrived at noon -...   \n","2     they never brought us complimentary noodles , ...   \n","3     the food was lousy - too sweet or too salty an...   \n","4     after all that , they complained to me about t...   \n","...                                                 ...   \n","1525  i ca n ' t believe that it was , but please pu...   \n","1526  the waitress came to check in on us every few ...   \n","1527  i could n ' t ignore the fact that she reach o...   \n","1528  she then put the check down without asking if ...   \n","1529  i wish i could like this place more , and i wi...   \n","\n","                                                  label  \n","0                      10,11 RESTAURANT#GENERAL 0 13,16  \n","1                         19,20 SERVICE#GENERAL 0 31,32  \n","2                         -1,-1 SERVICE#GENERAL 0 -1,-1  \n","3     1,2 FOOD#QUALITY 0 3,4 1,2 FOOD#QUALITY 0 5,7 ...  \n","4                           -1,-1 SERVICE#GENERAL 0 5,6  \n","...                                                 ...  \n","1525                      -1,-1 SERVICE#GENERAL 0 -1,-1  \n","1526                        1,2 SERVICE#GENERAL 0 -1,-1  \n","1527                      -1,-1 SERVICE#GENERAL 0 -1,-1  \n","1528                      -1,-1 SERVICE#GENERAL 0 -1,-1  \n","1529  6,7 RESTAURANT#GENERAL 0 -1,-1 16,17 SERVICE#G...  \n","\n","[1530 rows x 2 columns]\n"]}],"source":["print(train_dataframe)"]},{"cell_type":"markdown","metadata":{"id":"5aRhKFHQ9yXY"},"source":["#Convert ACOS data to JSON"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FK-XtR_rMsKQ"},"outputs":[],"source":["def labelConversion(label):\n","    ret = []\n","    lstlabel = label.split(\" \")\n","    for i in range(0,len(lstlabel)):\n","        if i%4==0 or i%4==3:\n","            ret.extend(lstlabel[i].split(\",\"))\n","        else:\n","            ret.append(lstlabel[i])\n","    return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrVgonN493EM"},"outputs":[],"source":["\"\"\"words: List[str]\n","        aspects: [{\n","            'index': int\n","            'from': int\n","            'to': int\n","            'polarity': str\n","            'cat_obj':str\n","            'category':str\n","            'term': List[str]\n","        }],\n","        opinions: [{\n","            'index': int\n","            'from': int\n","            'to': int\n","            'term': List[str]\n","        }]\"\"\"\n","import json\n","def fromACOSToJSON(path):\n","    lst = []\n","    lst_aspect = []\n","    lst_opinion = []\n","    dict_templat = {}\n","    dict_template_aspect = {}\n","    dict_template_opinion = {}\n","    polarity = [\"NEG\",\"NEU\",\"POS\"]\n","    dev_dataframe = pd.read_csv(path, sep='\\t',on_bad_lines='skip',names=[\"text\",\"label\"])\n","    for i in range(0,len(dev_dataframe)):\n","        word = \"<<null>> \"+dev_dataframe.iloc[i][\"text\"]\n","        word.translate({\"/' \":\"'\"})\n","        dict_templat[\"raw_words\"] = word\n","        dict_templat[\"words\"] = word.split(\" \")\n","        labels = labelConversion(dev_dataframe.iloc[i][\"label\"])\n","        amount = int(len(labels)/6)\n","        for i in range(amount):\n","            dict_template_aspect[\"index\"] = i\n","            dict_template_opinion[\"index\"] = i\n","            pos = i*6\n","            if int(labels[pos])<0:\n","                dict_template_aspect[\"from\"] = 0\n","                dict_template_aspect[\"to\"] = 1\n","            else:\n","                dict_template_aspect[\"from\"] = int(labels[pos])+1\n","                dict_template_aspect[\"to\"] = int(labels[pos+1])+1\n","            cat1,cat2 =  labels[pos+2].split(\"#\")\n","            dict_template_aspect[\"cat_obj\"] = cat1\n","            dict_template_aspect[\"category\"] = cat2\n","            dict_template_aspect[\"term\"] = dict_templat[\"words\"][dict_template_aspect[\"from\"]:dict_template_aspect[\"to\"]] \n","            dict_template_aspect[\"polarity\"] = polarity[int(labels[pos+3])]\n","            if int(labels[pos+4])<0:\n","                dict_template_opinion[\"from\"] = 0\n","                dict_template_opinion[\"to\"] = 1\n","            else:\n","                dict_template_opinion[\"from\"] = int(labels[pos+4])+1\n","                dict_template_opinion[\"to\"] = int(labels[pos+5])+1\n","            dict_template_opinion[\"term\"] = dict_templat[\"words\"][dict_template_opinion[\"from\"]:dict_template_opinion[\"to\"]]\n","            lst_aspect.append(dict_template_aspect.copy())\n","            lst_opinion.append(dict_template_opinion.copy())\n","            dict_template_aspect={}\n","            dict_template_opinion={}\n","        dict_templat[\"aspects\"]=lst_aspect.copy()\n","        dict_templat[\"opinions\"]=lst_opinion.copy()\n","        lst_aspect = []\n","        lst_opinion = []\n","        lst.append(dict_templat.copy())\n","        dict_templat={}\n","    return json.dumps(lst)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFYiPtnTZ9Lj"},"outputs":[],"source":["json_object = fromACOSToJSON(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_dev_adjusted.tsv\")\n","with open(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/json/dev_convert.json\", \"w\") as outfile:\n","    outfile.write(json_object)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPU_Cj_Fuji8"},"outputs":[],"source":["json_object =fromACOSToJSON(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_test_adjusted.tsv\")\n","with open(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/json/test_convert.json\", \"w\") as outfile:\n","    outfile.write(json_object)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FIKkqWquj7N"},"outputs":[],"source":["json_object =fromACOSToJSON(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/rest16_quad_train_adjusted.tsv\")\n","with open(\"/content/drive/MyDrive/Sentiment Analysis/data/Restaurant-ACOS/json/train_convert.json\", \"w\") as outfile:\n","    outfile.write(json_object)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"auaXEPadCkzQ"},"outputs":[],"source":["l = ['MULTIMEDIA_DEVICES#PRICE', 'OS#QUALITY', 'SHIPPING#QUALITY', 'GRAPHICS#OPERATION_PERFORMANCE', 'CPU#OPERATION_PERFORMANCE', \n","        'COMPANY#DESIGN_FEATURES', 'MEMORY#OPERATION_PERFORMANCE', 'SHIPPING#PRICE', 'POWER_SUPPLY#CONNECTIVITY', 'SOFTWARE#USABILITY', \n","        'FANS&COOLING#GENERAL', 'GRAPHICS#DESIGN_FEATURES', 'BATTERY#GENERAL', 'HARD_DISC#USABILITY', 'FANS&COOLING#DESIGN_FEATURES', \n","        'MEMORY#DESIGN_FEATURES', 'MOUSE#USABILITY', 'CPU#GENERAL', 'LAPTOP#QUALITY', 'POWER_SUPPLY#GENERAL', 'PORTS#QUALITY', \n","        'KEYBOARD#PORTABILITY', 'SUPPORT#DESIGN_FEATURES', 'MULTIMEDIA_DEVICES#USABILITY', 'MOUSE#GENERAL', 'KEYBOARD#MISCELLANEOUS', \n","        'MULTIMEDIA_DEVICES#DESIGN_FEATURES', 'OS#MISCELLANEOUS', 'LAPTOP#MISCELLANEOUS', 'SOFTWARE#PRICE', 'FANS&COOLING#OPERATION_PERFORMANCE', \n","        'MEMORY#QUALITY', 'OPTICAL_DRIVES#OPERATION_PERFORMANCE', 'HARD_DISC#GENERAL', 'MEMORY#GENERAL', 'DISPLAY#OPERATION_PERFORMANCE', \n","        'MULTIMEDIA_DEVICES#GENERAL', 'LAPTOP#GENERAL', 'MOTHERBOARD#QUALITY', 'LAPTOP#PORTABILITY', 'KEYBOARD#PRICE', 'SUPPORT#OPERATION_PERFORMANCE', \n","        'GRAPHICS#GENERAL', 'MOTHERBOARD#OPERATION_PERFORMANCE', 'DISPLAY#GENERAL', 'BATTERY#QUALITY', 'LAPTOP#USABILITY', 'LAPTOP#DESIGN_FEATURES', \n","        'PORTS#CONNECTIVITY', 'HARDWARE#QUALITY', 'SUPPORT#GENERAL', 'MOTHERBOARD#GENERAL', 'PORTS#USABILITY', 'KEYBOARD#QUALITY', 'GRAPHICS#USABILITY', \n","        'HARD_DISC#PRICE', 'OPTICAL_DRIVES#USABILITY', 'MULTIMEDIA_DEVICES#CONNECTIVITY', 'HARDWARE#DESIGN_FEATURES', 'MEMORY#USABILITY', \n","        'SHIPPING#GENERAL', 'CPU#PRICE', 'Out_Of_Scope#DESIGN_FEATURES', 'MULTIMEDIA_DEVICES#QUALITY', 'OS#PRICE', 'SUPPORT#QUALITY', \n","        'OPTICAL_DRIVES#GENERAL', 'HARDWARE#USABILITY', 'DISPLAY#DESIGN_FEATURES', 'PORTS#GENERAL', 'COMPANY#OPERATION_PERFORMANCE', \n","        'COMPANY#GENERAL', 'Out_Of_Scope#GENERAL', 'KEYBOARD#DESIGN_FEATURES', 'Out_Of_Scope#OPERATION_PERFORMANCE', \n","        'OPTICAL_DRIVES#DESIGN_FEATURES', 'LAPTOP#OPERATION_PERFORMANCE', 'KEYBOARD#USABILITY', 'DISPLAY#USABILITY', 'POWER_SUPPLY#QUALITY', \n","        'HARD_DISC#DESIGN_FEATURES', 'DISPLAY#QUALITY', 'MOUSE#DESIGN_FEATURES', 'COMPANY#QUALITY', 'HARDWARE#GENERAL', 'COMPANY#PRICE', \n","        'MULTIMEDIA_DEVICES#OPERATION_PERFORMANCE', 'KEYBOARD#OPERATION_PERFORMANCE', 'SOFTWARE#PORTABILITY', 'HARD_DISC#OPERATION_PERFORMANCE', \n","        'BATTERY#DESIGN_FEATURES', 'CPU#QUALITY', 'WARRANTY#GENERAL', 'OS#DESIGN_FEATURES', 'OS#OPERATION_PERFORMANCE', 'OS#USABILITY', \n","        'SOFTWARE#GENERAL', 'SUPPORT#PRICE', 'SHIPPING#OPERATION_PERFORMANCE', 'DISPLAY#PRICE', 'LAPTOP#PRICE', 'OS#GENERAL', 'HARDWARE#PRICE', \n","        'SOFTWARE#DESIGN_FEATURES', 'HARD_DISC#MISCELLANEOUS', 'PORTS#PORTABILITY', 'FANS&COOLING#QUALITY', 'BATTERY#OPERATION_PERFORMANCE', \n","        'CPU#DESIGN_FEATURES', 'PORTS#OPERATION_PERFORMANCE', 'SOFTWARE#OPERATION_PERFORMANCE', 'KEYBOARD#GENERAL', 'SOFTWARE#QUALITY', \n","        'LAPTOP#CONNECTIVITY', 'POWER_SUPPLY#DESIGN_FEATURES', 'HARDWARE#OPERATION_PERFORMANCE', 'WARRANTY#QUALITY', 'HARD_DISC#QUALITY', \n","        'POWER_SUPPLY#OPERATION_PERFORMANCE', 'PORTS#DESIGN_FEATURES', 'Out_Of_Scope#USABILITY']"]},{"cell_type":"code","source":["laptop_obj =['<<MULTIMEDIA_DEVICES>>', '<<OS>>', '<<SHIPPING>>', '<<GRAPHICS>>', '<<CPU>>', '<<COMPANY>>', '<<MEMORY>>', '<<POWER_SUPPLY>>', \n","             '<<SOFTWARE>>', '<<FANS&COOLING>>', '<<BATTERY>>', '<<HARD_DISC>>', '<<MOUSE>>', '<<LAPTOP>>', '<<PORTS>>', '<<KEYBOARD>>', \n","             '<<SUPPORT>>', '<<OPTICAL_DRIVES>>', '<<DISPLAY>>', '<<MOTHERBOARD>>', '<<HARDWARE>>', '<<Out_Of_Scope>>', '<<WARRANTY>>']\n","laptop_catergory =['<<PRICE>>', '<<QUALITY>>', '<<OPERATION_PERFORMANCE>>', '<<DESIGN_FEATURES>>', '<<CONNECTIVITY>>', '<<USABILITY>>', \n","                   '<<GENERAL>>', '<<PORTABILITY>>', '<<MISCELLANEOUS>>']\n","                   \n","restaurant_obj =['<<RESTAURANT>>', '<<SERVICE>>', '<<FOOD>>', '<<DRINKS>>', '<<AMBIENCE>>', '<<LOCATION>>']\n","restaurant_catergory =['<<GENERAL>>', '<<QUALITY>>', '<<STYLE_OPTIONS>>', '<<PRICES>>', '<<MISCELLANEOUS>>']"],"metadata":{"id":"LZMP-zDdMxwZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ur1HXPDMclxI","executionInfo":{"status":"ok","timestamp":1654964900800,"user_tz":-420,"elapsed":351,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["laptop_obj =['<<MULTIMEDIA_DEVICES>>', '<<OS>>', '<<SHIPPING>>', '<<GRAPHICS>>', '<<CPU>>', '<<COMPANY>>', '<<MEMORY>>', '<<POWER_SUPPLY>>', \n","             '<<SOFTWARE>>', '<<FANS&COOLING>>', '<<BATTERY>>', '<<HARD_DISC>>', '<<MOUSE>>', '<<LAPTOP>>', '<<PORTS>>', '<<KEYBOARD>>', \n","             '<<SUPPORT>>', '<<OPTICAL_DRIVES>>', '<<DISPLAY>>', '<<MOTHERBOARD>>', '<<HARDWARE>>', '<<Out_Of_Scope>>', '<<WARRANTY>>']\n","laptop_catergory =['<<PRICE>>', '<<QUALITY>>', '<<OPERATION_PERFORMANCE>>', '<<DESIGN_FEATURES>>', '<<CONNECTIVITY>>', '<<USABILITY>>', \n","                   '<<GENERAL>>', '<<PORTABILITY>>', '<<MISCELLANEOUS>>']\n","laptop_obj_key = ['MULTIMEDIA_DEVICES', 'OS', 'SHIPPING', 'GRAPHICS', 'CPU', 'COMPANY', 'MEMORY', 'POWER_SUPPLY', 'SOFTWARE', 'FANS&COOLING', 'BATTERY', 'HARD_DISC', 'MOUSE', 'LAPTOP', 'PORTS', 'KEYBOARD', 'SUPPORT', 'OPTICAL_DRIVES', 'DISPLAY', 'MOTHERBOARD', 'HARDWARE', 'Out_Of_Scope', 'WARRANTY']\n","laptop_catergory_key = ['PRICE', 'QUALITY', 'OPERATION_PERFORMANCE', 'DESIGN_FEATURES', 'CONNECTIVITY', 'USABILITY', 'GENERAL', 'PORTABILITY', 'MISCELLANEOUS']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwhxyH02Du6E"},"outputs":[],"source":["l = ['RESTAURANT#GENERAL', 'SERVICE#GENERAL', 'FOOD#GENERAL', 'FOOD#QUALITY', 'FOOD#STYLE_OPTIONS', 'DRINKS#STYLE_OPTIONS', 'DRINKS#PRICES', \n","        'AMBIENCE#GENERAL', 'RESTAURANT#PRICES', 'FOOD#PRICES', 'RESTAURANT#MISCELLANEOUS', 'DRINKS#QUALITY', 'LOCATION#GENERAL']"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bBpp0tCii4N0","executionInfo":{"status":"ok","timestamp":1654964902262,"user_tz":-420,"elapsed":5,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["restaurant_obj =['<<RESTAURANT>>', '<<SERVICE>>', '<<FOOD>>', '<<DRINKS>>', '<<AMBIENCE>>', '<<LOCATION>>']\n","restaurant_catergory =['<<GENERAL>>', '<<QUALITY>>', '<<STYLE_OPTIONS>>', '<<PRICES>>', '<<MISCELLANEOUS>>']\n","restaurant_obj_key =['RESTAURANT', 'SERVICE', 'FOOD', 'DRINKS', 'AMBIENCE', 'LOCATION']\n","restaurant_catergory_key =['GENERAL', 'QUALITY', 'STYLE_OPTIONS', 'PRICES', 'MISCELLANEOUS']\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"0lmWnXtFtYud","executionInfo":{"status":"ok","timestamp":1654964903373,"user_tz":-420,"elapsed":3,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["restaurant_obj_dict = dict(zip(restaurant_obj_key, restaurant_obj))\n","restaurant_catergory_dict = dict(zip(restaurant_catergory_key, restaurant_catergory))\n","laptop_obj_dict = dict(zip(laptop_obj_key, laptop_obj))\n","laptop_catergory_dict = dict(zip(laptop_catergory_key, laptop_catergory))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjggXwnfK7nj"},"outputs":[],"source":["obj =[]\n","aspect= []\n","for i in l:\n","    check = i.split(\"#\")\n","    if check[0] not in obj:\n","        obj.append(check[0]) \n","    if check[1] not in aspect:\n","        aspect.append(check[1]) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1649580246804,"user":{"displayName":"Duy Hoang","userId":"17457647490907439684"},"user_tz":-420},"id":"xmvfAlf-Dl47","outputId":"8f8999d3-a830-4f33-9741-c678419b9786"},"outputs":[{"name":"stdout","output_type":"stream","text":["['MULTIMEDIA_DEVICES', 'OS', 'SHIPPING', 'GRAPHICS', 'CPU', 'COMPANY', 'MEMORY', 'POWER_SUPPLY', 'SOFTWARE', 'FANS&COOLING', 'BATTERY', 'HARD_DISC', 'MOUSE', 'LAPTOP', 'PORTS', 'KEYBOARD', 'SUPPORT', 'OPTICAL_DRIVES', 'DISPLAY', 'MOTHERBOARD', 'HARDWARE', 'Out_Of_Scope', 'WARRANTY']\n","['PRICE', 'QUALITY', 'OPERATION_PERFORMANCE', 'DESIGN_FEATURES', 'CONNECTIVITY', 'USABILITY', 'GENERAL', 'PORTABILITY', 'MISCELLANEOUS']\n"]}],"source":["print(obj)\n","print(aspect)"]},{"cell_type":"markdown","metadata":{"id":"BWeNjY13jF5C"},"source":["#Implicit/Explicit Test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbIobibajFQF"},"outputs":[],"source":["import json\n","def IEPartitioner(path):\n","    base = json.loads(open(path, \"r\").read())\n","    IAIO = []\n","    IAEO = []\n","    EAIO = []\n","    EAEO = []\n","    IAIO_template = {}\n","    IAEO_template = {}\n","    EAIO_template = {}\n","    EAEO_template = {}\n","    for i in base:\n","        IAIO_template = i.copy()\n","        IAEO_template = i.copy()\n","        EAIO_template = i.copy()\n","        EAEO_template = i.copy()\n","        IAIO_template[\"aspects\"],IAIO_template[\"opinions\"] = [],[]\n","        IAEO_template[\"aspects\"],IAEO_template[\"opinions\"] = [],[]\n","        EAIO_template[\"aspects\"],EAIO_template[\"opinions\"] = [],[]\n","        EAEO_template[\"aspects\"],EAEO_template[\"opinions\"] = [],[]\n","        for k in range(0,len(i[\"aspects\"])):\n","            if i[\"aspects\"][k][\"from\"]==0:\n","                if i[\"opinions\"][k][\"from\"]==0:\n","                    IAIO_template[\"aspects\"].append(i[\"aspects\"][k])\n","                    IAIO_template[\"opinions\"].append(i[\"opinions\"][k])\n","                else:\n","                    IAEO_template[\"aspects\"].append(i[\"aspects\"][k])\n","                    IAEO_template[\"opinions\"].append(i[\"opinions\"][k])\n","            else:\n","                if i[\"opinions\"][k][\"from\"]==0:\n","                    EAIO_template[\"aspects\"].append(i[\"aspects\"][k])\n","                    EAIO_template[\"opinions\"].append(i[\"opinions\"][k])\n","                else:\n","                    EAEO_template[\"aspects\"].append(i[\"aspects\"][k])\n","                    EAEO_template[\"opinions\"].append(i[\"opinions\"][k])\n","        if(len(IAIO_template[\"aspects\"])>0):\n","            IAIO.append(IAIO_template.copy())\n","        if(len(IAEO_template[\"aspects\"])>0):\n","            IAEO.append(IAEO_template.copy())\n","        if(len(EAIO_template[\"aspects\"])>0):\n","            EAIO.append(EAIO_template.copy())\n","        if(len(EAEO_template[\"aspects\"])>0):\n","            EAEO.append(EAEO_template.copy())  \n","        IAIO_template = {}\n","        IAEO_template = {}\n","        EAIO_template = {}\n","        EAEO_template = {}\n","    return json.dumps(IAIO),json.dumps(IAEO),json.dumps(EAIO),json.dumps(EAEO)\n","IAIO,IAEO,EAIO,EAEO = IEPartitioner(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/json/test_convert.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnA5KZ7asLxD"},"outputs":[],"source":["with open(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/json/Implicit Explicit/IAIO.json\", \"w\") as outfile:\n","    outfile.write(IAIO)\n","with open(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/json/Implicit Explicit/IAEO.json\", \"w\") as outfile:\n","    outfile.write(IAEO)\n","with open(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/json/Implicit Explicit/EAIO.json\", \"w\") as outfile:\n","    outfile.write(EAIO)\n","with open(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/json/Implicit Explicit/EAEO.json\", \"w\") as outfile:\n","    outfile.write(EAEO)"]},{"cell_type":"markdown","metadata":{"id":"dCtAWimziVXR"},"source":["# Data pipeline"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Z3rcZPOhhJKB","executionInfo":{"status":"ok","timestamp":1654964922988,"user_tz":-420,"elapsed":14781,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["from fastNLP.io import Pipe, DataBundle, Loader\n","import os\n","import json\n","from fastNLP import DataSet, Instance\n","from transformers import AutoTokenizer\n","import numpy as np\n","from itertools import chain\n","from functools import cmp_to_key\n","\n","\n","def cmp_aspect(v1, v2):\n","    if v1[0]['from']==v2[0]['from']:\n","        return v1[1]['from'] - v2[1]['from']\n","    return v1[0]['from'] - v2[0]['from']\n","\n","def cmp_opinion(v1, v2):\n","    if v1[1]['from']==v2[1]['from']:\n","        return v1[0]['from'] - v2[0]['from']\n","    return v1[1]['from'] - v2[1]['from']\n","\n","\n","class T5BPEABSAPipe(Pipe):\n","    def __init__(self, tokenizer='facebook/bart-base', opinion_first=False):\n","        super(T5BPEABSAPipe, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n","        self.tokenizer.add_tokens(\"<<null>>\")\n","        self.mapping = laptop_obj_dict\n","        self.mapping.update(laptop_catergory_dict)\n","        self.mapping.update({  # so that the label word can be initialized in a better embedding.\n","            'POS': '<<positive>>',\n","            'NEG': '<<negative>>',\n","            'NEU': '<<neutral>>'\n","        })\n","        self.opinion_first = opinion_first  # 是否先生成opinion\n","        cur_num_tokens = self.tokenizer.vocab_size\n","        self.cur_num_token = cur_num_tokens\n","  \n","        tokens_to_add = sorted(list(self.mapping.values()), key=lambda x:len(x), reverse=True)\n","        unique_no_split_tokens = self.tokenizer.unique_no_split_tokens\n","        sorted_add_tokens = sorted(list(tokens_to_add), key=lambda x:len(x), reverse=True)\n","        for tok in sorted_add_tokens:\n","            assert self.tokenizer.convert_tokens_to_ids([tok])[0]==self.tokenizer.unk_token_id\n","        self.tokenizer.unique_no_split_tokens = unique_no_split_tokens + sorted_add_tokens\n","        self.tokenizer.add_tokens(sorted_add_tokens)\n","        \n","        self.mapping2id = {}\n","        self.mapping2targetid = {}\n","\n","        for key, value in self.mapping.items():\n","            key_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(value))\n","            assert len(key_id) == 1, value\n","            assert key_id[0] >= cur_num_tokens\n","            self.mapping2id[key] = key_id[0]\n","            self.mapping2targetid[key] = len(self.mapping2targetid)\n","\n","    def process(self, data_bundle: DataBundle) -> DataBundle:\n","        \"\"\"\n","        words: List[str]\n","        aspects: [{\n","            'index': int\n","            'from': int\n","            'to': int\n","            'polarity': str\n","            'cat_obj':str\n","            'category':str\n","            'term': List[str]\n","        }],\n","        opinions: [{\n","            'index': int\n","            'from': int\n","            'to': int\n","            'term': List[str]\n","        }]\n","        输出为[o_s, o_e, a_s, a_e, c]或者[a_s, a_e, o_s, o_e, c]\n","        :param data_bundle:\n","        :return:\n","        \"\"\"\n","        target_shift = len(self.mapping) + 2  # 是由于第一位是sos，紧接着是eos, 然后是\n","\n","        def prepare_target(ins):\n","            raw_words = ins['raw_words']\n","            word_bpes = [[self.tokenizer.pad_token_id]]\n","            for word in raw_words:\n","                bpes = self.tokenizer.tokenize(word)\n","                bpes = self.tokenizer.convert_tokens_to_ids(bpes)\n","                word_bpes.append(bpes)\n","            word_bpes.append([self.tokenizer.eos_token_id])\n","\n","            lens = list(map(len, word_bpes))\n","            cum_lens = np.cumsum(list(lens)).tolist()\n","            target = [0]  # 特殊的开始\n","            target_spans = []\n","            _word_bpes = list(chain(*word_bpes))\n","            aspects_opinions = [(a, o) for a, o in zip(ins['aspects'], ins['opinions'])]\n","            if self.opinion_first:\n","                aspects_opinions = sorted(aspects_opinions, key=cmp_to_key(cmp_opinion))\n","            else:\n","                aspects_opinions = sorted(aspects_opinions, key=cmp_to_key(cmp_aspect))\n","\n","            for aspects, opinions in aspects_opinions:  # 预测bpe的start\n","                assert aspects['index'] == opinions['index']\n","                a_start_bpe = cum_lens[aspects['from']]  # 因为有一个sos shift\n","                a_end_bpe = cum_lens[aspects['to']-1]  # 这里由于之前是开区间，刚好取到最后一个word的开头\n","                o_start_bpe = cum_lens[opinions['from']]  # 因为有一个sos shift\n","                o_end_bpe = cum_lens[opinions['to']-1]  # 因为有一个sos shift\n","                # 这里需要evaluate是否是对齐的\n","                for idx, word in zip((o_start_bpe, o_end_bpe, a_start_bpe, a_end_bpe),\n","                                     (opinions['term'][0], opinions['term'][-1], aspects['term'][0], aspects['term'][-1])):\n","                    assert _word_bpes[idx] == self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(word)[:1])[0] or \\\n","                           _word_bpes[idx] == self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(word)[-1:])[0]\n","\n","                if self.opinion_first:\n","                    target_spans.append([o_start_bpe+target_shift, o_end_bpe+target_shift,\n","                                         a_start_bpe+target_shift, a_end_bpe+target_shift,\n","                                         self.mapping2targetid[aspects['cat_obj']]+2,\n","                                         self.mapping2targetid[aspects['category']]+2])\n","                else:\n","                    target_spans.append([a_start_bpe+target_shift, a_end_bpe+target_shift,\n","                                         self.mapping2targetid[aspects['cat_obj']]+2,\n","                                         self.mapping2targetid[aspects['category']]+2,\n","                                         o_start_bpe+target_shift, o_end_bpe+target_shift])\n","                target_spans[-1].append(self.mapping2targetid[aspects['polarity']]+2)   # 前面有sos和eos\n","                target_spans[-1] = tuple(target_spans[-1])\n","            target.extend(list(chain(*target_spans)))\n","            target.append(1)  # append 1是由于特殊的eos\n","            return {'tgt_tokens': target, 'target_span': target_spans, 'src_tokens': list(chain(*word_bpes))}\n","\n","        data_bundle.apply_more(prepare_target, use_tqdm=True, tqdm_desc='Pre. tgt.')\n","\n","        data_bundle.set_ignore_type('target_span')\n","        data_bundle.set_pad_val('tgt_tokens', 1)  # 设置为eos所在的id\n","        data_bundle.set_pad_val('src_tokens', self.tokenizer.pad_token_id)\n","\n","        data_bundle.apply_field(lambda x: len(x), field_name='src_tokens', new_field_name='src_seq_len')\n","        data_bundle.apply_field(lambda x: len(x), field_name='tgt_tokens', new_field_name='tgt_seq_len')\n","        data_bundle.set_input('tgt_tokens', 'src_tokens', 'src_seq_len', 'tgt_seq_len')\n","        data_bundle.set_target('tgt_tokens', 'tgt_seq_len', 'target_span')\n","\n","        return data_bundle\n","\n","    def process_from_file(self, paths, demo=False) -> DataBundle:\n","        \"\"\"\n","        :param paths: 支持路径类型参见 :class:`fastNLP.io.loader.ConllLoader` 的load函数。\n","        :return: DataBundle\n","        \"\"\"\n","        # 读取数据\n","        data_bundle = ABSALoader(demo=demo).load(paths)\n","        data_bundle = self.process(data_bundle)\n","\n","        return data_bundle\n","\n","class BartBPEABSAPipe(Pipe):\n","    def __init__(self, tokenizer='facebook/bart-base', opinion_first=False):\n","        super(BartBPEABSAPipe, self).__init__()\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n","        self.tokenizer.add_tokens(\"<<null>>\")\n","        self.mapping = laptop_obj_dict\n","        self.mapping.update(laptop_catergory_dict)\n","        self.mapping.update({  # so that the label word can be initialized in a better embedding.\n","            'POS': '<<positive>>',\n","            'NEG': '<<negative>>',\n","            'NEU': '<<neutral>>'\n","        })\n","        self.opinion_first = opinion_first  # 是否先生成opinion\n","        cur_num_tokens = self.tokenizer.vocab_size\n","        self.cur_num_token = cur_num_tokens\n","  \n","        tokens_to_add = sorted(list(self.mapping.values()), key=lambda x:len(x), reverse=True)\n","        unique_no_split_tokens = self.tokenizer.unique_no_split_tokens\n","        sorted_add_tokens = sorted(list(tokens_to_add), key=lambda x:len(x), reverse=True)\n","        for tok in sorted_add_tokens:\n","            assert self.tokenizer.convert_tokens_to_ids([tok])[0]==self.tokenizer.unk_token_id\n","        self.tokenizer.unique_no_split_tokens = unique_no_split_tokens + sorted_add_tokens\n","        self.tokenizer.add_tokens(sorted_add_tokens)\n","        \n","        self.mapping2id = {}\n","        self.mapping2targetid = {}\n","\n","        for key, value in self.mapping.items():\n","            key_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(value))\n","            assert len(key_id) == 1, value\n","            assert key_id[0] >= cur_num_tokens\n","            self.mapping2id[key] = key_id[0]\n","            self.mapping2targetid[key] = len(self.mapping2targetid)\n","\n","    def process(self, data_bundle: DataBundle) -> DataBundle:\n","        \"\"\"\n","        words: List[str]\n","        aspects: [{\n","            'index': int\n","            'from': int\n","            'to': int\n","            'polarity': str\n","            'cat_obj':str\n","            'category':str\n","            'term': List[str]\n","        }],\n","        opinions: [{\n","            'index': int\n","            'from': int\n","            'to': int\n","            'term': List[str]\n","        }]\n","        输出为[o_s, o_e, a_s, a_e, c]或者[a_s, a_e, o_s, o_e, c]\n","        :param data_bundle:\n","        :return:\n","        \"\"\"\n","        target_shift = len(self.mapping) + 2  # 是由于第一位是sos，紧接着是eos, 然后是\n","\n","        def prepare_target(ins):\n","            raw_words = ins['raw_words']\n","            word_bpes = [[self.tokenizer.bos_token_id]]\n","            for word in raw_words:\n","                bpes = self.tokenizer.tokenize(word, add_prefix_space=True)\n","                bpes = self.tokenizer.convert_tokens_to_ids(bpes)\n","                word_bpes.append(bpes)\n","            word_bpes.append([self.tokenizer.eos_token_id])\n","\n","            lens = list(map(len, word_bpes))\n","            cum_lens = np.cumsum(list(lens)).tolist()\n","            target = [0]  # 特殊的开始\n","            target_spans = []\n","            _word_bpes = list(chain(*word_bpes))\n","            aspects_opinions = [(a, o) for a, o in zip(ins['aspects'], ins['opinions'])]\n","            if self.opinion_first:\n","                aspects_opinions = sorted(aspects_opinions, key=cmp_to_key(cmp_opinion))\n","            else:\n","                aspects_opinions = sorted(aspects_opinions, key=cmp_to_key(cmp_aspect))\n","\n","            for aspects, opinions in aspects_opinions:  # 预测bpe的start\n","                assert aspects['index'] == opinions['index']\n","                a_start_bpe = cum_lens[aspects['from']]  # 因为有一个sos shift\n","                a_end_bpe = cum_lens[aspects['to']-1]  # 这里由于之前是开区间，刚好取到最后一个word的开头\n","                o_start_bpe = cum_lens[opinions['from']]  # 因为有一个sos shift\n","                o_end_bpe = cum_lens[opinions['to']-1]  # 因为有一个sos shift\n","                # 这里需要evaluate是否是对齐的\n","                for idx, word in zip((o_start_bpe, o_end_bpe, a_start_bpe, a_end_bpe),\n","                                     (opinions['term'][0], opinions['term'][-1], aspects['term'][0], aspects['term'][-1])):\n","                    assert _word_bpes[idx] == self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(word, add_prefix_space=True)[:1])[0] or \\\n","                           _word_bpes[idx] == self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(word, add_prefix_space=True)[-1:])[0]\n","\n","                if self.opinion_first:\n","                    target_spans.append([o_start_bpe+target_shift, o_end_bpe+target_shift,\n","                                         a_start_bpe+target_shift, a_end_bpe+target_shift,\n","                                         self.mapping2targetid[aspects['cat_obj']]+2,\n","                                         self.mapping2targetid[aspects['category']]+2])\n","                else:\n","                    target_spans.append([a_start_bpe+target_shift, a_end_bpe+target_shift,\n","                                         self.mapping2targetid[aspects['cat_obj']]+2,\n","                                         self.mapping2targetid[aspects['category']]+2,\n","                                         o_start_bpe+target_shift, o_end_bpe+target_shift])\n","                target_spans[-1].append(self.mapping2targetid[aspects['polarity']]+2)   # 前面有sos和eos\n","                target_spans[-1] = tuple(target_spans[-1])\n","            target.extend(list(chain(*target_spans)))\n","            target.append(1)  # append 1是由于特殊的eos\n","            return {'tgt_tokens': target, 'target_span': target_spans, 'src_tokens': list(chain(*word_bpes))}\n","\n","        data_bundle.apply_more(prepare_target, use_tqdm=True, tqdm_desc='Pre. tgt.')\n","\n","        data_bundle.set_ignore_type('target_span')\n","        data_bundle.set_pad_val('tgt_tokens', 1)  # 设置为eos所在的id\n","        data_bundle.set_pad_val('src_tokens', self.tokenizer.pad_token_id)\n","\n","        data_bundle.apply_field(lambda x: len(x), field_name='src_tokens', new_field_name='src_seq_len')\n","        data_bundle.apply_field(lambda x: len(x), field_name='tgt_tokens', new_field_name='tgt_seq_len')\n","        data_bundle.set_input('tgt_tokens', 'src_tokens', 'src_seq_len', 'tgt_seq_len')\n","        data_bundle.set_target('tgt_tokens', 'tgt_seq_len', 'target_span')\n","\n","        return data_bundle\n","\n","    def process_from_file(self, paths, demo=False) -> DataBundle:\n","        \"\"\"\n","        :param paths: 支持路径类型参见 :class:`fastNLP.io.loader.ConllLoader` 的load函数。\n","        :return: DataBundle\n","        \"\"\"\n","        # 读取数据\n","        data_bundle = ABSALoader(demo=demo).load(paths)\n","        data_bundle = self.process(data_bundle)\n","\n","        return data_bundle\n","\n","\n","class ABSALoader(Loader):\n","    def __init__(self, demo=False):\n","        super().__init__()\n","        self.demo = demo\n","\n","    def _load(self, path):\n","        with open(path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","        ds = DataSet()\n","        for ins in data:\n","            tokens = ins['words']\n","            aspects = ins['aspects']\n","            opinions = ins['opinions']\n","            assert len(aspects)==len(opinions)\n","            ins = Instance(raw_words=tokens, aspects=aspects, opinions=opinions)\n","            ds.append(ins)\n","            if self.demo and len(ds)>30:\n","                break\n","        return ds\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HcuQh7sVUPq5"},"source":["#BART code"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZijvJnk-XLE5","executionInfo":{"status":"ok","timestamp":1654964937122,"user_tz":-420,"elapsed":14146,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["# Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"PyTorch BART model, ported from the fairseq repo.\"\"\"\n","import math\n","import random\n","import warnings\n","from typing import Dict, List, Optional, Tuple\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import Tensor, nn\n","from torch.nn import CrossEntropyLoss\n","\n","from transformers.modeling_bart import *\n","\n","logger = logging.get_logger(__name__)\n","\n","_CONFIG_FOR_DOC = \"BartConfig\"\n","_TOKENIZER_FOR_DOC = \"BartTokenizer\"\n","\n","BART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n","    \"facebook/bart-base\",\n","    \"facebook/bart-large\",\n","    \"facebook/bart-large-mnli\",\n","    \"facebook/bart-large-cnn\",\n","    \"facebook/bart-large-xsum\",\n","    \"facebook/mbart-large-en-ro\",\n","]\n","# This list is incomplete. See all BART models at https://huggingface.co/models?filter=bart\n","\n","\n","BART_START_DOCSTRING = r\"\"\"\n","    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n","    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n","    pruning heads etc.)\n","    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass.\n","    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general\n","    usage and behavior.\n","    Parameters:\n","        config (:class:`~transformers.BartConfig`): Model configuration class with all the parameters of the model.\n","            Initializing with a config file does not load the weights associated with the model, only the configuration.\n","            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n","\"\"\"\n","\n","BART_GENERATION_EXAMPLE = r\"\"\"\n","    Summarization example::\n","        >>> from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n","        >>> # see ``examples/summarization/bart/run_eval.py`` for a longer example\n","        >>> model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","        >>> tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","        >>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n","        >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n","        >>> # Generate Summary\n","        >>> summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n","        >>> print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n","\"\"\"\n","\n","BART_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n","            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n","            it.\n","            Indices can be obtained using :class:`~transformers.BartTokenizer`.\n","            See :meth:`transformers.PreTrainedTokenizer.encode` and\n","            :meth:`transformers.PreTrainedTokenizer.__call__` for details.\n","            `What are input IDs? <../glossary.html#input-ids>`__\n","        attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Mask to avoid performing attention on padding token indices.\n","            Mask values selected in ``[0, 1]``:\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","            `What are attention masks? <../glossary.html#attention-mask>`__\n","        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):\n","            Provide for translation and summarization training. By default, the model will create this tensor by\n","            shifting the :obj:`input_ids` to the right, following the paper.\n","        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`):\n","            Default behavior: generate a tensor that ignores pad tokens in :obj:`decoder_input_ids`. Causal mask will\n","            also be used by default.\n","            If you want to change padding behavior, you should read :func:`modeling_bart._prepare_decoder_inputs` and\n","            modify to your needs. See diagram 1 in `the paper <https://arxiv.org/abs/1910.13461>`__ for more\n","            information on the default strategy.\n","        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`):\n","            Tuple consists of (:obj:`last_hidden_state`, `optional`: :obj:`hidden_states`, `optional`: :obj:`attentions`)\n","            :obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) is a\n","            sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of\n","            the decoder.\n","        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n","            Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.\n","            If :obj:`past_key_values` are used, the user can optionally input only the last\n","            ``decoder_input_ids`` (those that don't have their past key value states given to this model) of shape\n","            :obj:`(batch_size, 1)` instead of all ``decoder_input_ids`` of shape :obj:`(batch_size, sequence_length)`.\n","        use_cache (:obj:`bool`, `optional`):\n","            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n","            decoding (see :obj:`past_key_values`).\n","        output_attentions (:obj:`bool`, `optional`):\n","            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n","            tensors for more detail.\n","        output_hidden_states (:obj:`bool`, `optional`):\n","            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n","            more detail.\n","        return_dict (:obj:`bool`, `optional`):\n","            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n","\"\"\"\n","\n","\n","def invert_mask(attention_mask):\n","    \"\"\"Turns 1->0, 0->1, False->True, True-> False\"\"\"\n","    assert attention_mask.dim() == 2\n","    return attention_mask.eq(0)\n","\n","\n","def _prepare_bart_decoder_inputs(\n","        config, input_ids, decoder_input_ids=None, decoder_padding_mask=None, causal_mask_dtype=torch.float32\n","):\n","    \"\"\"Prepare masks that ignore padding tokens in the decoder and a causal mask for the decoder if\n","    none are provided. This mimics the default behavior in fairseq. To override it pass in masks.\n","    Note: this is not called during generation\n","    \"\"\"\n","    pad_token_id = config.pad_token_id\n","    if decoder_input_ids is None:\n","        decoder_input_ids = shift_tokens_right(input_ids, pad_token_id)\n","    bsz, tgt_len = decoder_input_ids.size()\n","    if decoder_padding_mask is None:\n","        decoder_padding_mask = make_padding_mask(decoder_input_ids, pad_token_id)\n","    else:\n","        decoder_padding_mask = invert_mask(decoder_padding_mask)\n","    if decoder_padding_mask is not None and decoder_padding_mask.shape[1] > 1:\n","        # never mask leading token, even if it is pad\n","        decoder_padding_mask[:, 0] = decoder_padding_mask[:, 1]\n","    tmp = fill_with_neg_inf(torch.zeros(tgt_len, tgt_len))\n","    mask = torch.arange(tmp.size(-1))\n","    tmp.masked_fill_(mask < (mask + 1).view(tmp.size(-1), 1), 0)\n","    causal_mask = tmp.to(dtype=causal_mask_dtype, device=decoder_input_ids.device)\n","    return decoder_input_ids, decoder_padding_mask, causal_mask\n","\n","\n","class PretrainedBartModel(PreTrainedModel):\n","    config_class = BartConfig\n","    base_model_prefix = \"model\"\n","\n","    def _init_weights(self, module):\n","        std = self.config.init_std\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=std)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, SinusoidalPositionalEmbedding):\n","            pass\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=std)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","\n","    @property\n","    def dummy_inputs(self):\n","        pad_token = self.config.pad_token_id\n","        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n","        dummy_inputs = {\n","            \"attention_mask\": input_ids.ne(pad_token),\n","            \"input_ids\": input_ids,\n","        }\n","        return dummy_inputs\n","\n","\n","def _make_linear_from_emb(emb):\n","    vocab_size, emb_size = emb.weight.shape\n","    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n","    lin_layer.weight.data = emb.weight.data\n","    return lin_layer\n","\n","\n","# Helper Functions, mostly for making masks\n","def _check_shapes(shape_1, shape2):\n","    if shape_1 != shape2:\n","        raise AssertionError(\"shape mismatch: {} != {}\".format(shape_1, shape2))\n","\n","\n","def shift_tokens_right(input_ids, pad_token_id):\n","    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n","    prev_output_tokens = input_ids.clone()\n","    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n","    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n","    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n","    return prev_output_tokens\n","\n","\n","def make_padding_mask(input_ids, padding_idx=1):\n","    \"\"\"True for pad tokens\"\"\"\n","    padding_mask = input_ids.eq(padding_idx)\n","    if not padding_mask.any():\n","        padding_mask = None\n","    return padding_mask\n","\n","\n","# Helper Modules\n","\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, config: BartConfig):\n","        super().__init__()\n","        self.embed_dim = config.d_model\n","        self.self_attn = Attention(self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout)\n","        self.normalize_before = config.normalize_before\n","        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n","        self.dropout = config.dropout\n","        self.activation_fn = ACT2FN[config.activation_function]\n","        self.activation_dropout = config.activation_dropout\n","        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n","        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n","        self.final_layer_norm = LayerNorm(self.embed_dim)\n","\n","    def forward(self, x, encoder_padding_mask, output_attentions=False):\n","        \"\"\"\n","        Args:\n","            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n","            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n","                `(batch, src_len)` where padding elements are indicated by ``1``.\n","            for t_tgt, t_src is excluded (or masked out), =0 means it is\n","            included in attention\n","        Returns:\n","            encoded output of shape `(seq_len, batch, embed_dim)`\n","        \"\"\"\n","        residual = x\n","        if self.normalize_before:\n","            x = self.self_attn_layer_norm(x)\n","        x, attn_weights = self.self_attn(\n","            query=x, key=x, key_padding_mask=encoder_padding_mask, output_attentions=output_attentions\n","        )\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = residual + x\n","        if not self.normalize_before:\n","            x = self.self_attn_layer_norm(x)\n","\n","        residual = x\n","        if self.normalize_before:\n","            x = self.final_layer_norm(x)\n","        x = self.activation_fn(self.fc1(x))\n","        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n","        x = self.fc2(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = residual + x\n","        if not self.normalize_before:\n","            x = self.final_layer_norm(x)\n","        if torch.isinf(x).any() or torch.isnan(x).any():\n","            clamp_value = torch.finfo(x.dtype).max - 1000\n","            x = torch.clamp(x, min=-clamp_value, max=clamp_value)\n","        return x, attn_weights\n","\n","\n","class BartEncoder(nn.Module):\n","    \"\"\"\n","    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n","    is a :class:`EncoderLayer`.\n","    Args:\n","        config: BartConfig\n","    \"\"\"\n","\n","    def __init__(self, config: BartConfig, embed_tokens):\n","        super().__init__()\n","\n","        self.dropout = config.dropout\n","        self.layerdrop = config.encoder_layerdrop\n","\n","        embed_dim = embed_tokens.embedding_dim\n","        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n","        self.padding_idx = embed_tokens.padding_idx\n","        self.max_source_positions = config.max_position_embeddings\n","\n","        self.embed_tokens = embed_tokens\n","        if config.static_position_embeddings:\n","            self.embed_positions = SinusoidalPositionalEmbedding(\n","                config.max_position_embeddings, embed_dim, self.padding_idx\n","            )\n","        else:\n","            self.embed_positions = LearnedPositionalEmbedding(\n","                config.max_position_embeddings,\n","                embed_dim,\n","                self.padding_idx,\n","                config.extra_pos_embeddings,\n","            )\n","        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n","        self.layernorm_embedding = LayerNorm(embed_dim) if config.normalize_embedding else nn.Identity()\n","        # mbart has one extra layer_norm\n","        self.layer_norm = LayerNorm(config.d_model) if config.add_final_layer_norm else None\n","\n","    def forward(\n","            self, input_ids, attention_mask=None, output_attentions=False, output_hidden_states=False, return_dict=False\n","    ):\n","        \"\"\"\n","        Args:\n","            input_ids (LongTensor): tokens in the source language of shape\n","                `(batch, src_len)`\n","            attention_mask (torch.LongTensor): indicating which indices are padding tokens.\n","        Returns:\n","            BaseModelOutput or Tuple comprised of:\n","                - **x** (Tensor): the last encoder layer's output of\n","                  shape `(src_len, batch, embed_dim)`\n","                - **encoder_states** (tuple(torch.FloatTensor)): all intermediate\n","                  hidden states of shape `(src_len, batch, embed_dim)`.\n","                  Only populated if *output_hidden_states:* is True.\n","                - **all_attentions** (tuple(torch.FloatTensor)): Attention weights for each layer.\n","                During training might not be of length n_layers because of layer dropout.\n","        \"\"\"\n","        # check attention mask and invert\n","        if attention_mask is not None:\n","            attention_mask = invert_mask(attention_mask)\n","\n","        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n","        embed_pos = self.embed_positions(input_ids)\n","        x = inputs_embeds + embed_pos\n","        x = self.layernorm_embedding(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","\n","        # B x T x C -> T x B x C\n","        x = x.transpose(0, 1)\n","\n","        encoder_states = [] if output_hidden_states else None\n","        all_attentions = () if output_attentions else None\n","        for encoder_layer in self.layers:\n","            if output_hidden_states:\n","                encoder_states.append(x)\n","            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n","            dropout_probability = random.uniform(0, 1)\n","            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n","                attn = None\n","            else:\n","                x, attn = encoder_layer(x, attention_mask, output_attentions=output_attentions)\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + (attn,)\n","\n","        if self.layer_norm:\n","            x = self.layer_norm(x)\n","        if output_hidden_states:\n","            encoder_states.append(x)\n","            # T x B x C -> B x T x C\n","            encoder_states = tuple(hidden_state.transpose(0, 1) for hidden_state in encoder_states)\n","\n","        # T x B x C -> B x T x C\n","        x = x.transpose(0, 1)\n","\n","        if not return_dict:\n","            return tuple(v for v in [x, encoder_states, all_attentions] if v is not None)\n","        return BaseModelOutput(last_hidden_state=x, hidden_states=encoder_states, attentions=all_attentions)\n","\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self, config: BartConfig):\n","        super().__init__()\n","        self.embed_dim = config.d_model\n","\n","        self.self_attn = Attention(\n","            embed_dim=self.embed_dim,\n","            num_heads=config.decoder_attention_heads,\n","            dropout=config.attention_dropout,\n","        )\n","        self.dropout = config.dropout\n","        self.activation_fn = ACT2FN[config.activation_function]\n","        self.activation_dropout = config.activation_dropout\n","        self.normalize_before = config.normalize_before\n","\n","        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n","        self.encoder_attn = Attention(\n","            self.embed_dim,\n","            config.decoder_attention_heads,\n","            dropout=config.attention_dropout,\n","            encoder_decoder_attention=True,\n","        )\n","        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n","        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n","        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n","        self.final_layer_norm = LayerNorm(self.embed_dim)\n","\n","    def forward(\n","            self,\n","            x,\n","            encoder_hidden_states,\n","            encoder_attn_mask=None,\n","            layer_state=None,\n","            causal_mask=None,\n","            decoder_padding_mask=None,\n","            output_attentions=False,\n","    ):\n","        residual = x\n","\n","        if layer_state is None:\n","            layer_state = {}\n","        if self.normalize_before:\n","            x = self.self_attn_layer_norm(x)\n","        # Self Attention\n","\n","        x, self_attn_weights = self.self_attn(\n","            query=x,\n","            key=x,\n","            layer_state=layer_state,  # adds keys to layer state\n","            key_padding_mask=decoder_padding_mask,\n","            attn_mask=causal_mask,\n","            output_attentions=output_attentions,\n","        )\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = residual + x\n","        if not self.normalize_before:\n","            x = self.self_attn_layer_norm(x)\n","\n","        # Cross attention\n","        residual = x\n","        assert self.encoder_attn.cache_key != self.self_attn.cache_key\n","        if self.normalize_before:\n","            x = self.encoder_attn_layer_norm(x)\n","        x, _ = self.encoder_attn(\n","            query=x,\n","            key=encoder_hidden_states,\n","            key_padding_mask=encoder_attn_mask,\n","            layer_state=layer_state,  # mutates layer state\n","        )\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = residual + x\n","        if not self.normalize_before:\n","            x = self.encoder_attn_layer_norm(x)\n","\n","        # Fully Connected\n","        residual = x\n","        if self.normalize_before:\n","            x = self.final_layer_norm(x)\n","        x = self.activation_fn(self.fc1(x))\n","        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n","        x = self.fc2(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = residual + x\n","        if not self.normalize_before:\n","            x = self.final_layer_norm(x)\n","        return (\n","            x,\n","            self_attn_weights,\n","            layer_state,\n","        )  # just self_attn weights for now, following t5, layer_state = cache for decoding\n","\n","\n","class BartDecoder(nn.Module):\n","    \"\"\"\n","    Transformer decoder consisting of *config.decoder_layers* layers. Each layer\n","    is a :class:`DecoderLayer`.\n","    Args:\n","        config: BartConfig\n","        embed_tokens (torch.nn.Embedding): output embedding\n","    \"\"\"\n","\n","    def __init__(self, config: BartConfig, embed_tokens: nn.Embedding):\n","        super().__init__()\n","        self.dropout = config.dropout\n","        self.layerdrop = config.decoder_layerdrop\n","        self.do_blenderbot_90_layernorm = config.do_blenderbot_90_layernorm  # layernorm variant\n","        self.padding_idx = embed_tokens.padding_idx\n","        self.max_target_positions = config.max_position_embeddings\n","        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n","        self.embed_tokens = embed_tokens\n","        if config.static_position_embeddings:\n","            self.embed_positions = SinusoidalPositionalEmbedding(\n","                config.max_position_embeddings, config.d_model, config.pad_token_id\n","            )\n","        else:\n","            self.embed_positions = LearnedPositionalEmbedding(\n","                config.max_position_embeddings,\n","                config.d_model,\n","                self.padding_idx,\n","                config.extra_pos_embeddings\n","            )\n","        self.layers = nn.ModuleList(\n","            [DecoderLayer(config) for _ in range(config.decoder_layers)]\n","        )  # type: List[DecoderLayer]\n","        self.layernorm_embedding = LayerNorm(config.d_model) if config.normalize_embedding else nn.Identity()\n","        self.layer_norm = LayerNorm(config.d_model) if config.add_final_layer_norm else None\n","        self.config = config\n","\n","    def set_position_embedding(self, special_tag_start_id, tag_first=True):\n","        if tag_first:\n","            embed_positions = DecoderLearnedPositionalEmbedding(\n","                self.config.max_position_embeddings,\n","                self.config.d_model,\n","                self.padding_idx,\n","                self.config.extra_pos_embeddings,\n","                special_tag_start_id\n","            )\n","        else:\n","            embed_positions = DecoderLearnedPositionalEmbedding2(\n","                self.config.max_position_embeddings,\n","                self.config.d_model,\n","                self.padding_idx,\n","                self.config.extra_pos_embeddings,\n","                special_tag_start_id\n","            )\n","\n","        embed_positions.weight.data = self.embed_positions.weight.data\n","        self.embed_positions = embed_positions\n","\n","    def forward(\n","            self,\n","            input_ids,\n","            encoder_hidden_states,\n","            encoder_padding_mask,\n","            decoder_padding_mask,\n","            decoder_causal_mask,\n","            past_key_values=None,\n","            use_cache=False,\n","            output_attentions=False,\n","            output_hidden_states=False,\n","            return_dict=False,\n","            use_pos_cache=False,\n","            **unused,\n","    ):\n","        \"\"\"\n","        Includes several features from \"Jointly Learning to Align and\n","        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n","        Args:\n","            input_ids (LongTensor): previous decoder outputs of shape\n","                `(batch, tgt_len)`, for teacher forcing\n","            encoder_hidden_states: output from the encoder, used for\n","                encoder-side attention\n","            encoder_padding_mask: for ignoring pad tokens\n","            past_key_values (dict or None): dictionary used for storing state during generation\n","        Returns:\n","            BaseModelOutputWithPast or tuple:\n","                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n","                - the cache\n","                - hidden states\n","                - attentions\n","        \"\"\"\n","        if \"decoder_cached_states\" in unused:\n","            warnings.warn(\n","                \"The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = unused.pop(\"decoder_cached_states\")\n","        if \"decoder_past_key_values\" in unused:\n","            warnings.warn(\n","                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = unused.pop(\"decoder_past_key_values\")\n","\n","        # check attention mask and invert\n","        if encoder_padding_mask is not None:\n","            encoder_padding_mask = invert_mask(encoder_padding_mask)\n","\n","        # embed positions\n","        positions = self.embed_positions(input_ids, use_cache=use_pos_cache)\n","\n","        if use_pos_cache:\n","            input_ids = input_ids[:, -1:]\n","            positions = positions[:, -1:]\n","\n","        x = self.embed_tokens(input_ids) * self.embed_scale\n","        if self.do_blenderbot_90_layernorm:\n","            x = self.layernorm_embedding(x)\n","            x += positions\n","        else:\n","            x += positions\n","            x = self.layernorm_embedding(x)\n","\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","\n","        # Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n","        x = x.transpose(0, 1)\n","        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n","\n","        # decoder layers\n","        all_hidden_states = () if output_hidden_states else None\n","        all_self_attns = () if output_attentions else None\n","        next_decoder_cache = []\n","        for idx, decoder_layer in enumerate(self.layers):\n","            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n","            if output_hidden_states:\n","                all_hidden_states += (x,)\n","            dropout_probability = random.uniform(0, 1)\n","            if self.training and (dropout_probability < self.layerdrop):\n","                continue\n","\n","            layer_state = past_key_values[idx] if past_key_values is not None else None\n","\n","            x, layer_self_attn, layer_past = decoder_layer(\n","                x,\n","                encoder_hidden_states,\n","                encoder_attn_mask=encoder_padding_mask,\n","                decoder_padding_mask=decoder_padding_mask,\n","                layer_state=layer_state,\n","                causal_mask=decoder_causal_mask,\n","                output_attentions=output_attentions,\n","            )\n","\n","            if use_cache:\n","                next_decoder_cache.append(layer_past.copy())\n","\n","            if output_attentions:\n","                all_self_attns += (layer_self_attn,)\n","\n","        if self.layer_norm:  # if config.add_final_layer_norm (mBART)\n","            x = self.layer_norm(x)\n","\n","        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n","        if output_hidden_states:\n","            all_hidden_states = tuple(hidden_state.transpose(0, 1) for hidden_state in all_hidden_states)\n","        x = x.transpose(0, 1)\n","        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n","\n","        next_cache = next_decoder_cache if use_cache else None\n","\n","        if not return_dict:\n","            return tuple(v for v in [x, next_cache, all_hidden_states, all_self_attns] if v is not None)\n","        return BaseModelOutputWithPast(\n","            last_hidden_state=x, past_key_values=next_cache, hidden_states=all_hidden_states, attentions=all_self_attns\n","        )\n","\n","\n","def _reorder_buffer(attn_cache, new_order):\n","    for k, input_buffer_k in attn_cache.items():\n","        if input_buffer_k is not None:\n","            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n","    return attn_cache\n","\n","\n","class Attention(nn.Module):\n","    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n","\n","    def __init__(\n","            self,\n","            embed_dim,\n","            num_heads,\n","            dropout=0.0,\n","            bias=True,\n","            encoder_decoder_attention=False,  # otherwise self_attention\n","    ):\n","        super().__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.dropout = dropout\n","        self.head_dim = embed_dim // num_heads\n","        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n","        self.scaling = self.head_dim ** -0.5\n","\n","        self.encoder_decoder_attention = encoder_decoder_attention\n","        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","        self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n","\n","    def _shape(self, tensor, seq_len, bsz):\n","        return tensor.contiguous().view(seq_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n","\n","    def forward(\n","            self,\n","            query,\n","            key: Optional[Tensor],\n","            key_padding_mask: Optional[Tensor] = None,\n","            layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n","            attn_mask: Optional[Tensor] = None,\n","            output_attentions=False,\n","    ) -> Tuple[Tensor, Optional[Tensor]]:\n","        \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n","        static_kv: bool = self.encoder_decoder_attention\n","        tgt_len, bsz, embed_dim = query.size()\n","        assert embed_dim == self.embed_dim\n","        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n","        # get here for encoder decoder cause of static_kv\n","        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n","            saved_state = layer_state.get(self.cache_key, {})\n","            if \"prev_key\" in saved_state and static_kv:\n","                # previous time steps are cached - no need to recompute key and value if they are static\n","                key = None\n","        else:\n","            saved_state = None\n","            layer_state = {}\n","\n","        q = self.q_proj(query) * self.scaling\n","        if static_kv:\n","            if key is None:\n","                k = v = None\n","            else:\n","                k = self.k_proj(key)\n","                v = self.v_proj(key)\n","        else:\n","            k = self.k_proj(query)\n","            v = self.v_proj(query)\n","\n","        q = self._shape(q, tgt_len, bsz)\n","        if k is not None:\n","            k = self._shape(k, -1, bsz)\n","        if v is not None:\n","            v = self._shape(v, -1, bsz)\n","        if saved_state is not None:\n","            k, v, key_padding_mask = self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)\n","        # Update cache\n","        layer_state[self.cache_key] = {\n","            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n","            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n","            \"prev_key_padding_mask\": key_padding_mask if not static_kv else None,\n","        }\n","\n","        assert k is not None\n","        src_len = k.size(1)\n","        attn_weights = torch.bmm(q, k.transpose(1, 2))\n","        assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n","\n","        if attn_mask is not None:\n","            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attn_mask\n","            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n","\n","        # This is part of a workaround to get around fork/join parallelism not supporting Optional types.\n","        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n","            key_padding_mask = None\n","        assert key_padding_mask is None or key_padding_mask.size()[:2] == (\n","            bsz,\n","            src_len,\n","        )\n","\n","        if key_padding_mask is not None:  # don't attend to padding symbols\n","            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n","            reshaped = key_padding_mask.unsqueeze(1).unsqueeze(2)\n","            attn_weights = attn_weights.masked_fill(reshaped, float(\"-inf\"))\n","            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n","        attn_weights = F.softmax(attn_weights, dim=-1)\n","        attn_probs = F.dropout(\n","            attn_weights,\n","            p=self.dropout,\n","            training=self.training,\n","        )\n","\n","        assert v is not None\n","        attn_output = torch.bmm(attn_probs, v)\n","        assert attn_output.size() == (bsz * self.num_heads, tgt_len, self.head_dim)\n","        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n","        attn_output = self.out_proj(attn_output)\n","        if output_attentions:\n","            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n","        else:\n","            attn_weights = None\n","        return attn_output, attn_weights\n","\n","    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv, bsz):\n","        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n","        if \"prev_key\" in saved_state:\n","            _prev_key = saved_state[\"prev_key\"]\n","            assert _prev_key is not None\n","            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n","            if static_kv:\n","                k = prev_key\n","            else:\n","                assert k is not None\n","                k = torch.cat([prev_key, k], dim=1)\n","        if \"prev_value\" in saved_state:\n","            _prev_value = saved_state[\"prev_value\"]\n","            assert _prev_value is not None\n","            prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n","            if static_kv:\n","                v = prev_value\n","            else:\n","                assert v is not None\n","                v = torch.cat([prev_value, v], dim=1)\n","        assert k is not None and v is not None\n","        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\"prev_key_padding_mask\", None)\n","        if prev_key_padding_mask is not None:\n","            if static_kv:\n","                new_key_padding_mask = prev_key_padding_mask\n","            else:\n","                new_key_padding_mask = torch.cat([prev_key_padding_mask, key_padding_mask], dim=1)\n","        else:\n","            new_key_padding_mask = key_padding_mask\n","        return k, v, new_key_padding_mask\n","\n","\n","class BartClassificationHead(nn.Module):\n","    \"\"\"Head for sentence-level classification tasks.\"\"\"\n","\n","    # This can trivially be shared with RobertaClassificationHead\n","\n","    def __init__(\n","            self,\n","            input_dim,\n","            inner_dim,\n","            num_classes,\n","            pooler_dropout,\n","    ):\n","        super().__init__()\n","        self.dense = nn.Linear(input_dim, inner_dim)\n","        self.dropout = nn.Dropout(p=pooler_dropout)\n","        self.out_proj = nn.Linear(inner_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","\n","class LearnedPositionalEmbedding(nn.Embedding):\n","    \"\"\"\n","    This module learns positional embeddings up to a fixed maximum size.\n","    Padding ids are ignored by either offsetting based on padding_idx\n","    or by setting padding_idx to None and ensuring that the appropriate\n","    position ids are passed to the forward function.\n","    \"\"\"\n","\n","    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, offset):\n","        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n","        # and adjust num_embeddings appropriately. Other models dont have this hack\n","        self.offset = offset\n","        assert padding_idx is not None\n","        num_embeddings += offset\n","        super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n","\n","    def forward(self, input_ids, use_cache=False):\n","        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n","        bsz, seq_len = input_ids.shape[:2]\n","        if use_cache:\n","            positions = input_ids.data.new(1, 1).fill_(seq_len - 1)  # called before slicing\n","        else:\n","            # starts at 0, ends at 1-seq_len\n","            positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n","        return super().forward(positions + self.offset)\n","\n","\n","class DecoderLearnedPositionalEmbedding(nn.Embedding):\n","    \"\"\"\n","    主要修改是，position的是循环的\n","    This module learns positional embeddings up to a fixed maximum size.\n","    Padding ids are ignored by either offsetting based on padding_idx\n","    or by setting padding_idx to None and ensuring that the appropriate\n","    position ids are passed to the forward function.\n","    \"\"\"\n","\n","    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, offset,\n","                 special_tag_start_id=None):\n","        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n","        # and adjust num_embeddings appropriately. Other models dont have this hack\n","        self.offset = offset\n","        assert padding_idx is not None\n","        num_embeddings += offset\n","        self.special_tag_start_id = special_tag_start_id  # 这个id之后的词是特殊词汇\n","        super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n","\n","    def forward(self, input_ids, use_cache=False):\n","        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n","        if self.special_tag_start_id is None or input_ids.size(1)<2:\n","            bsz, seq_len = input_ids.shape[:2]\n","            if use_cache:\n","                positions = input_ids.data.new(1, 1).fill_(seq_len - 1)  # called before slicing\n","            else:\n","                # starts at 0, ends at 1-seq_len\n","                positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n","        else:\n","            # 实现的是每个位置重新开始position\n","            \"\"\"\n","                大概意思是，假设input_ids中假设大于4是特殊符号，那么输入是\n","                [[2, 4, 1, 2, 3, 5, 1],\n","                 [2, 5, 3, 3, 0, 0, 0]]时，输出为\n","                [[0, 1, 2, 3, 4, 1, 2],\n","                 [0, 1, 2, 3, 4, 5, 6] 每个大于4的位置都会重置\n","            \"\"\"\n","            _input_ids = input_ids[:, 1:]\n","            bsz, seq_len = _input_ids.shape[:2]\n","            special_tag_mask = _input_ids.ge(self.special_tag_start_id)  # bsz x max_len\n","            if special_tag_mask.sum()>0:\n","                num_masks = special_tag_mask.cumsum(dim=1).max()  # 表示最长的\n","                arange_indices = torch.arange(seq_len).to(_input_ids).expand_as(_input_ids)  # bsz x max_len\n","                special_tag_indice = arange_indices.masked_select(special_tag_mask)  # a vector只包含所有的special的indice\n","                indices = torch.arange(num_masks).to(_input_ids)[None].repeat(bsz, 1)  # bsz x mask_len\n","                mask = indices.lt(special_tag_mask.sum(dim=1, keepdim=True))\n","                indices = indices.masked_scatter(mask, special_tag_indice)\n","                _, inverted_indices = special_tag_mask.cumsum(dim=-1).unique(return_inverse=True)\n","\n","                inverted_indices = inverted_indices - inverted_indices[:, :1]\n","                inverted_indices = inverted_indices.masked_fill(inverted_indices.ge(indices.size(1)), max(indices.size(1)-1, 0))\n","                positions = indices.gather(index=inverted_indices, dim=1)\n","                positions = (arange_indices - positions) + 1\n","            else:\n","                positions = torch.arange(seq_len+1, dtype=torch.long, device=self.weight.device)[None]\n","\n","            if use_cache:\n","                positions = positions[:, -1:]\n","            else:\n","                positions = torch.cat([input_ids.new_zeros(bsz, 1), positions], dim=1)\n","\n","        return super().forward(positions + self.offset)\n","\n","\n","class DecoderLearnedPositionalEmbedding2(nn.Embedding):\n","    \"\"\"\n","    主要修改是，position的是循环的, 和上面的区别是tag所在的位置不同\n","    This module learns positional embeddings up to a fixed maximum size.\n","    Padding ids are ignored by either offsetting based on padding_idx\n","    or by setting padding_idx to None and ensuring that the appropriate\n","    position ids are passed to the forward function.\n","    \"\"\"\n","\n","    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, offset,\n","                 special_tag_start_id=None):\n","        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n","        # and adjust num_embeddings appropriately. Other models dont have this hack\n","        self.offset = offset\n","        assert padding_idx is not None\n","        num_embeddings += offset\n","        self.special_tag_start_id = special_tag_start_id  # 这个id之后的词是特殊词汇\n","        super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n","\n","    def forward(self, input_ids, use_cache=False):\n","        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n","        if self.special_tag_start_id is None or input_ids.size(1)<2:\n","            bsz, seq_len = input_ids.shape[:2]\n","            if use_cache:\n","                positions = input_ids.data.new(1, 1).fill_(seq_len - 1)  # called before slicing\n","            else:\n","                # starts at 0, ends at 1-seq_len\n","                positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n","        else:\n","            # 实现的是每个位置重新开始position\n","            \"\"\"\n","                大概意思是，假设input_ids中假设大于4是特殊符号，那么输入是\n","                [[2, 1, 2, 3, 4, 1, 5],\n","                 [2, 3, 3, 5, 0, 0, 0]]时，输出为\n","                [[0, 1, 2, 3, 4, 1, 2],\n","                 [0, 1, 2, 3, 4, 5, 6] 每个大于4的位置都会重置\n","            \"\"\"\n","            _input_ids = input_ids[:, 1:]  # 把sos去掉\n","            bsz, seq_len = _input_ids.shape[:2]\n","            special_tag_mask = _input_ids.ge(self.special_tag_start_id)  # bsz x max_len\n","            if special_tag_mask.sum()>0:\n","                num_masks = special_tag_mask.cumsum(dim=1)  # 表示最长的\n","                num_masks_value = num_masks.max()\n","                arange_indices = torch.arange(seq_len).to(_input_ids).expand_as(_input_ids)  # bsz x max_len\n","\n","                special_tag_indice = arange_indices.masked_select(special_tag_mask)  # a vector只包含所有的special的indice\n","                indices = torch.arange(num_masks_value).to(_input_ids)[None].repeat(bsz, 1)  # bsz x mask_len\n","                mask = indices.lt(special_tag_mask.sum(dim=-1, keepdim=True))\n","                special_tag_indice = indices.masked_scatter(mask, special_tag_indice)\n","\n","                indices = torch.cat([special_tag_indice.new_zeros(bsz, 1), special_tag_indice[:, :-1] + 1], dim=1)\n","                _, inverted_indices = special_tag_mask.flip(dims=[1]).cumsum(dim=-1).flip(dims=[1]).unique(\n","                    return_inverse=True)\n","                values = inverted_indices[:, 0]  # bsz\n","                inverted_indices = values[:, None] - inverted_indices\n","                inverted_indices = inverted_indices.masked_fill(inverted_indices.ge(indices.size(1)), indices.size(1)-1)\n","\n","                positions = indices.gather(index=inverted_indices, dim=1)\n","                positions = arange_indices - positions + 1\n","            else:\n","                positions = torch.arange(seq_len+1, dtype=torch.long, device=self.weight.device)[None]\n","\n","        if use_cache:\n","            positions = positions[:, -1:]\n","        else:\n","            positions = torch.cat([input_ids.new_zeros(bsz, 1), positions], dim=1)\n","\n","        return super().forward(positions + self.offset)\n","\n","\n","def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True):\n","    if torch.cuda.is_available():\n","        try:\n","            from apex.normalization import FusedLayerNorm\n","\n","            return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n","        except ImportError:\n","            pass\n","    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n","\n","\n","def fill_with_neg_inf(t):\n","    \"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"\n","    return t.float().fill_(float(\"-inf\")).type_as(t)\n","\n","\n","# Public API\n","def _get_shape(t):\n","    return getattr(t, \"shape\", None)\n","\n","\n","class BartModel(PretrainedBartModel):\n","    def __init__(self, config: BartConfig):\n","        super().__init__(config)\n","\n","        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n","        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n","\n","        self.encoder = BartEncoder(config, self.shared)\n","        self.decoder = BartDecoder(config, self.shared)\n","\n","        self.init_weights()\n","\n","    \n","    def forward(\n","            self,\n","            input_ids,\n","            attention_mask=None,\n","            decoder_input_ids=None,\n","            decoder_attention_mask=None,\n","            encoder_outputs: Optional[Tuple] = None,\n","            past_key_values=None,\n","            use_cache=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","            **kwargs,\n","    ):\n","        if \"decoder_past_key_values\" in kwargs:\n","            warnings.warn(\n","                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = kwargs.pop(\"decoder_past_key_values\")\n","\n","        if decoder_input_ids is None:\n","            use_cache = False\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # make masks if user doesn't supply\n","        if not use_cache:\n","            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n","                self.config,\n","                input_ids,\n","                decoder_input_ids=decoder_input_ids,\n","                decoder_padding_mask=decoder_attention_mask,\n","                causal_mask_dtype=self.shared.weight.dtype,\n","            )\n","        else:\n","            decoder_padding_mask, causal_mask = None, None\n","\n","        assert decoder_input_ids is not None\n","\n","        if encoder_outputs is None:\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOuput when return_dict=False\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","            )\n","\n","        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n","        decoder_outputs = self.decoder(\n","            decoder_input_ids,\n","            encoder_outputs[0],\n","            attention_mask,\n","            decoder_padding_mask,\n","            decoder_causal_mask=causal_mask,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        if not return_dict:\n","            return decoder_outputs + encoder_outputs\n","\n","        return Seq2SeqModelOutput(\n","            last_hidden_state=decoder_outputs.last_hidden_state,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            encoder_hidden_states=encoder_outputs.hidden_states,\n","            encoder_attentions=encoder_outputs.attentions,\n","        )\n","\n","    def get_input_embeddings(self):\n","        return self.shared\n","\n","    def set_input_embeddings(self, value):\n","        self.shared = value\n","        self.encoder.embed_tokens = self.shared\n","        self.decoder.embed_tokens = self.shared\n","\n","    def get_output_embeddings(self):\n","        return _make_linear_from_emb(self.shared)  # make it on the fly\n","\n","\n","\n","class BartForConditionalGeneration(PretrainedBartModel):\n","    base_model_prefix = \"model\"\n","    authorized_missing_keys = [r\"final_logits_bias\", r\"encoder\\.version\", r\"decoder\\.version\"]\n","\n","    def __init__(self, config: BartConfig):\n","        super().__init__(config)\n","        base_model = BartModel(config)\n","        self.model = base_model\n","        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n","\n","    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n","        old_num_tokens = self.model.shared.num_embeddings\n","        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n","        self.model.shared = new_embeddings\n","        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n","        return new_embeddings\n","\n","    def _resize_final_logits_bias(self, new_num_tokens: int, old_num_tokens: int) -> None:\n","        if new_num_tokens <= old_num_tokens:\n","            new_bias = self.final_logits_bias[:, :new_num_tokens]\n","        else:\n","            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n","            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n","        self.register_buffer(\"final_logits_bias\", new_bias)\n","\n","    \n","    def forward(\n","            self,\n","            input_ids,\n","            attention_mask=None,\n","            decoder_input_ids=None,\n","            decoder_attention_mask=None,\n","            encoder_outputs=None,\n","            past_key_values=None,\n","            labels=None,\n","            use_cache=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","            **unused,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the masked language modeling loss.\n","            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).\n","            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens\n","            with labels in ``[0, ..., config.vocab_size]``.\n","        Returns:\n","        Conditional generation example::\n","            >>> # Mask filling only works for bart-large\n","            >>> from transformers import BartTokenizer, BartForConditionalGeneration\n","            >>> tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n","            >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n","            >>> model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n","            >>> input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']\n","            >>> logits = model(input_ids).logits\n","            >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n","            >>> probs = logits[0, masked_index].softmax(dim=0)\n","            >>> values, predictions = probs.topk(5)\n","            >>> tokenizer.decode(predictions).split()\n","            >>> # ['good', 'great', 'all', 'really', 'very']\n","        \"\"\"\n","        if \"lm_labels\" in unused:\n","            warnings.warn(\n","                \"The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n","                FutureWarning,\n","            )\n","            labels = unused.pop(\"lm_labels\")\n","        if \"decoder_cached_states\" in unused:\n","            warnings.warn(\n","                \"The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = unused.pop(\"decoder_cached_states\")\n","        if \"decoder_past_key_values\" in unused:\n","            warnings.warn(\n","                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = unused.pop(\"decoder_past_key_values\")\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if labels is not None:\n","            use_cache = False\n","            if decoder_input_ids is None:\n","                decoder_input_ids = shift_tokens_right(labels, self.config.pad_token_id)\n","\n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            encoder_outputs=encoder_outputs,\n","            decoder_attention_mask=decoder_attention_mask,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        lm_logits = F.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)\n","\n","        masked_lm_loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # TODO(SS): do we need to ignore pad tokens in labels?\n","            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (lm_logits,) + outputs[1:]\n","            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n","\n","        return Seq2SeqLMOutput(\n","            loss=masked_lm_loss,\n","            logits=lm_logits,\n","            past_key_values=outputs.past_key_values,\n","            decoder_hidden_states=outputs.decoder_hidden_states,\n","            decoder_attentions=outputs.decoder_attentions,\n","            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n","            encoder_hidden_states=outputs.encoder_hidden_states,\n","            encoder_attentions=outputs.encoder_attentions,\n","        )\n","\n","    def prepare_inputs_for_generation(\n","            self, decoder_input_ids, past, attention_mask, use_cache, encoder_outputs, **kwargs\n","    ):\n","        return {\n","            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n","            \"encoder_outputs\": encoder_outputs,\n","            \"past_key_values\": past,\n","            \"decoder_input_ids\": decoder_input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n","        }\n","\n","    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n","        if cur_len == 1 and self.config.force_bos_token_to_be_generated:\n","            self._force_token_ids_generation(logits, self.config.bos_token_id)\n","        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:\n","            self._force_token_ids_generation(logits, self.config.eos_token_id)\n","        return logits\n","\n","    def _force_token_ids_generation(self, scores, token_id) -> None:\n","        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float(\"inf\"))\"\"\"\n","        scores[:, [x for x in range(self.config.vocab_size) if x != token_id]] = -float(\"inf\")\n","\n","    @staticmethod\n","    def _reorder_cache(past, beam_idx):\n","        reordered_past = []\n","        for layer_past in past:\n","            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n","            layer_past_new = {\n","                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()\n","            }\n","            reordered_past.append(layer_past_new)\n","        return reordered_past\n","\n","    def get_encoder(self):\n","        return self.model.encoder\n","\n","    def get_output_embeddings(self):\n","        return _make_linear_from_emb(self.model.shared)  # make it on the fly\n","\n","\n","\n","class BartForSequenceClassification(PretrainedBartModel):\n","    def __init__(self, config: BartConfig, **kwargs):\n","        super().__init__(config, **kwargs)\n","        self.model = BartModel(config)\n","        self.classification_head = BartClassificationHead(\n","            config.d_model,\n","            config.d_model,\n","            config.num_labels,\n","            config.classifier_dropout,\n","        )\n","        self.model._init_weights(self.classification_head.dense)\n","        self.model._init_weights(self.classification_head.out_proj)\n","\n","    \n","    def forward(\n","            self,\n","            input_ids,\n","            attention_mask=None,\n","            decoder_input_ids=None,\n","            decoder_attention_mask=None,\n","            encoder_outputs=None,\n","            labels=None,\n","            use_cache=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for computing the sequence classification/regression loss.\n","            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n","            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        if labels is not None:\n","            use_cache = False\n","\n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            encoder_outputs=encoder_outputs,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        x = outputs[0]  # last hidden state\n","        eos_mask = input_ids.eq(self.config.eos_token_id)\n","        if len(torch.unique(eos_mask.sum(1))) > 1:\n","            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n","        sentence_representation = x[eos_mask, :].view(x.size(0), -1, x.size(-1))[:, -1, :]\n","        logits = self.classification_head(sentence_representation)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return Seq2SeqSequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            past_key_values=outputs.past_key_values,\n","            decoder_hidden_states=outputs.decoder_hidden_states,\n","            decoder_attentions=outputs.decoder_attentions,\n","            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n","            encoder_hidden_states=outputs.encoder_hidden_states,\n","            encoder_attentions=outputs.encoder_attentions,\n","        )\n","\n","\n","\n","class BartForQuestionAnswering(PretrainedBartModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        config.num_labels = 2\n","        self.num_labels = config.num_labels\n","\n","        self.model = BartModel(config)\n","        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        self.model._init_weights(self.qa_outputs)\n","\n","    \n","    def forward(\n","            self,\n","            input_ids,\n","            attention_mask=None,\n","            decoder_input_ids=None,\n","            decoder_attention_mask=None,\n","            encoder_outputs=None,\n","            start_positions=None,\n","            end_positions=None,\n","            use_cache=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","    ):\n","        r\"\"\"\n","        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n","            Positions are clamped to the length of the sequence (`sequence_length`).\n","            Position outside of the sequence are not taken into account for computing the loss.\n","        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n","            Positions are clamped to the length of the sequence (`sequence_length`).\n","            Position outside of the sequence are not taken into account for computing the loss.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        if start_positions is not None and end_positions is not None:\n","            use_cache = False\n","\n","        outputs = self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            encoder_outputs=encoder_outputs,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        logits = self.qa_outputs(sequence_output)\n","        start_logits, end_logits = logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","\n","        total_loss = None\n","        if start_positions is not None and end_positions is not None:\n","            # If we are on multi-GPU, split add a dimension\n","            if len(start_positions.size()) > 1:\n","                start_positions = start_positions.squeeze(-1)\n","            if len(end_positions.size()) > 1:\n","                end_positions = end_positions.squeeze(-1)\n","            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n","            ignored_index = start_logits.size(1)\n","            start_positions.clamp_(0, ignored_index)\n","            end_positions.clamp_(0, ignored_index)\n","\n","            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n","            start_loss = loss_fct(start_logits, start_positions)\n","            end_loss = loss_fct(end_logits, end_positions)\n","            total_loss = (start_loss + end_loss) / 2\n","\n","        if not return_dict:\n","            output = (\n","                         start_logits,\n","                         end_logits,\n","                     ) + outputs[1:]\n","            return ((total_loss,) + output) if total_loss is not None else output\n","\n","        return Seq2SeqQuestionAnsweringModelOutput(\n","            loss=total_loss,\n","            start_logits=start_logits,\n","            end_logits=end_logits,\n","            past_key_values=outputs.past_key_values,\n","            decoder_hidden_states=outputs.decoder_hidden_states,\n","            decoder_attentions=outputs.decoder_attentions,\n","            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n","            encoder_hidden_states=outputs.encoder_hidden_states,\n","            encoder_attentions=outputs.encoder_attentions,\n","        )\n","\n","\n","class SinusoidalPositionalEmbedding(nn.Embedding):\n","    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n","\n","    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n","        super().__init__(num_positions, embedding_dim)\n","        if embedding_dim % 2 != 0:\n","            raise NotImplementedError(f\"odd embedding_dim {embedding_dim} not supported\")\n","        self.weight = self._init_weight(self.weight)\n","\n","    @staticmethod\n","    def _init_weight(out: nn.Parameter):\n","        \"\"\"Identical to the XLM create_sinusoidal_embeddings except features are not interleaved.\n","        The cos features are in the 2nd half of the vector. [dim // 2:]\n","        \"\"\"\n","        n_pos, dim = out.shape\n","        position_enc = np.array(\n","            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n","        )\n","        out[:, 0: dim // 2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))  # This line breaks for odd n_pos\n","        out[:, dim // 2:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n","        out.detach_()\n","        out.requires_grad = False\n","        return out\n","\n","    @torch.no_grad()\n","    def forward(self, input_ids, use_cache=False):\n","        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n","        bsz, seq_len = input_ids.shape[:2]\n","        if use_cache:\n","            positions = input_ids.data.new(1, 1).fill_(seq_len - 1)  # called before slicing\n","        else:\n","            # starts at 0, ends at 1-seq_len\n","            positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n","        return super().forward(positions)"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"gM9Rbqb5DZzZ","executionInfo":{"status":"ok","timestamp":1654961034177,"user_tz":-420,"elapsed":1697,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["import torch\n","from transformers import BartTokenizer\n","from fastNLP import seq_len_to_mask\n","from fastNLP.modules import Seq2SeqEncoder, Seq2SeqDecoder, State\n","import torch.nn.functional as F\n","from fastNLP.models import Seq2SeqModel\n","from torch import nn\n","import math\n","\n","\n","class FBartEncoder(Seq2SeqEncoder):\n","    def __init__(self, encoder):\n","        super().__init__()\n","        assert isinstance(encoder, BartEncoder)\n","        self.bart_encoder = encoder\n","\n","    def forward(self, src_tokens, src_seq_len):\n","        mask = seq_len_to_mask(src_seq_len, max_len=src_tokens.size(1))\n","        dict = self.bart_encoder(input_ids=src_tokens, attention_mask=mask, return_dict=True,\n","                                 output_hidden_states=True)\n","        encoder_outputs = dict.last_hidden_state\n","        hidden_states = dict.hidden_states\n","        return encoder_outputs, mask, hidden_states\n","\n","\n","class FBartDecoder(Seq2SeqDecoder):\n","    def __init__(self, decoder, pad_token_id, label_ids, use_encoder_mlp=True):\n","        super().__init__()\n","        assert isinstance(decoder, BartDecoder)\n","        self.decoder = decoder\n","        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n","        causal_mask = causal_mask.triu(diagonal=1)\n","        self.register_buffer('causal_masks', causal_mask.float())\n","        self.pad_token_id = pad_token_id\n","        self.label_start_id = label_ids[0]\n","        self.label_end_id = label_ids[-1]+1\n","        # 0th position is <s>, 1st position is </s>\n","        mapping = torch.LongTensor([0, 2]+sorted(label_ids, reverse=False))\n","        self.register_buffer('mapping', mapping)\n","        self.src_start_index = len(mapping)  # 加上一个\n","        hidden_size = decoder.embed_tokens.weight.size(1)\n","        self.encoder_attention = nn.Sequential(nn.Linear(hidden_size, hidden_size//2),\n","                                             nn.Dropout(0.3),\n","                                             nn.ReLU(),\n","                                             nn.Linear(hidden_size//2, 64))\n","        self.decoder_attention = nn.Sequential(nn.Linear(hidden_size, hidden_size//2),\n","                                             nn.Dropout(0.3),\n","                                             nn.ReLU(),\n","                                             nn.Linear(hidden_size//2, 64))\n","        if use_encoder_mlp:\n","            self.encoder_mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n","                                             nn.Dropout(0.3),\n","                                             nn.ReLU(),\n","                                             nn.Linear(hidden_size, hidden_size))\n","\n","    def forward(self, tokens, state):\n","        # bsz, max_len = tokens.size()\n","        encoder_outputs = state.encoder_output\n","        encoder_pad_mask = state.encoder_mask\n","\n","        first = state.first\n","        # eos is 1\n","        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n","        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n","\n","        # mapping to the BART token index\n","        mapping_token_mask = tokens.lt(self.src_start_index)  #\n","        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n","        tag_mapped_tokens = self.mapping[mapped_tokens]\n","\n","        src_tokens_index = tokens - self.src_start_index # bsz x num_src_token\n","        src_tokens_index = src_tokens_index.masked_fill(src_tokens_index.lt(0), 0)\n","        src_tokens = state.src_tokens\n","        if first is not None:\n","            src_tokens = src_tokens.gather(index=first, dim=1)\n","        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n","\n","        tokens = torch.where(mapping_token_mask, tag_mapped_tokens, word_mapped_tokens)\n","        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n","\n","        if self.training:\n","            tokens = tokens[:, :-1]\n","            decoder_pad_mask = tokens.eq(self.pad_token_id)\n","            dict = self.decoder(input_ids=tokens,\n","                                encoder_hidden_states=encoder_outputs,\n","                                encoder_padding_mask=encoder_pad_mask,\n","                                decoder_padding_mask=decoder_pad_mask,\n","                                decoder_causal_mask=self.causal_masks[:tokens.size(1), :tokens.size(1)],\n","                                return_dict=True)\n","        else:\n","            past_key_values = state.past_key_values\n","            dict = self.decoder(input_ids=tokens,\n","                                encoder_hidden_states=encoder_outputs,\n","                                encoder_padding_mask=encoder_pad_mask,\n","                                decoder_padding_mask=None,\n","                                decoder_causal_mask=None,\n","                                past_key_values=past_key_values,\n","                                use_cache=True,\n","                                return_dict=True)\n","        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size\n","        hidden_state = self.decoder_attention(hidden_state)\n","        if not self.training:\n","            state.past_key_values = dict.past_key_values\n","\n","        logits = hidden_state.new_full((hidden_state.size(0), hidden_state.size(1), self.src_start_index+src_tokens.size(-1)),\n","                                       fill_value=-1e24)\n","\n","        # first get the\n","        eos_scores = F.linear(hidden_state, self.decoder.embed_tokens.weight[2:3])  # bsz x max_len x 1\n","        tag_scores = F.linear(hidden_state, self.decoder.embed_tokens.weight[self.label_start_id:self.label_end_id])  # bsz x max_len x num_class\n","\n","        # bsz x max_word_len x hidden_size\n","        src_outputs = state.encoder_output\n","\n","        if hasattr(self, 'encoder_mlp'):\n","            src_outputs = self.encoder_mlp(src_outputs)\n","\n","        if first is not None:\n","            mask = first.eq(0)  # bsz x 1 x max_word_len, 为1的地方是padding\n","            src_outputs = src_outputs.gather(index=first.unsqueeze(2).repeat(1, 1, src_outputs.size(-1)), dim=1)\n","        else:\n","            mask = state.encoder_mask.eq(0)\n","        src_outputs = self.decoder_attention(src_outputs)\n","        mask = mask.unsqueeze(1).__or__(src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1))\n","        word_scores = torch.einsum('blh,bnh->bln', hidden_state, src_outputs)  # bsz x max_len x max_word_len\n","        word_scores = word_scores.masked_fill(mask, -1e32)\n","\n","        logits[:, :, 1:2] = eos_scores\n","        logits[:, :, 2:self.src_start_index] = tag_scores\n","        logits[:, :, self.src_start_index:] = word_scores\n","\n","        return logits\n","\n","    def decode(self, tokens, state):\n","        return self(tokens, state)[:, -1]\n","\n","\n","class CaGFBartDecoder(FBartDecoder):\n","    # Copy and generate,\n","    def __init__(self, decoder, pad_token_id, label_ids, use_encoder_mlp=False):\n","        super().__init__(decoder, pad_token_id, label_ids, use_encoder_mlp=use_encoder_mlp)\n","        \n","    def forward(self, tokens, state):\n","        encoder_outputs = state.encoder_output\n","        encoder_pad_mask = state.encoder_mask\n","        first = state.first\n","\n","        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n","        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n","\n","        mapping_token_mask = tokens.lt(self.src_start_index)\n","        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n","        tag_mapped_tokens = self.mapping[mapped_tokens]\n","\n","        src_tokens_index = tokens - self.src_start_index # bsz x num_src_token\n","        src_tokens_index = src_tokens_index.masked_fill(src_tokens_index.lt(0), 0)\n","        src_tokens = state.src_tokens\n","        if first is not None:\n","            src_tokens = src_tokens.gather(index=first, dim=1)\n","        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n","\n","        tokens = torch.where(mapping_token_mask, tag_mapped_tokens, word_mapped_tokens)  # bsz x max_len\n","        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n","\n","        if self.training:\n","            tokens = tokens[:, :-1]\n","            decoder_pad_mask = tokens.eq(self.pad_token_id)  # decoder需要让pad位置为1\n","            dict = self.decoder(input_ids=tokens,\n","                                encoder_hidden_states=encoder_outputs,\n","                                encoder_padding_mask=encoder_pad_mask,\n","                                decoder_padding_mask=decoder_pad_mask,\n","                                decoder_causal_mask=self.causal_masks[:tokens.size(1), :tokens.size(1)],\n","                                return_dict=True)\n","        else:\n","            past_key_values = state.past_key_values\n","            dict = self.decoder(input_ids=tokens,\n","                                encoder_hidden_states=encoder_outputs,\n","                                encoder_padding_mask=encoder_pad_mask,\n","                                decoder_padding_mask=None,\n","                                decoder_causal_mask=None,\n","                                past_key_values=past_key_values,\n","                                use_cache=True,\n","                                return_dict=True)\n","        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size\n","        hidden_state = self.decoder_attention(hidden_state)\n","        if not self.training:\n","            state.past_key_values = dict.past_key_values\n","\n","        logits = hidden_state.new_full((hidden_state.size(0), hidden_state.size(1), self.src_start_index+src_tokens.size(-1)),\n","                                       fill_value=-1e24)\n","\n","        eos_scores = F.linear(hidden_state, self.encoder_attention(self.decoder.embed_tokens.weight[2:3]))  # bsz x max_len x 1\n","        tag_scores = F.linear(hidden_state, self.encoder_attention(self.decoder.embed_tokens.weight[self.label_start_id:self.label_end_id]))  # bsz x max_len x num_class\n","\n","\n","        # bsz x max_bpe_len x hidden_size\n","        src_outputs = state.encoder_output\n","        if hasattr(self, 'encoder_mlp'):\n","            src_outputs = self.encoder_mlp(src_outputs)\n","\n","        if first is not None:\n","            mask = first.eq(0)  # bsz x 1 x max_word_len, 为1的地方是padding\n","            # bsz x max_word_len x hidden_size\n","            src_outputs = src_outputs.gather(index=first.unsqueeze(2).repeat(1, 1, src_outputs.size(-1)), dim=1)\n","        else:\n","            mask = state.encoder_mask.eq(0)\n","            # src_outputs = self.decoder.embed_tokens(src_tokens)\n","        mask = mask.unsqueeze(1)\n","\n","        src_outputs = self.encoder_attention(src_outputs)\n","        input_embed = self.encoder_attention(self.decoder.embed_tokens(src_tokens) ) # bsz x max_word_len x hidden_size\n","        \n","        word_scores = torch.einsum('blh,bnh->bln', hidden_state, src_outputs)  # bsz x max_len x max_word_len\n","        gen_scores = torch.einsum('blh,bnh->bln', hidden_state, input_embed)  # bsz x max_len x max_word_len\n","        word_scores = (gen_scores + word_scores)/2\n","        mask = mask.__or__(src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1))\n","        word_scores = word_scores.masked_fill(mask, -1e32)\n","\n","        logits[:, :, 1:2] = eos_scores\n","        logits[:, :, 2:self.src_start_index] = tag_scores\n","        logits[:, :, self.src_start_index:] = word_scores\n","\n","        return logits\n","\n","\n","class BartSeq2SeqModel(Seq2SeqModel):\n","    @classmethod\n","    def build_model(cls, bart_model, tokenizer, label_ids, decoder_type=None, copy_gate=False,\n","                    use_encoder_mlp=False, use_recur_pos=False, tag_first=False):\n","        model = BartModel.from_pretrained(bart_model)\n","        num_tokens, _ = model.encoder.embed_tokens.weight.shape\n","        model.resize_token_embeddings(len(tokenizer.unique_no_split_tokens)+num_tokens)\n","        encoder = model.encoder\n","        decoder = model.decoder\n","\n","        if use_recur_pos:\n","            decoder.set_position_embedding(label_ids[0], tag_first)\n","\n","        _tokenizer = BartTokenizer.from_pretrained(bart_model)\n","        for token in tokenizer.unique_no_split_tokens:\n","            if token[:2] == '<<':\n","                index = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token))\n","                if len(index)>1:\n","                    raise RuntimeError(f\"{token} wrong split\")\n","                else:\n","                    index = index[0]\n","                assert index>=num_tokens, (index, num_tokens, token)\n","                indexes = _tokenizer.convert_tokens_to_ids(_tokenizer.tokenize(token[2:-2]))\n","                embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n","                for i in indexes[1:]:\n","                    embed += model.decoder.embed_tokens.weight.data[i]\n","                embed /= len(indexes)\n","                model.decoder.embed_tokens.weight.data[index] = embed\n","\n","        encoder = FBartEncoder(encoder)\n","        label_ids = sorted(label_ids)\n","        if decoder_type is None:\n","            assert copy_gate is False\n","            decoder = FBartDecoder(decoder, pad_token_id=tokenizer.pad_token_id, label_ids=label_ids)\n","        elif decoder_type =='avg_score':\n","            decoder = CaGFBartDecoder(decoder, pad_token_id=tokenizer.pad_token_id, label_ids=label_ids,\n","                                              use_encoder_mlp=use_encoder_mlp)\n","        else:\n","            raise RuntimeError(\"Unsupported feature.\")\n","\n","        return cls(encoder=encoder, decoder=decoder)\n","\n","    def prepare_state(self, src_tokens, src_seq_len=None, first=None, tgt_seq_len=None):\n","        encoder_outputs, encoder_mask, hidden_states = self.encoder(src_tokens, src_seq_len)\n","        src_embed_outputs = hidden_states[0]\n","        state = BartState(encoder_outputs, encoder_mask, src_tokens, first, src_embed_outputs)\n","        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n","        return state\n","\n","    def forward(self, src_tokens, tgt_tokens, src_seq_len, tgt_seq_len, first):\n","        \"\"\"\n","        :param torch.LongTensor src_tokens: source的token\n","        :param torch.LongTensor tgt_tokens: target的token\n","        :param torch.LongTensor first: 显示每个, bsz x max_word_len\n","        :param torch.LongTensor src_seq_len: src的长度\n","        :param torch.LongTensor tgt_seq_len: target的长度，默认用不上\n","        :return: {'pred': torch.Tensor}, 其中pred的shape为bsz x max_len x vocab_size\n","        \"\"\"\n","        state = self.prepare_state(src_tokens, src_seq_len, first, tgt_seq_len)\n","        decoder_output = self.decoder(tgt_tokens, state)\n","        if isinstance(decoder_output, torch.Tensor):\n","            return {'pred': decoder_output}\n","        elif isinstance(decoder_output, (tuple, list)):\n","            return {'pred': decoder_output[0]}\n","        else:\n","            raise TypeError(f\"Unsupported return type from Decoder:{type(self.decoder)}\")\n","\n","\n","\n","class BartState(State):\n","    def __init__(self, encoder_output, encoder_mask, src_tokens, first, src_embed_outputs):\n","        super().__init__(encoder_output, encoder_mask)\n","        self.past_key_values = None\n","        self.src_tokens = src_tokens\n","        self.first = first\n","        self.src_embed_outputs = src_embed_outputs\n","\n","    def reorder_state(self, indices: torch.LongTensor):\n","        super().reorder_state(indices)\n","        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n","        if self.first is not None:\n","            self.first = self._reorder_state(self.first, indices)\n","        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs, indices)\n","        if self.past_key_values is not None:\n","            new = []\n","            for layer in self.past_key_values:\n","                new_layer = {}\n","                print(layer[\"encoder_decoder\"].keys())\n","                for key1 in list(layer.keys()):\n","                    new_layer_ = {}\n","                    for key2 in list(layer[key1].keys()):\n","                        if layer[key1][key2] is not None:\n","                            layer[key1][key2] = self._reorder_state(layer[key1][key2], indices)\n","                            # print(key1, key2, layer[key1][key2].shape)\n","                        new_layer_[key2] = layer[key1][key2]\n","                    new_layer[key1] = new_layer_\n","                new.append(new_layer)\n","            self.past_key_values = new"]},{"cell_type":"markdown","metadata":{"id":"VEWncIejgMQ9"},"source":["#T5 code"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YuypYl29gMQ-","executionInfo":{"status":"ok","timestamp":1654964941876,"user_tz":-420,"elapsed":4763,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["# Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"PyTorch T5 model, ported from the fairseq repo.\"\"\"\n","import math\n","import random\n","import warnings\n","from typing import Dict, List, Optional, Tuple\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import Tensor, nn\n","from torch.nn import CrossEntropyLoss\n","\n","from transformers.modeling_t5 import *\n","\n","logger = logging.get_logger(__name__)\n","\n","_CONFIG_FOR_DOC = \"T5Config\"\n","_TOKENIZER_FOR_DOC = \"T5Tokenizer\"\n","\n","####################################################\n","# This dict contrains shortcut names and associated url\n","# for the pretrained weights provided with the models\n","####################################################\n","T5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n","    \"t5-small\",\n","    \"t5-base\",\n","    \"t5-large\",\n","    \"t5-3b\",\n","    \"t5-11b\",\n","    # See all T5 models at https://huggingface.co/models?filter=t5\n","]\n","\n","\n","####################################################\n","# This is a conversion method from TF 1.0 to PyTorch\n","# More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28\n","####################################################\n","def load_tf_weights_in_t5(model, config, tf_checkpoint_path):\n","    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n","    try:\n","        import re\n","\n","        import numpy as np\n","        import tensorflow as tf\n","    except ImportError:\n","        logger.error(\n","            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n","            \"https://www.tensorflow.org/install/ for installation instructions.\"\n","        )\n","        raise\n","    tf_path = os.path.abspath(tf_checkpoint_path)\n","    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n","    # Load weights from TF model\n","    init_vars = tf.train.list_variables(tf_path)\n","    names = []\n","    tf_weights = {}\n","    for name, shape in init_vars:\n","        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n","        array = tf.train.load_variable(tf_path, name)\n","        names.append(name)\n","        tf_weights[name] = array\n","\n","    for txt_name in names:\n","        name = txt_name.split(\"/\")\n","        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n","        # which are not required for using pretrained model\n","        if any(\n","            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n","            for n in name\n","        ):\n","            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n","            tf_weights.pop(txt_name, None)\n","            continue\n","        if \"_slot_\" in name[-1]:\n","            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n","            tf_weights.pop(txt_name, None)\n","            continue\n","        pointer = model\n","        array = tf_weights[txt_name]\n","        for m_name in name:\n","            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n","                scope_names = re.split(r\"_(\\d+)\", m_name)\n","            else:\n","                scope_names = [m_name]\n","            if scope_names[0] in [\"kernel\", \"scale\", \"embedding\"]:\n","                pointer = getattr(pointer, \"weight\")\n","            # elif scope_names[0] == 'scale':\n","            #     pointer = getattr(pointer, 'weight')\n","            # elif scope_names[0] == 'output_bias' or scope_names[0] == 'beta':\n","            #     pointer = getattr(pointer, 'bias')\n","            # elif scope_names[0] == 'squad':\n","            #     pointer = getattr(pointer, 'classifier')\n","            else:\n","                try:\n","                    pointer = getattr(pointer, scope_names[0])\n","                except AttributeError:\n","                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n","                    continue\n","            if len(scope_names) >= 2:\n","                num = int(scope_names[1])\n","                pointer = pointer[num]\n","        if scope_names[0] not in [\"kernel\", \"scale\", \"embedding\"]:\n","            pointer = getattr(pointer, \"weight\")\n","        if scope_names[0] != \"embedding\":\n","            logger.info(\"Transposing numpy weight of shape {} for {}\".format(array.shape, name))\n","            array = np.transpose(array)\n","        try:\n","            assert (\n","                pointer.shape == array.shape\n","            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n","        except AssertionError as e:\n","            e.args += (pointer.shape, array.shape)\n","            raise\n","        logger.info(\"Initialize PyTorch weight {}\".format(name))\n","        pointer.data = torch.from_numpy(array.astype(np.float32))\n","        tf_weights.pop(txt_name, None)\n","\n","    logger.info(\"Weights not copied to PyTorch model: {}\".format(\", \".join(tf_weights.keys())))\n","    # logger.info(\"Weights not copied to PyTorch model: {}\".format(', '.join(tf_weights.keys())))\n","    return model\n","\n","\n","####################################################\n","# PyTorch Models are constructed by sub-classing\n","# - torch.nn.Module for the layers and\n","# - PreTrainedModel for the models (it-self a sub-class of torch.nn.Module)\n","####################################################\n","\n","\n","class T5LayerNorm(nn.Module):\n","    def __init__(self, hidden_size, eps=1e-6):\n","        \"\"\"Construct a layernorm module in the T5 style\n","        No bias and no substraction of mean.\n","        \"\"\"\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(hidden_size))\n","        self.variance_epsilon = eps\n","\n","    def forward(self, x):\n","        # layer norm should always be calculated in float32\n","        variance = x.to(torch.float32).pow(2).mean(-1, keepdim=True)\n","        x = x / torch.sqrt(variance + self.variance_epsilon)\n","\n","        if self.weight.dtype == torch.float16:\n","            x = x.to(torch.float16)\n","        return self.weight * x\n","\n","\n","class T5DenseReluDense(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n","        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(self, hidden_states):\n","        h = self.wi(hidden_states)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        h = self.wo(h)\n","        return h\n","\n","\n","class T5LayerFF(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.DenseReluDense = T5DenseReluDense(config)\n","        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(self, hidden_states):\n","        norm_x = self.layer_norm(hidden_states)\n","        y = self.DenseReluDense(norm_x)\n","        layer_output = hidden_states + self.dropout(y)\n","        return layer_output\n","\n","\n","class T5Attention(nn.Module):\n","    def __init__(self, config: T5Config, has_relative_attention_bias=False, is_bidirectional=False):\n","        super().__init__()\n","        self.is_bidirectional = is_bidirectional\n","        self.is_decoder = config.is_decoder\n","        self.has_relative_attention_bias = has_relative_attention_bias\n","\n","        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n","        self.d_model = config.d_model\n","        self.d_kv = config.d_kv\n","        self.n_heads = config.num_heads\n","        self.dropout = config.dropout_rate\n","        self.inner_dim = self.n_heads * self.d_kv\n","\n","        # Mesh TensorFlow initialization to avoid scaling before softmax\n","        self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n","        self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)\n","        self.v = nn.Linear(self.d_model, self.inner_dim, bias=False)\n","        self.o = nn.Linear(self.inner_dim, self.d_model, bias=False)\n","\n","        if self.has_relative_attention_bias:\n","            self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n","        self.pruned_heads = set()\n","\n","    def prune_heads(self, heads):\n","        if len(heads) == 0:\n","            return\n","        heads, index = find_pruneable_heads_and_indices(heads, self.n_heads, self.d_kv, self.pruned_heads)\n","        # Prune linear layers\n","        self.q = prune_linear_layer(self.q, index)\n","        self.k = prune_linear_layer(self.k, index)\n","        self.v = prune_linear_layer(self.v, index)\n","        self.o = prune_linear_layer(self.o, index, dim=1)\n","        # Update hyper params\n","        self.n_heads = self.n_heads - len(heads)\n","        self.inner_dim = self.d_kv * self.n_heads\n","        self.pruned_heads = self.pruned_heads.union(heads)\n","\n","    @staticmethod\n","    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n","        \"\"\"\n","        Adapted from Mesh Tensorflow:\n","        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n","\n","        Translate relative position to a bucket number for relative attention.\n","        The relative position is defined as memory_position - query_position, i.e.\n","        the distance in tokens from the attending position to the attended-to\n","        position.  If bidirectional=False, then positive relative positions are\n","        invalid.\n","        We use smaller buckets for small absolute relative_position and larger buckets\n","        for larger absolute relative_positions.  All relative positions >=max_distance\n","        map to the same bucket.  All relative positions <=-max_distance map to the\n","        same bucket.  This should allow for more graceful generalization to longer\n","        sequences than the model has been trained on.\n","        Args:\n","            relative_position: an int32 Tensor\n","            bidirectional: a boolean - whether the attention is bidirectional\n","            num_buckets: an integer\n","            max_distance: an integer\n","        Returns:\n","            a Tensor with the same shape as relative_position, containing int32\n","            values in the range [0, num_buckets)\n","        \"\"\"\n","        ret = 0\n","        n = -relative_position\n","        if bidirectional:\n","            num_buckets //= 2\n","            ret += (n < 0).to(torch.long) * num_buckets  # mtf.to_int32(mtf.less(n, 0)) * num_buckets\n","            n = torch.abs(n)\n","        else:\n","            n = torch.max(n, torch.zeros_like(n))\n","        # now n is in the range [0, inf)\n","\n","        # half of the buckets are for exact increments in positions\n","        max_exact = num_buckets // 2\n","        is_small = n < max_exact\n","\n","        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n","        val_if_large = max_exact + (\n","            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n","        ).to(torch.long)\n","        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n","\n","        ret += torch.where(is_small, n, val_if_large)\n","        return ret\n","\n","    def compute_bias(self, qlen, klen):\n","        \"\"\" Compute binned relative position bias \"\"\"\n","        context_position = torch.arange(qlen, dtype=torch.long)[:, None]\n","        memory_position = torch.arange(klen, dtype=torch.long)[None, :]\n","        relative_position = memory_position - context_position  # shape (qlen, klen)\n","        rp_bucket = self._relative_position_bucket(\n","            relative_position,  # shape (qlen, klen)\n","            bidirectional=self.is_bidirectional,\n","            num_buckets=self.relative_attention_num_buckets,\n","        )\n","        rp_bucket = rp_bucket.to(self.relative_attention_bias.weight.device)\n","        values = self.relative_attention_bias(rp_bucket)  # shape (qlen, klen, num_heads)\n","        values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, qlen, klen)\n","        return values\n","\n","    def forward(\n","        self,\n","        input,\n","        mask=None,\n","        kv=None,\n","        position_bias=None,\n","        past_key_value=None,\n","        head_mask=None,\n","        query_length=None,\n","        use_cache=False,\n","        output_attentions=False,\n","    ):\n","        \"\"\"\n","        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n","        \"\"\"\n","        # Input is (bs, qlen, dim)\n","        # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n","        # past_key_value[0] is (bs, n_heads, q_len - 1, dim_per_head)\n","        bs, qlen, dim = input.size()\n","\n","        if past_key_value is not None:\n","            assert self.is_decoder is True, \"Encoder cannot cache past key value states\"\n","            assert (\n","                len(past_key_value) == 2\n","            ), \"past_key_value should have 2 past states: keys and values. Got {} past states\".format(\n","                len(past_key_value)\n","            )\n","            real_qlen = qlen + past_key_value[0].shape[2] if query_length is None else query_length\n","        else:\n","            real_qlen = qlen\n","\n","        if kv is None:\n","            klen = real_qlen\n","        else:\n","            klen = kv.size(1)\n","\n","        def shape(x):\n","            \"\"\"  projection \"\"\"\n","            return x.view(bs, -1, self.n_heads, self.d_kv).transpose(1, 2)\n","\n","        def unshape(x):\n","            \"\"\"  compute context \"\"\"\n","            return x.transpose(1, 2).contiguous().view(bs, -1, self.inner_dim)\n","\n","        q = shape(self.q(input))  # (bs, n_heads, qlen, dim_per_head)\n","\n","        if kv is None:\n","            k = shape(self.k(input))  # (bs, n_heads, qlen, dim_per_head)\n","            v = shape(self.v(input))  # (bs, n_heads, qlen, dim_per_head)\n","        elif past_key_value is None:\n","            k = v = kv\n","            k = shape(self.k(k))  # (bs, n_heads, qlen, dim_per_head)\n","            v = shape(self.v(v))  # (bs, n_heads, qlen, dim_per_head)\n","\n","        if past_key_value is not None:\n","            if kv is None:\n","                k_, v_ = past_key_value\n","                k = torch.cat([k_, k], dim=2)  # (bs, n_heads, klen, dim_per_head)\n","                v = torch.cat([v_, v], dim=2)  # (bs, n_heads, klen, dim_per_head)\n","            else:\n","                k, v = past_key_value\n","\n","        if self.is_decoder and use_cache is True:\n","            present_key_value_state = ((k, v),)\n","        else:\n","            present_key_value_state = (None,)\n","\n","        # (bs, n_heads, qlen, klen)\n","        scores = torch.matmul(\n","            q, k.transpose(3, 2)\n","        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", q, k), compatible with onnx op>9\n","\n","        if position_bias is None:\n","            if not self.has_relative_attention_bias:\n","                raise ValueError(\"No position_bias provided and no weights to compute position_bias\")\n","            position_bias = self.compute_bias(real_qlen, klen)\n","\n","            # if key and values are already calculated\n","            # we want only the last query position bias\n","            if past_key_value is not None:\n","                position_bias = position_bias[:, :, -qlen:, :]\n","\n","            if mask is not None:\n","                position_bias = position_bias + mask  # (bs, n_heads, qlen, klen)\n","\n","        scores += position_bias\n","        weights = F.softmax(scores.float(), dim=-1).type_as(scores)  # (bs, n_heads, qlen, klen)\n","        weights = F.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            weights = weights * head_mask\n","\n","        context = torch.matmul(weights, v)  # (bs, n_heads, qlen, dim_per_head)\n","        context = unshape(context)  # (bs, qlen, dim)\n","\n","        context = self.o(context)\n","\n","        outputs = (context,) + present_key_value_state\n","\n","        if output_attentions:\n","            outputs = outputs + (weights,)\n","        if self.has_relative_attention_bias:\n","            outputs = outputs + (position_bias,)\n","        return outputs\n","\n","\n","class T5LayerSelfAttention(nn.Module):\n","    def __init__(self, config, has_relative_attention_bias=False):\n","        super().__init__()\n","        self.SelfAttention = T5Attention(\n","            config, has_relative_attention_bias=has_relative_attention_bias, is_bidirectional=not config.is_decoder\n","        )\n","        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        position_bias=None,\n","        head_mask=None,\n","        past_key_value=None,\n","        use_cache=False,\n","        output_attentions=False,\n","    ):\n","        norm_x = self.layer_norm(hidden_states)\n","        attention_output = self.SelfAttention(\n","            norm_x,\n","            mask=attention_mask,\n","            position_bias=position_bias,\n","            head_mask=head_mask,\n","            past_key_value=past_key_value,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","        )\n","        y = attention_output[0]\n","        layer_output = hidden_states + self.dropout(y)\n","        outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n","        return outputs\n","\n","\n","class T5LayerCrossAttention(nn.Module):\n","    def __init__(self, config, has_relative_attention_bias=False):\n","        super().__init__()\n","        self.EncDecAttention = T5Attention(\n","            config, has_relative_attention_bias=has_relative_attention_bias, is_bidirectional=True\n","        )\n","        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        kv,\n","        attention_mask=None,\n","        position_bias=None,\n","        head_mask=None,\n","        past_key_value=None,\n","        use_cache=False,\n","        query_length=None,\n","        output_attentions=False,\n","    ):\n","        norm_x = self.layer_norm(hidden_states)\n","        attention_output = self.EncDecAttention(\n","            norm_x,\n","            mask=attention_mask,\n","            kv=kv,\n","            position_bias=position_bias,\n","            head_mask=head_mask,\n","            past_key_value=past_key_value,\n","            use_cache=use_cache,\n","            query_length=query_length,\n","            output_attentions=output_attentions,\n","        )\n","        y = attention_output[0]\n","        layer_output = hidden_states + self.dropout(y)\n","        outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n","        return outputs\n","\n","\n","class T5Block(nn.Module):\n","    def __init__(self, config, has_relative_attention_bias=False):\n","        super().__init__()\n","        self.is_decoder = config.is_decoder\n","        self.layer = nn.ModuleList()\n","        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n","        if self.is_decoder:\n","            self.layer.append(T5LayerCrossAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n","\n","        self.layer.append(T5LayerFF(config))\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        position_bias=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        encoder_decoder_position_bias=None,\n","        head_mask=None,\n","        past_key_value=None,\n","        use_cache=False,\n","        output_attentions=False,\n","    ):\n","\n","        if past_key_value is not None:\n","            assert self.is_decoder, \"Only decoder can use `past_key_values`\"\n","            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n","\n","            error_message = \"There should be {} past states. 2 (past / key) for self attention.{} Got {} past key / value states\".format(\n","                expected_num_past_key_values,\n","                \"2 (past / key) for cross attention\" if expected_num_past_key_values == 4 else \"\",\n","                len(past_key_value),\n","            )\n","            assert len(past_key_value) == expected_num_past_key_values, error_message\n","\n","            self_attn_past_key_value = past_key_value[:2]\n","            cross_attn_past_key_value = past_key_value[2:]\n","        else:\n","            self_attn_past_key_value, cross_attn_past_key_value = None, None\n","\n","        self_attention_outputs = self.layer[0](\n","            hidden_states,\n","            attention_mask=attention_mask,\n","            position_bias=position_bias,\n","            head_mask=head_mask,\n","            past_key_value=self_attn_past_key_value,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","        )\n","        hidden_states, present_key_value_state = self_attention_outputs[:2]\n","        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n","\n","        if self.is_decoder and encoder_hidden_states is not None:\n","            # the actual query length is unknown for cross attention\n","            # if using past key value states. Need to inject it here\n","            if present_key_value_state is not None:\n","                query_length = present_key_value_state[0].shape[2]\n","            else:\n","                query_length = None\n","\n","            cross_attention_outputs = self.layer[1](\n","                hidden_states,\n","                kv=encoder_hidden_states,\n","                attention_mask=encoder_attention_mask,\n","                position_bias=encoder_decoder_position_bias,\n","                head_mask=head_mask,\n","                past_key_value=cross_attn_past_key_value,\n","                query_length=query_length,\n","                use_cache=use_cache,\n","                output_attentions=output_attentions,\n","            )\n","            hidden_states = cross_attention_outputs[0]\n","            # Combine self attn and cross attn key value states\n","            if present_key_value_state is not None:\n","                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n","\n","            # Keep cross-attention outputs and relative position weights\n","            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n","\n","        # Apply Feed Forward layer\n","        hidden_states = self.layer[-1](hidden_states)\n","        outputs = (hidden_states,)\n","\n","        # Add attentions if we output them\n","        outputs = outputs + (present_key_value_state,) + attention_outputs\n","        return outputs  # hidden-states, present_key_value_states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n","\n","\n","class T5PreTrainedModel(PreTrainedModel):\n","    \"\"\"An abstract class to handle weights initialization and\n","    a simple interface for downloading and loading pretrained models.\n","    \"\"\"\n","\n","    config_class = T5Config\n","    load_tf_weights = load_tf_weights_in_t5\n","    base_model_prefix = \"transformer\"\n","\n","    @property\n","    def dummy_inputs(self):\n","        input_ids = torch.tensor(DUMMY_INPUTS)\n","        input_mask = torch.tensor(DUMMY_MASK)\n","        dummy_inputs = {\n","            \"decoder_input_ids\": input_ids,\n","            \"input_ids\": input_ids,\n","            \"decoder_attention_mask\": input_mask,\n","        }\n","        return dummy_inputs\n","\n","    def _init_weights(self, module):\n","        \"\"\" Initialize the weights \"\"\"\n","        factor = self.config.initializer_factor  # Used for testing weights initialization\n","        if isinstance(module, T5LayerNorm):\n","            module.weight.data.fill_(factor * 1.0)\n","        elif isinstance(module, (T5Model, T5ForConditionalGeneration)):\n","            # Mesh TensorFlow embeddings initialization\n","            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n","            module.shared.weight.data.normal_(mean=0.0, std=factor * 1.0)\n","        elif isinstance(module, T5DenseReluDense):\n","            # Mesh TensorFlow FF initialization\n","            # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n","            # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n","            module.wi.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n","            if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n","                module.wi.bias.data.zero_()\n","            module.wo.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n","            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n","                module.wo.bias.data.zero_()\n","        elif isinstance(module, T5Attention):\n","            # Mesh TensorFlow attention initialization to avoid scaling before softmax\n","            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n","            d_model = self.config.d_model\n","            d_kv = self.config.d_kv\n","            n_heads = self.config.num_heads\n","            module.q.weight.data.normal_(mean=0.0, std=factor * ((d_model * d_kv) ** -0.5))\n","            module.k.weight.data.normal_(mean=0.0, std=factor * (d_model ** -0.5))\n","            module.v.weight.data.normal_(mean=0.0, std=factor * (d_model ** -0.5))\n","            module.o.weight.data.normal_(mean=0.0, std=factor * ((n_heads * d_kv) ** -0.5))\n","            if module.has_relative_attention_bias:\n","                module.relative_attention_bias.weight.data.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n","\n","    def _shift_right(self, input_ids):\n","        decoder_start_token_id = self.config.decoder_start_token_id\n","        pad_token_id = self.config.pad_token_id\n","\n","        assert (\n","            decoder_start_token_id is not None\n","        ), \"self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id. See T5 docs for more information\"\n","\n","        # shift inputs to the right\n","        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n","        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n","        shifted_input_ids[..., 0] = decoder_start_token_id\n","\n","        assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n","        # replace possible -100 values in labels by `pad_token_id`\n","        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n","\n","        assert torch.all(shifted_input_ids >= 0).item(), \"Verify that `shifted_input_ids` has only positive values\"\n","\n","        return shifted_input_ids\n","\n","\n","class T5Stack(T5PreTrainedModel):\n","    def __init__(self, config, embed_tokens=None):\n","        super().__init__(config)\n","\n","        self.embed_tokens = embed_tokens\n","        self.is_decoder = config.is_decoder\n","\n","        self.block = nn.ModuleList(\n","            [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n","        )\n","        self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.embed_tokens\n","\n","    def get_output_embeddings(self):\n","        return self.embed_tokens\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.embed_tokens = new_embeddings\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        inputs_embeds=None,\n","        head_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n","            raise ValueError(\n","                f\"You cannot specify both {err_msg_prefix}inputs and {err_msg_prefix}inputs_embeds at the same time\"\n","            )\n","        elif input_ids is not None:\n","            input_shape = input_ids.size()\n","            input_ids = input_ids.view(-1, input_shape[-1])\n","        elif inputs_embeds is not None:\n","            input_shape = inputs_embeds.size()[:-1]\n","        else:\n","            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n","            raise ValueError(f\"You have to specify either {err_msg_prefix}inputs or {err_msg_prefix}inputs_embeds\")\n","\n","        if inputs_embeds is None:\n","            assert self.embed_tokens is not None, \"You have to intialize the model with valid token embeddings\"\n","            inputs_embeds = self.embed_tokens(input_ids)\n","\n","        batch_size, seq_length = input_shape\n","\n","        # required mask seq length can be calculated via length of past\n","        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n","\n","        if use_cache is True:\n","            assert self.is_decoder, \":obj:`use_cache` can only be set to `True` if {} is used as a decoder\".format(\n","                self\n","            )\n","\n","        if attention_mask is None:\n","            attention_mask = torch.ones(batch_size, mask_seq_length).to(inputs_embeds.device)\n","        if self.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:\n","            encoder_seq_length = encoder_hidden_states.shape[1]\n","            encoder_attention_mask = torch.ones(\n","                batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n","            )\n","\n","        # initialize past_key_values with `None` if past does not exist\n","        if past_key_values is None:\n","            past_key_values = [None] * len(self.block)\n","\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, inputs_embeds.device)\n","\n","        if self.is_decoder and encoder_attention_mask is not None:\n","            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n","        else:\n","            encoder_extended_attention_mask = None\n","\n","        # Prepare head mask if needed\n","        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n","        present_key_value_states = () if use_cache else None\n","        all_hidden_states = () if output_hidden_states else None\n","        all_attentions = () if output_attentions else None\n","        position_bias = None\n","        encoder_decoder_position_bias = None\n","\n","        hidden_states = self.dropout(inputs_embeds)\n","\n","        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","            layer_outputs = layer_module(\n","                hidden_states,\n","                attention_mask=extended_attention_mask,\n","                position_bias=position_bias,\n","                encoder_hidden_states=encoder_hidden_states,\n","                encoder_attention_mask=encoder_extended_attention_mask,\n","                encoder_decoder_position_bias=encoder_decoder_position_bias,\n","                head_mask=head_mask[i],\n","                past_key_value=past_key_value,\n","                use_cache=use_cache,\n","                output_attentions=output_attentions,\n","            )\n","            # layer_outputs is a tuple with:\n","            # hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n","            hidden_states, present_key_value_state = layer_outputs[:2]\n","\n","            if i == 0:\n","                # We share the position biases between the layers - the first layer store them\n","                # layer_outputs = hidden-states, key-value-states (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n","                position_bias = layer_outputs[3 if output_attentions else 2]\n","                if self.is_decoder and encoder_hidden_states is not None:\n","                    encoder_decoder_position_bias = layer_outputs[5 if output_attentions else 3]\n","            # append next layer key value states\n","            if use_cache:\n","                present_key_value_states = present_key_value_states + (present_key_value_state,)\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + (layer_outputs[2],)  # We keep only self-attention weights for now\n","\n","        hidden_states = self.final_layer_norm(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","\n","        # Add last layer\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(\n","                v\n","                for v in [hidden_states, present_key_value_states, all_hidden_states, all_attentions]\n","                if v is not None\n","            )\n","        return BaseModelOutputWithPast(\n","            last_hidden_state=hidden_states,\n","            past_key_values=present_key_value_states,\n","            hidden_states=all_hidden_states,\n","            attentions=all_attentions,\n","        )\n","\n","\n","T5_START_DOCSTRING = r\"\"\"\n","\n","    The T5 model was proposed in `Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n","    <https://arxiv.org/abs/1910.10683>`__ by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\n","    Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.\n","    It's an encoder decoder transformer pre-trained in a text-to-text denoising generative setting.\n","\n","    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n","    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n","    pruning heads etc.)\n","\n","    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass.\n","    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general\n","    usage and behavior.\n","\n","    Parameters:\n","        config (:class:`~transformers.T5Config`): Model configuration class with all the parameters of the model.\n","            Initializing with a config file does not load the weights associated with the model, only the configuration.\n","            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n","\"\"\"\n","\n","T5_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n","            Indices of input sequence tokens in the vocabulary.\n","            T5 is a model with relative position embeddings so you should be able to pad the inputs on both the right\n","            and the left.\n","\n","            Indices can be obtained using :class:`~transformers.T5Tokenizer`.\n","            See :meth:`transformers.PreTrainedTokenizer.encode` and\n","            :meth:`transformers.PreTrainedTokenizer.__call__` for detail.\n","\n","            To know more on how to prepare :obj:`input_ids` for pretraining take a look a\n","            `T5 Training <./t5.html#training>`__.\n","        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Mask to avoid performing attention on padding token indices.\n","            Mask values selected in ``[0, 1]``:\n","\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","\n","            `What are attention masks? <../glossary.html#attention-mask>`__\n","        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):\n","            Provide for sequence to sequence training. T5 uses the :obj:`pad_token_id` as the starting token for\n","            :obj:`decoder_input_ids` generation.\n","            If :obj:`past_key_values` is used, optionally only the last :obj:`decoder_input_ids` have to be input (see\n","            :obj:`past_key_values`).\n","\n","            To know more on how to prepare :obj:`decoder_input_ids` for pretraining take a look at\n","            `T5 Training <./t5.html#training>`__. If :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both\n","            unset, :obj:`decoder_input_ids` takes the value of :obj:`input_ids`.\n","        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`):\n","            Default behavior: generate a tensor that ignores pad tokens in :obj:`decoder_input_ids`. Causal mask will\n","            also be used by default.\n","        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`):\n","            Tuple consists of (:obj:`last_hidden_state`, :obj:`optional`: `hidden_states`, :obj:`optional`: `attentions`)\n","            :obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)` is a sequence of\n","            hidden states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n","        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n","            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n","\n","            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n","            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n","            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n","        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n","            Mask to nullify selected heads of the self-attention modules.\n","            Mask values selected in ``[0, 1]``:\n","\n","            - 1 indicates the head is **not masked**,\n","            - 0 indicates the head is **masked**.\n","\n","        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n","            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n","            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n","            vectors than the model's internal embedding lookup matrix.\n","        decoder_inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, target_sequence_length, hidden_size)`, `optional`):\n","            Optionally, instead of passing :obj:`decoder_input_ids` you can choose to directly pass an embedded\n","            representation.\n","            If :obj:`past_key_values` is used, optionally only the last :obj:`decoder_inputs_embeds` have to be input\n","            (see :obj:`past_key_values`).\n","            This is useful if you want more control over how to convert :obj:`decoder_input_ids` indices into\n","            associated vectors than the model's internal embedding lookup matrix.\n","\n","            If :obj:`decoder_input_ids` and :obj:`decoder_inputs_embeds` are both\n","            unset, :obj:`decoder_inputs_embeds` takes the value of :obj:`inputs_embeds`.\n","\n","        use_cache (:obj:`bool`, `optional`):\n","            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n","            decoding (see :obj:`past_key_values`).\n","\n","        output_attentions (:obj:`bool`, `optional`):\n","            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n","            tensors for more detail.\n","        output_hidden_states (:obj:`bool`, `optional`):\n","            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n","            more detail.\n","        return_dict (:obj:`bool`, `optional`):\n","            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n","\"\"\"\n","\n","\n","\n","class T5Model(T5PreTrainedModel):\n","    def __init__(self, config: T5Config):\n","        super().__init__(config)\n","        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n","\n","        encoder_config = copy.deepcopy(config)\n","        encoder_config.use_cache = False\n","        encoder_config.is_encoder_decoder = False\n","        self.encoder = T5Stack(encoder_config, self.shared)\n","\n","        decoder_config = copy.deepcopy(config)\n","        decoder_config.is_decoder = True\n","        decoder_config.is_encoder_decoder = False\n","        decoder_config.num_layers = config.num_decoder_layers\n","        self.decoder = T5Stack(decoder_config, self.shared)\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.shared\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.shared = new_embeddings\n","        self.encoder.set_input_embeddings(new_embeddings)\n","        self.decoder.set_input_embeddings(new_embeddings)\n","\n","    def get_encoder(self):\n","        return self.encoder\n","\n","    def get_decoder(self):\n","        return self.decoder\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"Prunes heads of the model.\n","        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","        See base class PreTrainedModel\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        decoder_input_ids=None,\n","        decoder_attention_mask=None,\n","        encoder_outputs=None,\n","        past_key_values=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        decoder_inputs_embeds=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        **kwargs,\n","    ):\n","        r\"\"\"\n","        Returns:\n","\n","        Example::\n","\n","            >>> from transformers import T5Tokenizer, T5Model\n","\n","            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","            >>> model = T5Model.from_pretrained('t5-small')\n","\n","            >>> input_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids  # Batch size 1\n","            >>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n","            >>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)\n","\n","            >>> last_hidden_states = outputs.last_hidden_state\n","        \"\"\"\n","        if \"decoder_past_key_value_states\" in kwargs:\n","            warnings.warn(\n","                \"The `decoder_past_key_value_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = kwargs.pop(\"decoder_past_key_value_states\")\n","        if \"decoder_past_key_values\" in kwargs:\n","            warnings.warn(\n","                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = kwargs.pop(\"decoder_past_key_values\")\n","        assert kwargs == {}, f\"Unexpected keyword arguments: {list(kwargs.keys())}.\"\n","\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # Encode if needed (training, first prediction pass)\n","        if encoder_outputs is None:\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","                head_mask=head_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","            )\n","\n","        hidden_states = encoder_outputs[0]\n","\n","        # Decode\n","        decoder_outputs = self.decoder(\n","            input_ids=decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            inputs_embeds=decoder_inputs_embeds,\n","            past_key_values=past_key_values,\n","            encoder_hidden_states=hidden_states,\n","            encoder_attention_mask=attention_mask,\n","            head_mask=head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        if not return_dict:\n","            return decoder_outputs + encoder_outputs\n","\n","        return Seq2SeqModelOutput(\n","            last_hidden_state=decoder_outputs.last_hidden_state,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            encoder_hidden_states=encoder_outputs.hidden_states,\n","            encoder_attentions=encoder_outputs.attentions,\n","        )\n","\n","\n","\n","class T5ForConditionalGeneration(T5PreTrainedModel):\n","    authorized_missing_keys = [r\"encoder\\.embed_tokens\\.weight\", r\"decoder\\.embed_tokens\\.weight\", r\"lm_head\\.weight\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.model_dim = config.d_model\n","\n","        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n","\n","        encoder_config = copy.deepcopy(config)\n","        encoder_config.use_cache = False\n","        encoder_config.is_encoder_decoder = False\n","        self.encoder = T5Stack(encoder_config, self.shared)\n","\n","        decoder_config = copy.deepcopy(config)\n","        decoder_config.is_decoder = True\n","        decoder_config.is_encoder_decoder = False\n","        decoder_config.num_layers = config.num_decoder_layers\n","        self.decoder = T5Stack(decoder_config, self.shared)\n","\n","        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.shared\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.shared = new_embeddings\n","        self.encoder.set_input_embeddings(new_embeddings)\n","        self.decoder.set_input_embeddings(new_embeddings)\n","\n","    def get_output_embeddings(self):\n","        return self.lm_head\n","\n","    def get_encoder(self):\n","        return self.encoder\n","\n","    def get_decoder(self):\n","        return self.decoder\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        decoder_input_ids=None,\n","        decoder_attention_mask=None,\n","        encoder_outputs=None,\n","        past_key_values=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        decoder_inputs_embeds=None,\n","        labels=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        **kwargs,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n","            Labels for computing the sequence classification/regression loss.\n","            Indices should be in :obj:`[-100, 0, ..., config.vocab_size - 1]`.\n","            All labels set to ``-100`` are ignored (masked), the loss is only\n","            computed for labels in ``[0, ..., config.vocab_size]``\n","        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n","            Used to hide legacy arguments that have been deprecated.\n","\n","        Returns:\n","\n","        Examples::\n","\n","            >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","            >>> tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","            >>> model = T5ForConditionalGeneration.from_pretrained('t5-small', return_dict=True)\n","\n","            >>> input_ids = tokenizer('The <extra_id_0> walks in <extra_id_1> park', return_tensors='pt').input_ids\n","            labels = tokenizer('<extra_id_0> cute dog <extra_id_1> the <extra_id_2> </s>', return_tensors='pt').input_ids\n","            >>> outputs = model(input_ids=input_ids, labels=labels)\n","            >>> loss = outputs.loss\n","            >>> logits = outputs.logits\n","\n","            >>> input_ids = tokenizer(\"summarize: studies have shown that owning a dog is good for you \", return_tensors=\"pt\").input_ids  # Batch size 1\n","            >>> outputs = model.generate(input_ids)\n","        \"\"\"\n","\n","        if \"lm_labels\" in kwargs:\n","            warnings.warn(\n","                \"The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n","                FutureWarning,\n","            )\n","            labels = kwargs.pop(\"lm_labels\")\n","        if \"decoder_past_key_value_states\" in kwargs:\n","            warnings.warn(\n","                \"The `decoder_past_key_value_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = kwargs.pop(\"decoder_past_key_value_states\")\n","        if \"decoder_past_key_values\" in kwargs:\n","            warnings.warn(\n","                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n","                FutureWarning,\n","            )\n","            past_key_values = kwargs.pop(\"decoder_past_key_values\")\n","        assert kwargs == {}, f\"Unexpected keyword arguments: {list(kwargs.keys())}.\"\n","\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # Encode if needed (training, first prediction pass)\n","        if encoder_outputs is None:\n","            # Convert encoder inputs in embeddings if needed\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","                head_mask=head_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n","            )\n","\n","        hidden_states = encoder_outputs[0]\n","\n","        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n","            # get decoder inputs from shifting lm labels to the right\n","            decoder_input_ids = self._shift_right(labels)\n","\n","        # If decoding with past key value states, only the last tokens\n","        # should be given as an input\n","        if past_key_values is not None:\n","            assert labels is None, \"Decoder should not use cached key value states when training.\"\n","            if decoder_input_ids is not None:\n","                decoder_input_ids = decoder_input_ids[:, -1:]\n","            if decoder_inputs_embeds is not None:\n","                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]\n","\n","        # Decode\n","        decoder_outputs = self.decoder(\n","            input_ids=decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            inputs_embeds=decoder_inputs_embeds,\n","            past_key_values=past_key_values,\n","            encoder_hidden_states=hidden_states,\n","            encoder_attention_mask=attention_mask,\n","            head_mask=head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = decoder_outputs[0]\n","        # Rescale output before projecting on vocab\n","        # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n","        sequence_output = sequence_output * (self.model_dim ** -0.5)\n","        lm_logits = self.lm_head(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n","            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n","\n","        if not return_dict:\n","            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return Seq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            decoder_attentions=decoder_outputs.attentions,\n","            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            encoder_hidden_states=encoder_outputs.hidden_states,\n","            encoder_attentions=encoder_outputs.attentions,\n","        )\n","\n","\n","    def prepare_inputs_for_generation(self, input_ids, past, attention_mask, use_cache, encoder_outputs, **kwargs):\n","\n","        # cut decoder_input_ids if past is used\n","        if past is not None:\n","            input_ids = input_ids[:, -1:]\n","\n","        return {\n","            \"decoder_input_ids\": input_ids,\n","            \"past_key_values\": past,\n","            \"encoder_outputs\": encoder_outputs,\n","            \"attention_mask\": attention_mask,\n","            \"use_cache\": use_cache,\n","        }\n","\n","    def _reorder_cache(self, past, beam_idx):\n","        # if decoder past is not included in output\n","        # speedy decoding is disabled and no need to reorder\n","        if past is None:\n","            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n","            return past\n","\n","        reordered_decoder_past = ()\n","        for layer_past_states in past:\n","            # get the correct batch idx from layer past batch dim\n","            # batch dim of `past` is at 2nd position\n","            reordered_layer_past_states = ()\n","            for layer_past_state in layer_past_states:\n","                # need to set correct `past` for each of the four key / value states\n","                reordered_layer_past_states = reordered_layer_past_states + (\n","                    layer_past_state.index_select(0, beam_idx),\n","                )\n","\n","            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n","            assert len(reordered_layer_past_states) == len(layer_past_states)\n","\n","            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n","        return reordered_decoder_past\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"absQrSqpgMRD","executionInfo":{"status":"ok","timestamp":1654964942440,"user_tz":-420,"elapsed":568,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["import torch\n","from transformers import T5Tokenizer\n","from fastNLP import seq_len_to_mask\n","from fastNLP.modules import Seq2SeqEncoder, Seq2SeqDecoder, State\n","import torch.nn.functional as F\n","from fastNLP.models import Seq2SeqModel\n","from torch import nn\n","import math\n","\n","\n","class FT5Encoder(Seq2SeqEncoder):\n","    def __init__(self, encoder):\n","        super().__init__()\n","        self.bart_encoder = encoder\n","\n","    def forward(self, src_tokens, src_seq_len):\n","        mask = seq_len_to_mask(src_seq_len, max_len=src_tokens.size(1))\n","        dict = self.bart_encoder(input_ids=src_tokens, attention_mask=mask, return_dict=True,\n","                                 output_hidden_states=True)\n","        encoder_outputs = dict.last_hidden_state\n","        hidden_states = dict.hidden_states\n","        return encoder_outputs, mask, hidden_states\n","\n","\n","class FT5Decoder(Seq2SeqDecoder):\n","    def __init__(self, decoder, pad_token_id, label_ids, use_encoder_mlp=True):\n","        super().__init__()\n","        self.decoder = decoder\n","        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n","        causal_mask = causal_mask.triu(diagonal=1)\n","        self.register_buffer('causal_masks', causal_mask.float())\n","        self.pad_token_id = pad_token_id\n","        self.label_start_id = label_ids[0]\n","        self.label_end_id = label_ids[-1]+1\n","        # 0th position is <s>, 1st position is </s>\n","        mapping = torch.LongTensor([0, 2]+sorted(label_ids, reverse=False))\n","        self.register_buffer('mapping', mapping)\n","        self.src_start_index = len(mapping)  # 加上一个\n","        hidden_size = decoder.embed_tokens.weight.size(1)\n","        self.encoder_attention = nn.Sequential(nn.Linear(hidden_size, hidden_size//2),\n","                                             nn.Dropout(0.3),\n","                                             nn.ReLU(),\n","                                             nn.Linear(hidden_size//2, 64))\n","        self.decoder_attention = nn.Sequential(nn.Linear(hidden_size, hidden_size//2),\n","                                             nn.Dropout(0.3),\n","                                             nn.ReLU(),\n","                                             nn.Linear(hidden_size//2, 64))\n","        if use_encoder_mlp:\n","            self.encoder_mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n","                                             nn.Dropout(0.3),\n","                                             nn.ReLU(),\n","                                             nn.Linear(hidden_size, hidden_size))\n","\n","    def forward(self, tokens, state):\n","        # bsz, max_len = tokens.size()\n","        encoder_outputs = state.encoder_output\n","        encoder_pad_mask = state.encoder_mask\n","\n","        first = state.first\n","        # eos is 1\n","        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n","        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n","\n","        # mapping to the BART token index\n","        mapping_token_mask = tokens.lt(self.src_start_index)  #\n","        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n","        tag_mapped_tokens = self.mapping[mapped_tokens]\n","\n","        src_tokens_index = tokens - self.src_start_index # bsz x num_src_token\n","        src_tokens_index = src_tokens_index.masked_fill(src_tokens_index.lt(0), 0)\n","        src_tokens = state.src_tokens\n","        if first is not None:\n","            src_tokens = src_tokens.gather(index=first, dim=1)\n","        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n","\n","        tokens = torch.where(mapping_token_mask, tag_mapped_tokens, word_mapped_tokens)\n","        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n","\n","        if self.training:\n","            tokens = tokens[:, :-1]\n","            decoder_pad_mask = tokens.eq(self.pad_token_id)\n","            dict = self.decoder(input_ids=tokens,\n","                                encoder_hidden_states=encoder_outputs,\n","                                decoder_padding_mask=decoder_pad_mask,\n","                                decoder_causal_mask=self.causal_masks[:tokens.size(1), :tokens.size(1)],\n","                                return_dict=True)\n","        else:\n","            past_key_values = state.past_key_values\n","            dict = self.decoder(input_ids=tokens,\n","                                encoder_hidden_states=encoder_outputs,\n","                                decoder_padding_mask=None,\n","                                decoder_causal_mask=None,\n","                                past_key_values=past_key_values,\n","                                use_cache=True,\n","                                return_dict=True)\n","        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size\n","        hidden_state = self.decoder_attention(hidden_state)\n","        if not self.training:\n","            state.past_key_values = dict.past_key_values\n","\n","        logits = hidden_state.new_full((hidden_state.size(0), hidden_state.size(1), self.src_start_index+src_tokens.size(-1)),\n","                                       fill_value=-1e24)\n","\n","        # first get the\n","        eos_scores = F.linear(hidden_state, self.decoder.embed_tokens.weight[2:3])  # bsz x max_len x 1\n","        tag_scores = F.linear(hidden_state, self.decoder.embed_tokens.weight[self.label_start_id:self.label_end_id])  # bsz x max_len x num_class\n","\n","        # bsz x max_word_len x hidden_size\n","        src_outputs = state.encoder_output\n","\n","        if hasattr(self, 'encoder_mlp'):\n","            src_outputs = self.encoder_mlp(src_outputs)\n","\n","        if first is not None:\n","            mask = first.eq(0)  # bsz x 1 x max_word_len, 为1的地方是padding\n","            src_outputs = src_outputs.gather(index=first.unsqueeze(2).repeat(1, 1, src_outputs.size(-1)), dim=1)\n","        else:\n","            mask = state.encoder_mask.eq(0)\n","        src_outputs = self.decoder_attention(src_outputs)\n","        mask = mask.unsqueeze(1).__or__(src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1))\n","        word_scores = torch.einsum('blh,bnh->bln', hidden_state, src_outputs)  # bsz x max_len x max_word_len\n","        word_scores = word_scores.masked_fill(mask, -1e32)\n","\n","        logits[:, :, 1:2] = eos_scores\n","        logits[:, :, 2:self.src_start_index] = tag_scores\n","        logits[:, :, self.src_start_index:] = word_scores\n","\n","        return logits\n","\n","    def decode(self, tokens, state):\n","        return self(tokens, state)[:, -1]\n","\n","\n","class CaGFT5Decoder(FT5Decoder):\n","    # Copy and generate,\n","    def __init__(self, decoder, pad_token_id, label_ids, use_encoder_mlp=False):\n","        super().__init__(decoder, pad_token_id, label_ids, use_encoder_mlp=use_encoder_mlp)\n","\n","    def forward(self, tokens, state):\n","        encoder_outputs = state.encoder_output\n","        encoder_pad_mask = state.encoder_mask\n","        first = state.first\n","\n","        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n","        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n","\n","        mapping_token_mask = tokens.lt(self.src_start_index)\n","        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n","        tag_mapped_tokens = self.mapping[mapped_tokens]\n","\n","        src_tokens_index = tokens - self.src_start_index # bsz x num_src_token\n","        src_tokens_index = src_tokens_index.masked_fill(src_tokens_index.lt(0), 0)\n","        src_tokens = state.src_tokens\n","        if first is not None:\n","            src_tokens = src_tokens.gather(index=first, dim=1)\n","        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n","\n","        tokens = torch.where(mapping_token_mask, tag_mapped_tokens, word_mapped_tokens)  # bsz x max_len\n","        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n","\n","        if self.training:\n","            tokens = tokens[:, :-1]\n","            decoder_pad_mask = tokens.eq(self.pad_token_id)  # decoder需要让pad位置为1\n","            dict = self.decoder(input_ids=tokens,\n","                                encoder_hidden_states=encoder_outputs,\n","                                return_dict=True)\n","        else:\n","            past_key_values = state.past_key_values\n","            dict = self.decoder(input_ids=tokens,\n","                                encoder_hidden_states=encoder_outputs,\n","                                past_key_values=past_key_values,\n","                                use_cache=True,\n","                                return_dict=True)\n","        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size\n","        hidden_state = self.decoder_attention(hidden_state)\n","        if not self.training:\n","            state.past_key_values = dict.past_key_values\n","\n","        logits = hidden_state.new_full((hidden_state.size(0), hidden_state.size(1), self.src_start_index+src_tokens.size(-1)),\n","                                       fill_value=-1e24)\n","\n","        eos_scores = F.linear(hidden_state, self.encoder_attention(self.decoder.embed_tokens.weight[2:3]))  # bsz x max_len x 1\n","        tag_scores = F.linear(hidden_state, self.encoder_attention(self.decoder.embed_tokens.weight[self.label_start_id:self.label_end_id]))  # bsz x max_len x num_class\n","\n","\n","        # bsz x max_bpe_len x hidden_size\n","        src_outputs = state.encoder_output\n","        if hasattr(self, 'encoder_mlp'):\n","            src_outputs = self.encoder_mlp(src_outputs)\n","\n","        if first is not None:\n","            mask = first.eq(0)  # bsz x 1 x max_word_len, 为1的地方是padding\n","            # bsz x max_word_len x hidden_size\n","            src_outputs = src_outputs.gather(index=first.unsqueeze(2).repeat(1, 1, src_outputs.size(-1)), dim=1)\n","        else:\n","            mask = state.encoder_mask.eq(0)\n","            # src_outputs = self.decoder.embed_tokens(src_tokens)\n","        mask = mask.unsqueeze(1)\n","\n","        src_outputs = self.encoder_attention(src_outputs)\n","        input_embed = self.encoder_attention(self.decoder.embed_tokens(src_tokens) ) # bsz x max_word_len x hidden_size\n","        \n","        word_scores = torch.einsum('blh,bnh->bln', hidden_state, src_outputs)  # bsz x max_len x max_word_len\n","        gen_scores = torch.einsum('blh,bnh->bln', hidden_state, input_embed)  # bsz x max_len x max_word_len\n","        word_scores = (gen_scores + word_scores)/2\n","        mask = mask.__or__(src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1))\n","        word_scores = word_scores.masked_fill(mask, -1e32)\n","\n","        logits[:, :, 1:2] = eos_scores\n","        logits[:, :, 2:self.src_start_index] = tag_scores\n","        logits[:, :, self.src_start_index:] = word_scores\n","\n","        return logits\n","\n","\n","class T5Seq2SeqModel(Seq2SeqModel):\n","    @classmethod\n","    def build_model(cls,t5_model, tokenizer, label_ids, decoder_type=None, copy_gate=False,\n","                    use_encoder_mlp=False, use_recur_pos=False, tag_first=False):\n","        model = T5Model.from_pretrained(t5_model)\n","        model.resize_token_embeddings(tokenizer.vocab_size)\n","        num_tokens, _ = model.encoder.embed_tokens.weight.shape\n","        print(num_tokens)\n","        model.resize_token_embeddings(len(tokenizer.unique_no_split_tokens)+num_tokens)\n","        encoder = model.encoder\n","        decoder = model.decoder\n","\n","        if use_recur_pos:\n","            decoder.set_position_embedding(label_ids[0], tag_first)\n","\n","        _tokenizer = T5Tokenizer.from_pretrained(t5_model)\n","        for token in tokenizer.unique_no_split_tokens:\n","            if token[:2] == '<<':\n","                index = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token))\n","                if len(index)>1:\n","                    raise RuntimeError(f\"{token} wrong split\")\n","                else:\n","                    index = index[0]\n","                assert index>=num_tokens, (index, num_tokens, token)\n","                indexes = _tokenizer.convert_tokens_to_ids(_tokenizer.tokenize(token[2:-2]))\n","                embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n","                for i in indexes[1:]:\n","                    embed += model.decoder.embed_tokens.weight.data[i]\n","                embed /= len(indexes)\n","                model.decoder.embed_tokens.weight.data[index] = embed\n","\n","        encoder = FT5Encoder(encoder)\n","        label_ids = sorted(label_ids)\n","        if decoder_type is None:\n","            assert copy_gate is False\n","            decoder = FT5Decoder(decoder, pad_token_id=tokenizer.pad_token_id, label_ids=label_ids)\n","        elif decoder_type =='avg_score':\n","            decoder = CaGFT5Decoder(decoder, pad_token_id=tokenizer.pad_token_id, label_ids=label_ids,\n","                                              use_encoder_mlp=use_encoder_mlp)\n","        else:\n","            raise RuntimeError(\"Unsupported feature.\")\n","\n","        return cls(encoder=encoder, decoder=decoder)\n","\n","    def prepare_state(self, src_tokens, src_seq_len=None, first=None, tgt_seq_len=None):\n","        encoder_outputs, encoder_mask, hidden_states = self.encoder(src_tokens, src_seq_len)\n","        src_embed_outputs = hidden_states[0]\n","        state = T5State(encoder_outputs, encoder_mask, src_tokens, first, src_embed_outputs)\n","        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n","        return state\n","\n","    def forward(self, src_tokens, tgt_tokens, src_seq_len, tgt_seq_len, first):\n","        \"\"\"\n","        :param torch.LongTensor src_tokens: source的token\n","        :param torch.LongTensor tgt_tokens: target的token\n","        :param torch.LongTensor first: 显示每个, bsz x max_word_len\n","        :param torch.LongTensor src_seq_len: src的长度\n","        :param torch.LongTensor tgt_seq_len: target的长度，默认用不上\n","        :return: {'pred': torch.Tensor}, 其中pred的shape为bsz x max_len x vocab_size\n","        \"\"\"\n","        state = self.prepare_state(src_tokens, src_seq_len, first, tgt_seq_len)\n","        decoder_output = self.decoder(tgt_tokens, state)\n","        if isinstance(decoder_output, torch.Tensor):\n","            return {'pred': decoder_output}\n","        elif isinstance(decoder_output, (tuple, list)):\n","            return {'pred': decoder_output[0]}\n","        else:\n","            raise TypeError(f\"Unsupported return type from Decoder:{type(self.decoder)}\")\n","\n","\n","\n","class T5State(State):\n","    def __init__(self, encoder_output, encoder_mask, src_tokens, first, src_embed_outputs):\n","        super().__init__(encoder_output, encoder_mask)\n","        self.past_key_values = None\n","        self.src_tokens = src_tokens\n","        self.first = first\n","        self.src_embed_outputs = src_embed_outputs\n","\n","    def reorder_state(self, indices: torch.LongTensor):\n","        super().reorder_state(indices)\n","        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n","        if self.first is not None:\n","            self.first = self._reorder_state(self.first, indices)\n","        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs, indices)\n","        if self.past_key_values is not None:\n","            self.past_key_values = self._reorder_state(self.past_key_values, indices)"]},{"cell_type":"markdown","metadata":{"id":"dVLiv9MiD_70"},"source":["#Model"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3OujcjF8D3qv","executionInfo":{"status":"ok","timestamp":1654964944293,"user_tz":-420,"elapsed":1856,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["\n","import torch\n","from torch import nn\n","from fastNLP.models.seq2seq_model import Seq2SeqModel\n","from fastNLP.modules.decoder.seq2seq_decoder import Seq2SeqDecoder, State\n","import torch.nn.functional as F\n","from fastNLP.core.utils import _get_model_device\n","from functools import partial\n","\n","\n","class SequenceGeneratorModel(nn.Module):\n","    \"\"\"\n","    用于封装Seq2SeqModel使其可以做生成任务\n","    \"\"\"\n","\n","    def __init__(self, seq2seq_model: Seq2SeqModel, bos_token_id, eos_token_id=None, max_length=30, max_len_a=0.0,\n","                 num_beams=1, do_sample=True,\n","                 repetition_penalty=1, length_penalty=1.0, pad_token_id=0,\n","                 restricter=None):\n","        \"\"\"\n","        :param Seq2SeqModel seq2seq_model: 序列到序列模型. 会使用seq2seq_model的decoder进行生成\n","        :param int,None bos_token_id: 句子开头的token id\n","        :param int,None eos_token_id: 句子结束的token id\n","        :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n","        :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n","        :param int num_beams: beam search的大小\n","        :param bool do_sample: 是否通过采样的方式生成\n","        :param float temperature: 只有在do_sample为True才有意义\n","        :param int top_k: 只从top_k中采样\n","        :param float top_p: 只从top_p的token中采样，nucles sample\n","        :param float repetition_penalty: 多大程度上惩罚重复的token\n","        :param float length_penalty: 对长度的惩罚，小于1鼓励长句，大于1鼓励短剧\n","        :param int pad_token_id: 当某句话生成结束之后，之后生成的内容用pad_token_id补充\n","        \"\"\"\n","        super().__init__()\n","        self.seq2seq_model = seq2seq_model\n","        self.restricter = restricter\n","        self.generator = SequenceGenerator(seq2seq_model.decoder, max_length=max_length, max_len_a=max_len_a,\n","                                           num_beams=num_beams,\n","                                           do_sample=do_sample,\n","                                           bos_token_id=bos_token_id,\n","                                           eos_token_id=eos_token_id,\n","                                           repetition_penalty=repetition_penalty, length_penalty=length_penalty,\n","                                           pad_token_id=pad_token_id,\n","                                           restricter=restricter)\n","\n","    def forward(self, src_tokens, tgt_tokens, src_seq_len=None, tgt_seq_len=None, first=None):\n","        \"\"\"\n","        透传调用seq2seq_model的forward\n","        :param torch.LongTensor src_tokens: bsz x max_len\n","        :param torch.LongTensor tgt_tokens: bsz x max_len'\n","        :param torch.LongTensor src_seq_len: bsz\n","        :param torch.LongTensor tgt_seq_len: bsz\n","        :return:\n","        \"\"\"\n","\n","        return self.seq2seq_model(src_tokens, tgt_tokens, src_seq_len, tgt_seq_len, first)\n","\n","    def predict(self, src_tokens, src_seq_len=None, first=None):\n","        \"\"\"\n","        给定source的内容，输出generate的内容\n","        :param torch.LongTensor src_tokens: bsz x max_len\n","        :param torch.LongTensor src_seq_len: bsz\n","        :return:\n","        \"\"\"\n","        state = self.seq2seq_model.prepare_state(src_tokens, src_seq_len, first)\n","        result = self.generator.generate(state)\n","        return {'pred': result}\n","\n","\n","r\"\"\"\n","\"\"\"\n","\n","__all__ = [\n","    'SequenceGenerator'\n","]\n","\n","\n","\n","class SequenceGenerator:\n","    \"\"\"\n","    给定一个Seq2SeqDecoder，decode出句子\n","    \"\"\"\n","    def __init__(self, decoder: Seq2SeqDecoder, max_length=20, max_len_a=0.0, num_beams=1,\n","                 do_sample=False, bos_token_id=None, eos_token_id=None,\n","                 repetition_penalty=1, length_penalty=1.0, pad_token_id=0, restricter=None):\n","        \"\"\"\n","        :param Seq2SeqDecoder decoder: Decoder对象\n","        :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n","        :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n","        :param int num_beams: beam search的大小\n","        :param bool do_sample: 是否通过采样的方式生成\n","        :param float temperature: 只有在do_sample为True才有意义\n","        :param int top_k: 只从top_k中采样\n","        :param float top_p: 只从top_p的token中采样，nucles sample\n","        :param int,None bos_token_id: 句子开头的token id\n","        :param int,None eos_token_id: 句子结束的token id\n","        :param float repetition_penalty: 多大程度上惩罚重复的token\n","        :param float length_penalty: 对长度的惩罚，小于1鼓励长句，大于1鼓励短剧\n","        :param int pad_token_id: 当某句话生成结束之后，之后生成的内容用pad_token_id补充\n","        \"\"\"\n","        self.generate_func = partial(greedy_generate, decoder=decoder, max_length=max_length, max_len_a=max_len_a,\n","                                     num_beams=num_beams,\n","                                     bos_token_id=bos_token_id, eos_token_id=eos_token_id,\n","                                     repetition_penalty=repetition_penalty,\n","                                     length_penalty=length_penalty, pad_token_id=pad_token_id,\n","                                     restricter=restricter)\n","        self.do_sample = do_sample\n","        self.max_length = max_length\n","        self.num_beams = num_beams\n","        self.bos_token_id = bos_token_id\n","        self.eos_token_id = eos_token_id\n","        self.repetition_penalty = repetition_penalty\n","        self.length_penalty = length_penalty\n","        self.decoder = decoder\n","        self.pad_token_id = pad_token_id\n","        self.restricter = restricter\n","        self.max_len_a = max_len_a\n","\n","    def set_new_generator(self, max_length=-1, max_len_a=-1, num_beams=-1,\n","                          repetition_penalty=-1, length_penalty=-1, restricter=-1):\n","        if max_length == -1:\n","            max_length = self.max_length\n","        if max_len_a == -1:\n","            max_len_a = self.max_len_a\n","        if num_beams == -1:\n","            num_beams = self.num_beams\n","        if repetition_penalty == -1:\n","            repetition_penalty = self.repetition_penalty\n","        if length_penalty == -1:\n","            length_penalty = self.length_penalty\n","        if restricter == -1:\n","            restricter = self.restricter\n","        self.generate_func = partial(greedy_generate, decoder=self.decoder, max_length=max_length, max_len_a=max_len_a,\n","                                     num_beams=num_beams,\n","                                     bos_token_id=self.bos_token_id, eos_token_id=self.eos_token_id,\n","                                     repetition_penalty=repetition_penalty,\n","                                     length_penalty=length_penalty, pad_token_id=self.pad_token_id,\n","                                     restricter=restricter)\n","\n","    @torch.no_grad()\n","    def generate(self, state, tokens=None):\n","        \"\"\"\n","        :param State state: encoder结果的State, 是与Decoder配套是用的\n","        :param torch.LongTensor,None tokens: batch_size x length, 开始的token\n","        :return: bsz x max_length' 生成的token序列。如果eos_token_id不为None, 每个sequence的结尾一定是eos_token_id\n","        \"\"\"\n","\n","        return self.generate_func(tokens=tokens, state=state)\n","\n","@torch.no_grad()\n","def greedy_generate(decoder, tokens=None, state=None, max_length=20, max_len_a=0.0, num_beams=1,\n","                    bos_token_id=None, eos_token_id=None, pad_token_id=0,\n","                    repetition_penalty=1, length_penalty=1.0, restricter=None):\n","    \"\"\"\n","    贪婪地搜索句子\n","    :param Decoder decoder: Decoder对象\n","    :param torch.LongTensor tokens: batch_size x len, decode的输入值，如果为None，则自动从bos_token_id开始生成\n","    :param State state: 应该包含encoder的一些输出。\n","    :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n","    :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n","    :param int num_beams: 使用多大的beam进行解码。\n","    :param int bos_token_id: 如果tokens传入为None，则使用bos_token_id开始往后解码。\n","    :param int eos_token_id: 结束的token，如果为None，则一定会解码到max_length这么长。\n","    :param int pad_token_id: pad的token id\n","    :param float repetition_penalty: 对重复出现的token多大的惩罚。\n","    :param float length_penalty: 对每个token（除了eos）按照长度进行一定的惩罚。\n","    :return:\n","    \"\"\"\n","    if num_beams == 1:\n","        token_ids = _no_beam_search_generate(decoder, tokens=tokens, state=state, max_length=max_length, max_len_a=max_len_a,\n","                                             bos_token_id=bos_token_id, eos_token_id=eos_token_id,\n","                                             repetition_penalty=repetition_penalty, length_penalty=length_penalty,\n","                                             pad_token_id=pad_token_id, restricter=restricter)\n","    else:\n","        token_ids = _beam_search_generate(decoder, tokens=tokens, state=state, max_length=max_length, max_len_a=max_len_a,\n","                                          num_beams=num_beams,\n","                                          bos_token_id=bos_token_id, eos_token_id=eos_token_id, do_sample=False,\n","                                          repetition_penalty=repetition_penalty, length_penalty=length_penalty,\n","                                          pad_token_id=pad_token_id, restricter=restricter)\n","\n","    return token_ids\n","\n","\n","def _no_beam_search_generate(decoder: Seq2SeqDecoder, state, tokens=None, max_length=20, max_len_a=0.0, bos_token_id=None,\n","                             eos_token_id=None,\n","                             repetition_penalty=1.0, length_penalty=1.0, pad_token_id=0,\n","                             restricter=None):\n","    device = _get_model_device(decoder)\n","    if tokens is None:\n","        if bos_token_id is None:\n","            raise RuntimeError(\"You have to specify either `tokens` or `bos_token_id`.\")\n","        batch_size = state.num_samples\n","        if batch_size is None:\n","            raise RuntimeError(\"Cannot infer the number of samples from `state`.\")\n","        tokens = torch.full([batch_size, 1], fill_value=bos_token_id, dtype=torch.long).to(device)\n","    batch_size = tokens.size(0)\n","    if state.num_samples:\n","        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n","\n","    if eos_token_id is None:\n","        _eos_token_id = -1\n","    else:\n","        _eos_token_id = eos_token_id\n","\n","    scores = decoder.decode(tokens=tokens, state=state)  # 主要是为了update state\n","    # 这里需要考虑如果在第一个位置就结束的情况\n","    # if _eos_token_id!=-1:\n","    #     scores[:, _eos_token_id] = -1e12\n","\n","    if restricter is not None:\n","        _, next_tokens = restricter(state, tokens, scores, num_beams=1)\n","    else:\n","        next_tokens = scores.argmax(dim=-1, keepdim=True)\n","    token_ids = torch.cat([tokens, next_tokens], dim=1)\n","    cur_len = token_ids.size(1)\n","    dones = token_ids.new_zeros(batch_size).eq(1).__or__(next_tokens.squeeze(1).eq(eos_token_id))\n","    # tokens = tokens[:, -1:]\n","\n","    if max_len_a!=0:\n","        # (bsz x num_beams, )\n","        if state.encoder_mask is not None:\n","            max_lengths = (state.encoder_mask.sum(dim=1).float()*max_len_a).long() + max_length\n","        else:\n","            max_lengths = tokens.new_full((tokens.size(0), ), fill_value=max_length, dtype=torch.long)\n","        real_max_length = max_lengths.max().item()\n","    else:\n","        real_max_length = max_length\n","        if state.encoder_mask is not None:\n","            max_lengths = state.encoder_mask.new_ones(state.encoder_mask.size(0)).long()*max_length\n","        else:\n","            max_lengths = tokens.new_full((tokens.size(0),), fill_value=max_length, dtype=torch.long)\n","\n","    while cur_len < real_max_length:\n","        scores = decoder.decode(tokens=token_ids, state=state)  # batch_size x vocab_size\n","\n","        if repetition_penalty != 1.0:\n","            token_scores = scores.gather(dim=1, index=token_ids)\n","            lt_zero_mask = token_scores.lt(0).float()\n","            ge_zero_mask = lt_zero_mask.eq(0).float()\n","            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n","            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n","\n","        if eos_token_id is not None and length_penalty != 1.0:\n","            token_scores = scores / cur_len ** length_penalty  # batch_size x vocab_size\n","            eos_mask = scores.new_ones(scores.size(1))\n","            eos_mask[eos_token_id] = 0\n","            eos_mask = eos_mask.unsqueeze(0).eq(1)\n","            scores = scores.masked_scatter(eos_mask, token_scores)  # 也即除了eos，其他词的分数经过了放大/缩小\n","\n","        if restricter is not None:\n","            _, next_tokens = restricter(state, token_ids, scores, 1)\n","        else:\n","            next_tokens = scores.argmax(dim=-1, keepdim=True)\n","        next_tokens = next_tokens.squeeze(-1)\n","\n","        # 如果已经达到对应的sequence长度了，就直接填为eos了\n","        if _eos_token_id!=-1:\n","            next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len+1), _eos_token_id)\n","        next_tokens = next_tokens.masked_fill(dones, pad_token_id)  # 对已经搜索完成的sample做padding\n","        tokens = next_tokens.unsqueeze(1)\n","\n","        token_ids = torch.cat([token_ids, tokens], dim=-1)  # batch_size x max_len\n","\n","        end_mask = next_tokens.eq(_eos_token_id)\n","        dones = dones.__or__(end_mask)\n","        cur_len += 1\n","\n","        if dones.min() == 1:\n","            break\n","\n","    # if eos_token_id is not None:\n","    #     tokens.scatter(index=max_lengths[:, None], dim=1, value=eos_token_id)  # 将最大长度位置设置为eos\n","    # if cur_len == max_length:\n","    #     token_ids[:, -1].masked_fill_(~dones, eos_token_id)  # 若到最长长度仍未到EOS，则强制将最后一个词替换成eos\n","    return token_ids\n","\n","\n","def _beam_search_generate(decoder: Seq2SeqDecoder, tokens=None, state=None, max_length=20, max_len_a=0.0, num_beams=4,\n","                          bos_token_id=None, eos_token_id=None, do_sample=True,\n","                          repetition_penalty=1.0, length_penalty=None, pad_token_id=0,\n","                          restricter=None) -> torch.LongTensor:\n","    assert do_sample is False\n","    # 进行beam search\n","    device = _get_model_device(decoder)\n","    if tokens is None:\n","        if bos_token_id is None:\n","            raise RuntimeError(\"You have to specify either `tokens` or `bos_token_id`.\")\n","        batch_size = state.num_samples\n","        if batch_size is None:\n","            raise RuntimeError(\"Cannot infer the number of samples from `state`.\")\n","        tokens = torch.full([batch_size, 1], fill_value=bos_token_id, dtype=torch.long).to(device)\n","    batch_size = tokens.size(0)\n","    if state.num_samples:\n","        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n","\n","    if eos_token_id is None:\n","        _eos_token_id = -1\n","    else:\n","        _eos_token_id = eos_token_id\n","\n","    scores = decoder.decode(tokens=tokens, state=state)  # 这里要传入的是整个句子的长度\n","    # 这里需要考虑如果在第一个位置就结束的情况\n","    # if _eos_token_id!=-1:\n","    #     scores[:, _eos_token_id] = -1e12\n","    vocab_size = scores.size(1)\n","    assert vocab_size >= num_beams, \"num_beams should be smaller than the number of vocabulary size.\"\n","\n","    scores = F.log_softmax(scores, dim=-1)  # (batch_size, vocab_size)\n","    # 得到(batch_size, num_beams), (batch_size, num_beams)\n","    # TODO 把限制写到这个位置, 加1是因为需要考虑输出就是eos的情况\n","    if restricter is not None:\n","        _next_scores, _next_tokens = restricter(state, tokens, scores, num_beams+1)\n","    else:\n","        # 是bsz x (num_beams+1)大小的东西\n","        _next_scores, _next_tokens = torch.topk(scores, num_beams+1, dim=1, largest=True, sorted=True)\n","\n","    # 根据index来做顺序的调转\n","    indices = torch.arange(batch_size, dtype=torch.long).to(device)\n","    indices = indices.repeat_interleave(num_beams)\n","    state.reorder_state(indices)\n","    tokens = tokens.index_select(dim=0, index=indices)  # batch_size * num_beams x length\n","\n","    # if hasattr(state, 'tgt_seq_len'):  # TODO 应该需要删除\n","    #     max_lengths = state.tgt_seq_len\n","    #     real_max_length = max_lengths.max().item()\n","    if max_len_a!=0:\n","        # (bsz x num_beams, )\n","        if state.encoder_mask is not None:\n","            max_lengths = (state.encoder_mask.sum(dim=1).float()*max_len_a).long() + max_length\n","        else:\n","            max_lengths = tokens.new_full((batch_size*num_beams, ), fill_value=max_length, dtype=torch.long)\n","        real_max_length = max_lengths.max().item()\n","    else:\n","        real_max_length = max_length\n","        if state.encoder_mask is not None:\n","            max_lengths = state.encoder_mask.new_ones(state.encoder_mask.size(0)).long()*max_length\n","        else:\n","            max_lengths = tokens.new_full((batch_size*num_beams,), fill_value=max_length, dtype=torch.long)\n","    hypos = [\n","        BeamHypotheses(num_beams, real_max_length, length_penalty, early_stopping=False) for _ in range(batch_size)\n","    ]\n","\n","    not_eos_mask = _next_tokens.ne(_eos_token_id)  # 为1的地方不是eos\n","    keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # 为1的地方需要保留\n","    keep_mask = not_eos_mask.__and__(keep_mask)  # 为1的地方是需要进行下一步search的\n","\n","    next_tokens = _next_tokens.masked_select(keep_mask).view(batch_size, num_beams)  # 这是真的接下来要继续的\n","    next_scores = _next_scores.masked_select(keep_mask).view(batch_size, num_beams)\n","\n","    rows, cols = not_eos_mask.eq(0)[:, :num_beams].nonzero(as_tuple=True)\n","\n","    if len(rows)>0:  # 说明有的开头就结束了\n","        for row, col in zip(rows.tolist(), cols.tolist()):\n","            _token = torch.cat([tokens[row*num_beams], _next_tokens[row, col:col+1]], dim=0)\n","            hypos[row].add(_token.clone(), _next_scores[row, col].item())\n","\n","    # 记录生成好的token (batch_size', cur_len)\n","    token_ids = torch.cat([tokens, next_tokens.view(-1, 1)], dim=-1)\n","    dones = [False] * batch_size\n","\n","    beam_scores = next_scores.view(-1)  # batch_size * num_beams\n","\n","    #  用来记录已经生成好的token的长度\n","    cur_len = token_ids.size(1)\n","\n","    # 0, num_beams, 2*num_beams, ...\n","    batch_inds_with_numbeams_interval = (torch.arange(batch_size) * num_beams).view(-1, 1).to(token_ids)\n","\n","    while cur_len < real_max_length:\n","        scores = decoder.decode(token_ids, state)  # (bsz x num_beams, vocab_size)\n","        if repetition_penalty != 1.0:\n","            token_scores = scores.gather(dim=1, index=token_ids)\n","            lt_zero_mask = token_scores.lt(0).float()\n","            ge_zero_mask = lt_zero_mask.eq(0).float()\n","            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n","            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n","\n","        if _eos_token_id!=-1:\n","            max_len_eos_mask = max_lengths.eq(cur_len+1)\n","            eos_scores = scores[:, _eos_token_id]\n","            # 如果已经达到最大长度，就把eos的分数加大\n","            scores[:, _eos_token_id] = torch.where(max_len_eos_mask, eos_scores+1e32, eos_scores)\n","\n","        scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n","        _scores = scores + beam_scores[:, None]  # (batch_size * num_beams, vocab_size)\n","        _scores = _scores.view(batch_size, -1)  # (batch_size, num_beams*vocab_size)\n","        # TODO 把限制加到这个位置\n","        if restricter is not None:\n","            next_scores, ids = restricter(state, token_ids, _scores, 2 * num_beams)\n","        else:\n","            next_scores, ids = torch.topk(_scores, 2 * num_beams, dim=1, largest=True, sorted=True)  # (bsz, 2*num_beams)\n","        from_which_beam = ids // vocab_size  # (batch_size, 2*num_beams)\n","        next_tokens = ids % vocab_size  # (batch_size, 2*num_beams)\n","\n","        #  接下来需要组装下一个batch的结果。\n","        #  需要选定哪些留下来\n","        # next_scores, sorted_inds = next_scores.sort(dim=-1, descending=True)\n","        # next_tokens = next_tokens.gather(dim=1, index=sorted_inds)\n","        # from_which_beam = from_which_beam.gather(dim=1, index=sorted_inds)\n","\n","        not_eos_mask = next_tokens.ne(_eos_token_id)  # 为1的地方不是eos\n","        keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # 为1的地方需要保留\n","        keep_mask = not_eos_mask.__and__(keep_mask)  # 为1的地方是需要进行下一步search的\n","\n","        _next_tokens = next_tokens.masked_select(keep_mask).view(-1, 1)\n","        _from_which_beam = from_which_beam.masked_select(keep_mask).view(batch_size, num_beams)  # 上面的token是来自哪个beam\n","        _next_scores = next_scores.masked_select(keep_mask).view(batch_size, num_beams)\n","        beam_scores = _next_scores.view(-1)\n","\n","        flag = True\n","        if cur_len+1 == real_max_length:\n","            eos_batch_idx = torch.arange(batch_size).to(next_tokens).repeat_interleave(repeats=num_beams, dim=0)\n","            eos_beam_ind = torch.arange(num_beams).to(token_ids).repeat(batch_size)  # 表示的是indice\n","            eos_beam_idx = from_which_beam[:, :num_beams].reshape(-1)  # 表示的是从哪个beam获取得到的\n","        else:\n","            # 将每个batch中在num_beam内的序列添加到结束中, 为1的地方需要结束了\n","            effective_eos_mask = next_tokens[:, :num_beams].eq(_eos_token_id)  # batch_size x num_beams\n","            if effective_eos_mask.sum().gt(0):\n","                eos_batch_idx, eos_beam_ind = effective_eos_mask.nonzero(as_tuple=True)\n","                # 是由于from_which_beam是 (batch_size, 2*num_beams)的，所以需要2*num_beams\n","                eos_beam_idx = eos_batch_idx * num_beams * 2 + eos_beam_ind\n","                eos_beam_idx = from_which_beam.view(-1)[eos_beam_idx]  # 获取真实的从哪个beam获取的eos\n","            else:\n","                flag = False\n","\n","        if flag:\n","            _token_ids = torch.cat([token_ids, _next_tokens], dim=-1)\n","            for batch_idx, beam_ind, beam_idx in zip(eos_batch_idx.tolist(), eos_beam_ind.tolist(),\n","                                                     eos_beam_idx.tolist()):\n","                if not dones[batch_idx]:\n","                    score = next_scores[batch_idx, beam_ind].item()\n","                    # 之后需要在结尾新增一个eos\n","                    if _eos_token_id!=-1:\n","                        hypos[batch_idx].add(_token_ids[batch_idx * num_beams + beam_idx, :cur_len].clone(), score)\n","                    else:\n","                        hypos[batch_idx].add(_token_ids[batch_idx * num_beams + beam_idx].clone(), score)\n","\n","        # 更改state状态, 重组token_ids\n","        reorder_inds = (batch_inds_with_numbeams_interval + _from_which_beam).view(-1)  # flatten成一维\n","        state.reorder_state(reorder_inds)\n","        # 重新组织token_ids的状态\n","        token_ids = torch.cat([token_ids.index_select(index=reorder_inds, dim=0), _next_tokens], dim=-1)\n","\n","        for batch_idx in range(batch_size):\n","            dones[batch_idx] = dones[batch_idx] or hypos[batch_idx].is_done(next_scores[batch_idx, 0].item()) or \\\n","                               max_lengths[batch_idx*num_beams]==cur_len+1\n","\n","        cur_len += 1\n","\n","        if all(dones):\n","            break\n","\n","    # select the best hypotheses\n","    tgt_len = token_ids.new_zeros(batch_size)\n","    best = []\n","\n","    for i, hypotheses in enumerate(hypos):\n","        best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n","        # 把上面替换为非eos的词替换回eos\n","        if _eos_token_id!=-1:\n","            best_hyp = torch.cat([best_hyp, best_hyp.new_ones(1)*_eos_token_id])\n","        tgt_len[i] = len(best_hyp)\n","        best.append(best_hyp)\n","\n","    # generate target batch\n","    decoded = token_ids.new_zeros(batch_size, tgt_len.max().item()).fill_(pad_token_id)\n","    for i, hypo in enumerate(best):\n","        decoded[i, :tgt_len[i]] = hypo\n","\n","    return decoded\n","\n","\n","class BeamHypotheses(object):\n","    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n","        \"\"\"\n","        Initialize n-best list of hypotheses.\n","        \"\"\"\n","        self.max_length = max_length - 1  # ignoring bos_token\n","        self.length_penalty = length_penalty\n","        self.early_stopping = early_stopping\n","        self.num_beams = num_beams\n","        self.hyp = []\n","        self.worst_score = 1e9\n","\n","    def __len__(self):\n","        \"\"\"\n","        Number of hypotheses in the list.\n","        \"\"\"\n","        return len(self.hyp)\n","\n","    def add(self, hyp, sum_logprobs):\n","        \"\"\"\n","        Add a new hypothesis to the list.\n","        \"\"\"\n","        score = sum_logprobs / len(hyp) ** self.length_penalty\n","        if len(self) < self.num_beams or score > self.worst_score:\n","            self.hyp.append((score, hyp))\n","            if len(self) > self.num_beams:\n","                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n","                del self.hyp[sorted_scores[0][1]]\n","                self.worst_score = sorted_scores[1][0]\n","            else:\n","                self.worst_score = min(score, self.worst_score)\n","\n","    def is_done(self, best_sum_logprobs):\n","        \"\"\"\n","        If there are enough hypotheses and that none of the hypotheses being generated\n","        can become better than the worst one in the heap, then we are done with this sentence.\n","        \"\"\"\n","        if len(self) < self.num_beams:\n","            return False\n","        elif self.early_stopping:\n","            return True\n","        else:\n","            return self.worst_score >= best_sum_logprobs / self.max_length ** self.length_penalty"]},{"cell_type":"markdown","metadata":{"id":"PE8Y8i_gt9rg"},"source":["#LOSS and METRIC"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Jpp_mQ5wuAyl","executionInfo":{"status":"ok","timestamp":1654964944294,"user_tz":-420,"elapsed":10,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["from fastNLP import LossBase\n","import torch.nn.functional as F\n","from fastNLP import seq_len_to_mask\n","\n","\n","class Seq2SeqLoss(LossBase):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def get_loss(self, tgt_tokens, tgt_seq_len, pred):\n","        \"\"\"\n","        :param tgt_tokens: bsz x max_len, [sos, tokens, eos]\n","        :param pred: bsz x max_len-1 x vocab_size\n","        :return:\n","        \"\"\"\n","        tgt_seq_len = tgt_seq_len - 1\n","        mask = seq_len_to_mask(tgt_seq_len, max_len=tgt_tokens.size(1) - 1).eq(0)\n","        tgt_tokens = tgt_tokens[:, 1:].masked_fill(mask, -100)\n","        loss = F.cross_entropy(target=tgt_tokens, input=pred.transpose(1, 2))\n","        return loss"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"aUQYJUKfuBn3","executionInfo":{"status":"ok","timestamp":1654964944295,"user_tz":-420,"elapsed":10,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["from fastNLP import MetricBase\n","from fastNLP.core.metrics import _compute_f_pre_rec\n","from collections import Counter\n","\n","\n","class Seq2SeqSpanMetric(MetricBase):\n","    def __init__(self, eos_token_id, num_labels, opinion_first=True):\n","        super(Seq2SeqSpanMetric, self).__init__()\n","        self.eos_token_id = eos_token_id\n","        self.num_labels = num_labels\n","        self.word_start_index = num_labels + 2  # +2, shift for sos and eos\n","\n","        self.quad_fp = 0\n","        self.quad_tp = 0\n","        self.quad_fn = 0\n","        self.em = 0\n","        self.invalid = 0\n","        self.total = 0\n","        assert opinion_first is False, \"Current metric only supports aspect first\"\n","\n","        self.opinin_first = opinion_first\n","\n","    def evaluate(self, target_span, pred, tgt_tokens):\n","        self.total += pred.size(0)\n","        pred_eos_index = pred.flip(dims=[1]).eq(self.eos_token_id).cumsum(dim=1).long()\n","        target_eos_index = tgt_tokens.flip(dims=[1]).eq(self.eos_token_id).cumsum(dim=1).long()\n","\n","        pred = pred[:, 1:]  # delete </s>\n","        tgt_tokens = tgt_tokens[:, 1:]\n","        pred_seq_len = pred_eos_index.flip(dims=[1]).eq(pred_eos_index[:, -1:]).sum(dim=1)  # bsz\n","        pred_seq_len = (pred_seq_len - 2).tolist()\n","        target_seq_len = target_eos_index.flip(dims=[1]).eq(target_eos_index[:, -1:]).sum(dim=1)  # bsz\n","        target_seq_len = (target_seq_len - 2).tolist()\n","        pred_spans = []\n","        for i, (ts, ps) in enumerate(zip(target_span, pred.tolist())):\n","            em = 0\n","            ps = ps[:pred_seq_len[i]]\n","            if pred_seq_len[i] == target_seq_len[i]:\n","                em = int(\n","                    tgt_tokens[i, :target_seq_len[i]].eq(pred[i, :target_seq_len[i]]).sum().item() == target_seq_len[i])\n","            self.em += em\n","            invalid = 0\n","            pairs = []\n","            cur_pair = []\n","            if len(ps):\n","                count = 0\n","                for index, j in enumerate(ps):\n","                    if j < self.word_start_index:\n","                        cur_pair.append(j)\n","                        if count<2:\n","                            count+=1\n","                        else:\n","                            if len(cur_pair) != 7 or cur_pair[0] > cur_pair[1] or cur_pair[4] > cur_pair[5]:\n","                                invalid = 1\n","                            else:\n","                                pairs.append(tuple(cur_pair))\n","                            cur_pair = []\n","                            count=0\n","                    else:\n","                        cur_pair.append(j)\n","            pred_spans.append(pairs.copy())\n","            self.invalid += invalid\n","\n","            ts = set([tuple(t) for t in ts])\n","            ps = set(pairs)\n","            for p in list(ps):\n","                if p in ts:\n","                    ts.remove(p)\n","                    self.quad_tp += 1\n","                else:\n","                    self.quad_fp += 1\n","\n","            self.quad_fn += len(ts)\n","\n","    def get_metric(self, reset=True):\n","        res = {}\n","        f, pre, rec = _compute_f_pre_rec(1, self.quad_tp, self.quad_fn, self.quad_fp)\n","\n","        res['quad_f'] = round(f, 4)*100\n","        res['quad_rec'] = round(rec, 4)*100\n","        res['quad_pre'] = round(pre, 4)*100\n","\n","        res['em'] = round(self.em / self.total, 4)\n","        res['invalid'] = round(self.invalid / self.total, 4)\n","        if reset:\n","            self.quad_fp = 0\n","            self.quad_tp = 0\n","            self.quad_fn = 0\n","            self.em = 0\n","            self.invalid = 0\n","            self.total = 0\n","        return res\n","\n","\n","def _compute_tp_fn_fp(ps, ts):\n","    ps = ps.copy()\n","    tp = 0\n","    fp = 0\n","    fn = 0\n","    if isinstance(ts, set):\n","        ts = {key: 1 for key in list(ts)}\n","    if isinstance(ps, set):\n","        ps = {key: 1 for key in list(ps)}\n","    for key in ts.keys():\n","        t_num = ts[key]\n","        if key not in ps:\n","            p_num = 0\n","        else:\n","            p_num = ps[key]\n","        tp += min(p_num, t_num)\n","        fp += max(p_num - t_num, 0)\n","        fn += max(t_num - p_num, 0)\n","        if key in ps:\n","            ps.pop(key)\n","    fp += sum(ps.values())\n","    return tp, fn, fp"]},{"cell_type":"markdown","metadata":{"id":"Zd3VqYYYuLc3"},"source":["#UTILS"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"GvIcO41euLVM","executionInfo":{"status":"ok","timestamp":1654964944295,"user_tz":-420,"elapsed":9,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["import numpy as np\n","\n","\n","def get_max_len_max_len_a(data_bundle, max_len=10):\n","    \"\"\"\n","    :param data_bundle:\n","    :param max_len:\n","    :return:\n","    \"\"\"\n","    max_len_a = -1\n","    for name, ds in data_bundle.iter_datasets():\n","        if name=='train':continue\n","        src_seq_len = np.array(ds.get_field('src_seq_len').content)\n","        tgt_seq_len = np.array(ds.get_field('tgt_seq_len').content)\n","        _len_a = round(max(np.maximum(tgt_seq_len - max_len+2, 0)/src_seq_len), 1)\n","\n","        if _len_a>max_len_a:\n","            max_len_a = _len_a\n","\n","    return max_len, max_len_a\n","\n","\n","def get_num_parameters(model):\n","    num_param = 0\n","    for name, param in model.named_parameters():\n","        num_param += np.prod(param.size())\n","    print(f\"The number of parameters is {num_param}\")"]},{"cell_type":"markdown","metadata":{"id":"eLT2o2uRM_1v"},"source":["#Train"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jGJdZwREM_Ff","executionInfo":{"status":"ok","timestamp":1654965480704,"user_tz":-420,"elapsed":391,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["import sys\n","sys.path.append('../')\n","import os\n","if 'p' in os.environ:\n","    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['p']\n","    # os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","from fastNLP import Trainer,Tester\n","from fastNLP import BucketSampler, GradientClipCallback, cache_results, WarmupCallback\n","from fastNLP import FitlogCallback\n","from fastNLP.core.sampler import SortedSampler\n","import fitlog\n","from torch import optim\n","\n","# fitlog.debug()\n","lr = 5e-5\n","n_epochs = 50\n","batch_size = 4\n","num_beams = 4\n","dataset_name = 'pengb/16res'\n","opinion_first = False\n","length_penalty = 1.0\n","\n","decoder_type = 'avg_score'\n","bart_name = 'facebook/bart-base'\n","t5_name = 't5-base'\n","use_encoder_mlp = 1"]},{"cell_type":"markdown","source":["##BART pipeline"],"metadata":{"id":"cS81fe29pPeQ"}},{"cell_type":"code","execution_count":58,"metadata":{"id":"s7DvIWAbQOCX","executionInfo":{"status":"ok","timestamp":1654960703589,"user_tz":-420,"elapsed":375,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["demo = False\n","if demo:\n","    cache_fn = f\"caches/data_{bart_name}_{dataset_name}_{opinion_first}_demo.pt\"\n","else:\n","    cache_fn = f\"caches/data_{bart_name}_{dataset_name}_{opinion_first}.pt\""]},{"cell_type":"code","execution_count":59,"metadata":{"id":"VRfiNhwJQN89","executionInfo":{"status":"ok","timestamp":1654960704109,"user_tz":-420,"elapsed":2,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["@cache_results(cache_fn, _refresh=False)\n","def get_data():\n","    pipe = BartBPEABSAPipe(tokenizer=bart_name, opinion_first=opinion_first)\n","    data_bundle = pipe.process_from_file(f'/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/json', demo=demo)\n","    return data_bundle, pipe.tokenizer, pipe.mapping2id, pipe.mapping2targetid"]},{"cell_type":"markdown","source":["##T5 pipeline"],"metadata":{"id":"0pJJk3AIpTK-"}},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"status":"ok","timestamp":1654964944297,"user_tz":-420,"elapsed":10,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}},"id":"UIQXp6gSpTK-"},"outputs":[],"source":["demo = False\n","if demo:\n","    cache_fn = f\"caches/data_{t5_name}_{dataset_name}_{opinion_first}_demo.pt\"\n","else:\n","    cache_fn = f\"caches/data_{t5_name}_{dataset_name}_{opinion_first}.pt\""]},{"cell_type":"code","execution_count":16,"metadata":{"id":"42CBUyHGpTK_","executionInfo":{"status":"ok","timestamp":1654964944298,"user_tz":-420,"elapsed":11,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["@cache_results(cache_fn, _refresh=False)\n","def get_data():\n","    pipe = T5BPEABSAPipe(tokenizer=t5_name, opinion_first=opinion_first)\n","    data_bundle = pipe.process_from_file(f'/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/json', demo=demo)\n","    return data_bundle, pipe.tokenizer, pipe.mapping2id, pipe.mapping2targetid"]},{"cell_type":"markdown","source":["##Continue"],"metadata":{"id":"Mxq7wIkRp3PM"}},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQGhrUXuR1Vz","outputId":"0a974c4e-0ad0-4ce1-d89b-eb0838978fe3","executionInfo":{"status":"ok","timestamp":1654964944718,"user_tz":-420,"elapsed":430,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Read cache from caches/data_t5-base_pengb/16res_False.pt.\n","The number of tokens in tokenizer  32100\n"]}],"source":["data_bundle, tokenizer, mapping2id, mapping2targetid = get_data()\n","max_len = 10\n","max_len_a = {\n","    'penga/14lap': 0.9,\n","    'penga/14res': 1,\n","    'penga/15res': 1.2,\n","    'penga/16res': 0.9,\n","    'pengb/14lap': 1.1,\n","    'pengb/14res': 1.2,\n","    'pengb/15res': 0.9,\n","    'pengb/16res': 1.2\n","}[dataset_name]\n","\n","print(\"The number of tokens in tokenizer \", tokenizer.vocab_size)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"kqv9619xJBEa","executionInfo":{"status":"ok","timestamp":1654964944719,"user_tz":-420,"elapsed":27,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["idtarget2map = {v: k for k, v in mapping2targetid.items()}"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"IBh3BFTWnkTX","executionInfo":{"status":"ok","timestamp":1654964944720,"user_tz":-420,"elapsed":17,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["bos_token_id = 0  #\n","eos_token_id = 1  #\n","label_ids = list(mapping2id.values())\n","vocab_size = len(tokenizer)\n"]},{"cell_type":"markdown","source":["##BART Model"],"metadata":{"id":"g76Vh4BDqFNb"}},{"cell_type":"code","execution_count":63,"metadata":{"id":"C4oB7u5hbfj7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654960767848,"user_tz":-420,"elapsed":12476,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}},"outputId":"76783ed7-db43-40c0-ae31-73cd831aad95"},"outputs":[{"output_type":"stream","name":"stdout","text":["FUCL\n","50301 50306\n"]}],"source":["\n","model = BartSeq2SeqModel.build_model(bart_name, tokenizer, label_ids=label_ids, decoder_type=decoder_type,\n","                                     copy_gate=False, use_encoder_mlp=use_encoder_mlp, use_recur_pos=False)\n","print(vocab_size, model.decoder.decoder.embed_tokens.weight.data.size(0))\n","model = SequenceGeneratorModel(model, bos_token_id=bos_token_id,\n","                               eos_token_id=eos_token_id,\n","                               max_length=max_len, max_len_a=max_len_a,num_beams=num_beams, do_sample=False,\n","                               repetition_penalty=1, length_penalty=length_penalty, pad_token_id=eos_token_id,\n","                               restricter=None)"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"FJz0cJszWZP1","executionInfo":{"status":"ok","timestamp":1654960711188,"user_tz":-420,"elapsed":547,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["model=torch.load(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/model/best_SequenceGeneratorModel_quad_f_2022-04-19-09-02-39-319227\")"]},{"cell_type":"markdown","source":["##T5 Model"],"metadata":{"id":"ZYf0Jry2qJ_a"}},{"cell_type":"code","source":["model = T5Seq2SeqModel.build_model(t5_name, tokenizer, label_ids=label_ids, decoder_type=decoder_type,\n","                                     copy_gate=False, use_encoder_mlp=use_encoder_mlp, use_recur_pos=False)\n","print(vocab_size, model.decoder.decoder.embed_tokens.weight.data.size(0))\n","model = SequenceGeneratorModel(model, bos_token_id=bos_token_id,\n","                               eos_token_id=eos_token_id,\n","                               max_length=max_len, max_len_a=max_len_a,num_beams=num_beams, do_sample=False,\n","                               repetition_penalty=1, length_penalty=length_penalty, pad_token_id=eos_token_id,\n","                               restricter=None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdHRsEXOqNBv","executionInfo":{"status":"ok","timestamp":1654964953406,"user_tz":-420,"elapsed":7011,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}},"outputId":"340fd99d-7d9a-4e9b-c3a7-5609bca409d1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of T5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["32100\n","32136 32239\n"]}]},{"cell_type":"code","source":["model = torch.load(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/model_T5/best_SequenceGeneratorModel_quad_f_2022-06-11-09-11-47-933097\")"],"metadata":{"id":"20dzKPoKPFNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{"id":"7iC0zaq2bYVv","executionInfo":{"status":"ok","timestamp":1654964953407,"user_tz":-420,"elapsed":17,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["\n","import torch\n","if torch.cuda.is_available():\n","    # device = list([i for i in range(torch.cuda.device_count())])\n","    device = 'cuda'\n","else:\n","    device = 'cpu'\n","\n","parameters = []\n","params = {'lr':lr, 'weight_decay':1e-2}\n","params['params'] = [param for name, param in model.named_parameters() if not ('bart_encoder' in name or 'bart_decoder' in name)]\n","parameters.append(params)\n","\n","params = {'lr':lr, 'weight_decay':1e-2}\n","params['params'] = []\n","for name, param in model.named_parameters():\n","    if ('bart_encoder' in name or 'bart_decoder' in name) and not ('layernorm' in name or 'layer_norm' in name):\n","        params['params'].append(param)\n","parameters.append(params)\n","\n","params = {'lr':lr, 'weight_decay':0}\n","params['params'] = []\n","for name, param in model.named_parameters():\n","    if ('bart_encoder' in name or 'bart_decoder' in name) and ('layernorm' in name or 'layer_norm' in name):\n","        params['params'].append(param)\n","parameters.append(params)\n","\n","optimizer = optim.AdamW(parameters)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"hEmyntdObt0i","executionInfo":{"status":"ok","timestamp":1654964953409,"user_tz":-420,"elapsed":11,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[],"source":["\n","callbacks = []\n","callbacks.append(GradientClipCallback(clip_value=5, clip_type='value'))\n","callbacks.append(WarmupCallback(warmup=0.01, schedule='linear'))\n","callbacks.append(FitlogCallback(data_bundle.get_dataset('test')))\n","\n","sampler = None\n","# sampler = ConstTokenNumSampler('src_seq_len', max_token=1000)\n","sampler = BucketSampler(seq_len_field_name='src_seq_len')\n","metric = Seq2SeqSpanMetric(eos_token_id, num_labels=len(label_ids), opinion_first=opinion_first)\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8251,"status":"ok","timestamp":1654964962218,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"},"user_tz":-420},"id":"TIlie3ekbzBK","outputId":"d3096b10-0eb4-4950-ab11-25cc9eae96ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["input fields after batch(if batch size is 2):\n","\ttgt_tokens: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 9]) \n","\tsrc_tokens: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 26]) \n","\tsrc_seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n","\ttgt_seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n","target fields after batch(if batch size is 2):\n","\ttgt_tokens: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 9]) \n","\ttarget_span: (1)type:numpy.ndarray (2)dtype:int64, (3)shape:(2, 1, 7) \n","\ttgt_seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n","\n"]}],"source":["fitlog.set_log_dir('/content/caches')\n","\n","model_path = \"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/model_T5\"\n","\n","\n","trainer = Trainer(train_data=data_bundle.get_dataset('train'), model=model, optimizer=optimizer,\n","                  loss=Seq2SeqLoss(),\n","                  batch_size=batch_size, sampler=sampler, drop_last=False, update_every=1,\n","                  num_workers=2, n_epochs=n_epochs, print_every=1,\n","                  dev_data=data_bundle.get_dataset('dev'), metrics=metric, metric_key='quad_f',\n","                  validate_every=-1, save_path=model_path, use_tqdm=True, device=device,\n","                  callbacks=callbacks, check_code_level=0, test_use_tqdm=False,\n","                  test_sampler=SortedSampler('src_seq_len'), dev_batch_size=batch_size)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429,"referenced_widgets":["0a16d15e65034125abe16dbc35931f2f","ff4c617ea47048daa597ee6a867fdff7","09b4a53201b240cbb7a604cfa4846465","0fe6665e4328452ea5d210dcbb1a167c","a3ea6eb801aa43a8a0e8cf7b392b5d64","eb29e58344c6433e9ff7d3d7aa8f1781","40f659454b8041d5815a2f74c9993f88","8085cc95d9fb40ce8a568ea4e2044333","d326b8c28d10484ba7f8ad8b40fc99a0","07ba7233c4164972b9066365cb4c9beb","a990ebb72b2547c7acf859f50dc55f54"]},"id":"8M33ue43b09m","outputId":"65c2d86e-4511-41ec-e029-ccb1b74f93f2","executionInfo":{"status":"error","timestamp":1654965115426,"user_tz":-420,"elapsed":153254,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["training epochs started 2022-06-11-16-29-23-309834\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/36700 [00:00<?, ?it/s, loss:{0:<6.5f}]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a16d15e65034125abe16dbc35931f2f"}},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-115d3b7a01d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastNLP/core/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, load_best_model, on_exception, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mon_exception\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCallbackException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mon_exception\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastNLP/core/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, load_best_model, on_exception, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastNLP/core/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;31m# ================= mini-batch end ==================== #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_every\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 在epoch结束之后的evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                     \u001b[0meval_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                     eval_str = \"Evaluation on dev at Epoch {}/{}. Step:{}/{}: \".format(epoch, self.n_epochs, self.step,\n\u001b[1;32m    761\u001b[0m                                                                                        self.n_steps)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastNLP/core/trainer.py\u001b[0m in \u001b[0;36m_do_validation\u001b[0;34m(self, epoch, step)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_valid_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mis_better_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastNLP/core/tester.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                                                    non_blocking=self.pin_memory)\n\u001b[1;32m    187\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                             \u001b[0mpred_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                                 raise TypeError(f\"The return value of {_get_func_signature(self._predict_func)} \"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastNLP/core/tester.py\u001b[0m in \u001b[0;36m_data_forward\u001b[0;34m(self, func, x)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;34mr\"\"\"A forward pass of the model. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_func_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-30ad8c3ece6b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, src_tokens, src_seq_len, first)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m     66\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-30ad8c3ece6b>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, state, tokens)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \"\"\"\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-30ad8c3ece6b>\u001b[0m in \u001b[0;36mgreedy_generate\u001b[0;34m(decoder, tokens, state, max_length, max_len_a, num_beams, bos_token_id, eos_token_id, pad_token_id, repetition_penalty, length_penalty, restricter)\u001b[0m\n\u001b[1;32m    178\u001b[0m                                           \u001b[0mbos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                                           \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                                           pad_token_id=pad_token_id, restricter=restricter)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-30ad8c3ece6b>\u001b[0m in \u001b[0;36m_beam_search_generate\u001b[0;34m(decoder, tokens, state, max_length, max_len_a, num_beams, bos_token_id, eos_token_id, do_sample, repetition_penalty, length_penalty, pad_token_id, restricter)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mreal_max_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bsz x num_beams, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mtoken_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-42b08e5425f0>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, tokens, state)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-42b08e5425f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, state)\u001b[0m\n\u001b[1;32m    171\u001b[0m                                 \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                                 return_dict=True)\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m  \u001b[0;31m# bsz x max_len x hidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-56ddcc11eaef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             )\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# layer_outputs is a tuple with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-56ddcc11eaef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         )\n\u001b[1;32m    523\u001b[0m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent_key_value_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-56ddcc11eaef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-56ddcc11eaef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask, kv, position_bias, past_key_value, head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, klen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, qlen, klen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1818\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1819\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 204.00 MiB (GPU 0; 14.76 GiB total capacity; 10.60 GiB already allocated; 71.75 MiB free; 13.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["trainer.train(load_best_model=False)\n"]},{"cell_type":"code","source":["!nvidia-smi --gpu-reset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-JgM8nfnv92","executionInfo":{"status":"ok","timestamp":1654965463352,"user_tz":-420,"elapsed":343,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"}},"outputId":"9f0eba64-6337-4e02-9fcc-0e98f005efe1"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 00000000:00:04.0 is currently in use by another process.\n","\n","1 device is currently being used by one or more other processes (e.g., Fabric Manager, CUDA application, graphics application such as an X server, or a monitoring application such as another instance of nvidia-smi). Please first kill all processes using this device and all compute applications running in the system.\n"]}]},{"cell_type":"markdown","metadata":{"id":"apnS0DwNQA58"},"source":["#Testing"]},{"cell_type":"code","source":["model.to(\"cuda\")"],"metadata":{"id":"Qkh9ez0c3vB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69,"referenced_widgets":["85873f08bfea4900ba4fcba519f182b5","ff16507299e84d8298c6060afbe56b3d","d8e7882f57dd41f7beb6e17a44741d7b","1660b201b9d94ca2b11c5bb23ba79466","dc98ba02df374ad1b7a0cb45db6c5b86","5e4c3d1864514bb697be62524962cc6a","3607437e511843b5aecff7b8697ee530","37c820ff2556469796f2f47dce677537","62b39d95f4924e67805d82407c029134","e25b8f5ce6944de9a38d44a10001b811","72982d228826466bbb979f8ff40afc11"]},"executionInfo":{"elapsed":206456,"status":"ok","timestamp":1652978510276,"user":{"displayName":"Duy Hoang","userId":"17457647490907439684"},"user_tz":-420},"id":"GNg64gFoOl8c","outputId":"a6d0c0e8-3668-49a2-fc4a-18b651b00196"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/164 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85873f08bfea4900ba4fcba519f182b5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluate data in 206.34 seconds!\n","[tester] \n","Seq2SeqSpanMetric: quad_f=39.660000000000004, quad_rec=38.06, quad_pre=41.39, em=0.337, invalid=0.0098\n"]}],"source":["tester = Tester(data_bundle.get_dataset('test'), model, metrics=metric,batch_size=batch_size)\n","eval_results = tester.test()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":561,"status":"ok","timestamp":1650365201789,"user":{"displayName":"Duy Hoang","userId":"17457647490907439684"},"user_tz":-420},"id":"1iiNWGhMQqY_","outputId":"84e7b9ec-9d8b-4715-e6cf-7cfbbfd4aae7"},"outputs":[{"name":"stdout","output_type":"stream","text":["816\n","326\n","2934\n"]}],"source":["print(len(data_bundle.get_dataset('test')))\n","print(len(data_bundle.get_dataset('dev')))\n","print(len(data_bundle.get_dataset('train')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0m8qQTv0FWw"},"outputs":[],"source":["pipe = BartBPEABSAPipe(tokenizer=bart_name, opinion_first=opinion_first)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["c20a73fba3b24a43be44f2850fab7649","6457a12f852c44b08f3036602c63f16a","4136225689e5409884824fc429f5e7f3","9fb40918075e4e348a58d7085a6a63a8","6b36e5eca83c4a2690db9e6b6f2957de","0717b9587d6147ff92c5ca6f8c76b587","ff6ef87977f14bf2b304e4aee56cf23b","744b54834b054ee59f87efa48fb41125","78db39eb97f74af5975905b9d1eb73ae","76966a47b81b4ed5a3776813ab08245b","dfd20a5297a3459187b6ac3a748d213a","41f4e6d4b55d4f90b0e66789020d5d7d","5bcb4ae6fb214f0fa10caca0fa511e62","b597d114bbfe46e3afac52e69f7c882c","d4b88231b8e44643a59cf46d02d183a7","4a3bd9546645452ca31b0c123919ed1c","209cd6349b91451290ad043ec3cb0569","98930f6252d940459f34ff8d24f4e292","1d6a7f3c2bd842c0a4a19a55342a519b","96edc6f0003741d39fa4a2e0caeb0526","3e8d2faa05754e929dc6cb255e53e16e","44f4b531a0f044d1aa15a97c5b202a8d"]},"executionInfo":{"elapsed":2737,"status":"ok","timestamp":1651161752800,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"},"user_tz":-420},"id":"iLgsk_kg1SdQ","outputId":"8c80938c-e74d-4846-ee1c-5c1ccb8753b7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Pre. tgt. for `test`:   0%|          | 0/64 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20a73fba3b24a43be44f2850fab7649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Pre. tgt. for `train`:   0%|          | 0/467 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f4e6d4b55d4f90b0e66789020d5d7d"}},"metadata":{}}],"source":["test_bundle = pipe.process_from_file(\"/content/drive/MyDrive/Sentiment Analysis/data/Laptop-ACOS/json/Implicit Explicit\")"]},{"cell_type":"markdown","metadata":{"id":"htqgAHuC50-3"},"source":["##IAIO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69,"referenced_widgets":["ba5b6487ccc24e59b84f97e48265f8e9","8af9caf917a441549b296fe08f4f0cff","a379af210f3443bc8bf16d9c6b9c441f","4fb8ff5a7fd047c581cb979b19c8bde2","e5c3650bc35d43f6a5ae442aada70996","ecb179a2fbf548759c1cb675f99654f6","1473c3dc47104a2e9c1d60a225ded991","4a952c78440445ddb3a045287b8d94d5","e05f09c5fd4549d099ed8c873da407b4","af5e3130f0024bfd93bf9dc105fc61a5","5ada919ba49f4affac3a46037800edb9"]},"executionInfo":{"elapsed":16108,"status":"ok","timestamp":1651161604943,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"},"user_tz":-420},"id":"zvpcgAO-12Is","outputId":"fda66896-b4eb-4818-e42f-0c2ac96df16c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/13 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba5b6487ccc24e59b84f97e48265f8e9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluate data in 16.18 seconds!\n","[tester] \n","Seq2SeqSpanMetric: quad_f=29.79, quad_rec=31.819999999999997, quad_pre=28.000000000000004, em=0.2969, invalid=0.0\n"]}],"source":["tester = Tester(test_bundle.get_dataset('test'), model, metrics=metric,batch_size=batch_size)\n","eval_results = tester.test()"]},{"cell_type":"markdown","metadata":{"id":"wUIufUfy53L4"},"source":["##EAEO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69,"referenced_widgets":["7f00d63532aa47a2bbfd3175eb6289c3","d83758beab174ae6b242dfab4f5d4e58","d18a0849c9834eee8931fc8ab566bb0a","1e9c6ea2d3d246b4b4f343d18f241671","5d04210ff8bb4a3088e624059b5fa43c","1e6fc2321fa7453598f2816a27976d5b","1fe4a50c9b02422d945ffa0095414cec","6a5d01d81680469b91057465c7921a67","0ef6d1941d9944ef9270e438c4f48ac6","2c79d553517941dcb4b54ca31c558ea2","cb6dfa452eeb4551a117bf9bf3a16a57"]},"executionInfo":{"elapsed":115117,"status":"ok","timestamp":1651161720020,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"},"user_tz":-420},"id":"fvBlQYuH19rw","outputId":"f810ec1b-eef8-4481-9bbe-601ffac3881f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/94 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f00d63532aa47a2bbfd3175eb6289c3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluate data in 114.85 seconds!\n","[tester] \n","Seq2SeqSpanMetric: quad_f=39.85, quad_rec=39.85, quad_pre=39.85, em=0.3276, invalid=0.0086\n"]}],"source":["tester = Tester(test_bundle.get_dataset('train'), model, metrics=metric,batch_size=batch_size)\n","eval_results = tester.test()"]},{"cell_type":"markdown","metadata":{"id":"8R292Wnd55pz"},"source":["##EAIO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"elapsed":13,"status":"error","timestamp":1651161752801,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"},"user_tz":-420},"id":"-AlquKtq1-NR","outputId":"105ca497-d7d6-465a-c6e7-cc4c150340fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["DataBundle do NOT have DataSet named dev. It should be one of dict_keys(['test', 'train']).\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-e124636ac606>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtester\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastNLP/io/data_bundle.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     74\u001b[0m                         \u001b[0;34mf'It should be one of {self.datasets.keys()}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"DataBundle do NOT have DataSet named dev. It should be one of dict_keys(['test', 'train']).\""]}],"source":["tester = Tester(test_bundle.get_dataset('dev'), model, metrics=metric,batch_size=batch_size)\n","eval_results = tester.test()"]},{"cell_type":"markdown","metadata":{"id":"YVsH5aS86GrU"},"source":["##IAEO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89,"referenced_widgets":["1d7a21ece00640a7a217f121b61af8d0","67e1c05200524ce0be5d0269ebc41de8","65d19bd7586c4169896f1482400f4351","11569ca5ca8c4e7d989151571b182faa","42625e8e3bf0424e9bba5e906740d5fd","619ef086d9b245c5bf476af4fa1068d2","41787032210a4beeb7c7ed3e03190c63","a555f9ce52204e1eba737cccda0c0216","b26e52a7360e447fa3a9b302fd74ae56","6e0ac7d933ac43b7995e804421c71409","2aac18cf00b84dccbe61354ac8aafca6"]},"executionInfo":{"elapsed":28614,"status":"ok","timestamp":1651161748608,"user":{"displayName":"Cao Duy Hoang","userId":"17498028471290040726"},"user_tz":-420},"id":"bnZl-7Vp6F1I","outputId":"559ad3e4-e165-43bf-9ce2-81f74124c018"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/26 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7a21ece00640a7a217f121b61af8d0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluate data in 27.74 seconds!\n","[tester] \n","Seq2SeqSpanMetric: quad_f=52.790000000000006, quad_rec=53.56999999999999, quad_pre=52.019999999999996, em=0.4453, invalid=0.0078\n"]}],"source":["tester = Tester(test_bundle.get_dataset('dev'), model, metrics=metric,batch_size=batch_size)\n","eval_results = tester.test()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1063,"status":"ok","timestamp":1650368782402,"user":{"displayName":"Duy Hoang","userId":"17457647490907439684"},"user_tz":-420},"id":"aFqm4tqYogvO","outputId":"bc9b8a07-1de3-46bf-fb9e-20908338aa9e"},"outputs":[{"data":{"text/plain":["+---------------+---------------+---------------+----------------+-----------------+----------------+-------------+-------------+\n","| raw_words     | aspects       | opinions      | tgt_tokens     | target_span     | src_tokens     | src_seq_len | tgt_seq_len |\n","+---------------+---------------+---------------+----------------+-----------------+----------------+-------------+-------------+\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 23          | 16          |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 22... | [0, 50265, ... | 12          | 16          |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 9           | 9           |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 26          | 9           |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 19          | 9           |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 14          | 9           |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 10          | 9           |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 19          | 9           |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 11          | 16          |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 17          | 9           |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 5           | 9           |\n","| ['<<null>>... | [{'index':... | [{'index':... | [0, 38, 38,... | [(38, 38, 15... | [0, 50265, ... | 18          | 9           |\n","| ...           | ...           | ...           | ...            | ...             | ...            | ...         | ...         |\n","+---------------+---------------+---------------+----------------+-----------------+----------------+-------------+-------------+"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["tester_dev = test_bundle.get_dataset('dev')\n","tester_dev"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1JsqrN0ooL5"},"outputs":[],"source":["tester_dev[0][\"aspects\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qDjqGVooSWb"},"outputs":[],"source":["amount = 0\n","for i in range(0,len(tester_dev)):\n","    amount+=len(tester_dev[i][\"opinions\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1650368663399,"user":{"displayName":"Duy Hoang","userId":"17457647490907439684"},"user_tz":-420},"id":"HgX4mUyhow_d","outputId":"18a1ab12-fe6b-40f3-876f-5a9fc0c0f7a8"},"outputs":[{"data":{"text/plain":["169"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["amount"]},{"cell_type":"markdown","metadata":{"id":"w132u__HLK5E"},"source":["#Test use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m61ToHByLKP0"},"outputs":[],"source":["import numpy as np\n","def tokenize_sentence(sentences,tokenizer,device = \"cpu\"):\n","    output_mold = []\n","    len_lst = []\n","    for i in range (0,len(sentences)):\n","        added_sentence = \"<<null>> \"+sentences[i]\n","        raw_words = added_sentence.split(\" \")\n","        word_bpes = [[tokenizer.bos_token_id]]\n","        for word in raw_words:\n","            bpes = tokenizer.tokenize(word, add_prefix_space=True)\n","            bpes = tokenizer.convert_tokens_to_ids(bpes)\n","            word_bpes.append(bpes)\n","        word_bpes.append([tokenizer.eos_token_id])\n","        output = list(chain(*word_bpes))\n","        output_mold.append(output)\n","    max_len = max(len(x) for x in output_mold)\n","    mold_np = np.ones([len(sentences),max_len])\n","    for i in range (0,len(sentences)):\n","        raw_words = output_mold[i]\n","        len_lst.append(len(raw_words))\n","        mold_np[i,:len_lst[-1]]=raw_words\n","    seg_token = torch.LongTensor(mold_np).to(device)\n","    seg_token_len = torch.LongTensor(len_lst).to(device)\n","    return seg_token , seg_token_len\n","\n","sentences = [\"\"]\n","seg_token, seg_token_len = tokenize_sentence(sentences,tokenizer,\"cuda\")\n","model.train(mode=False)\n","model.to(\"cuda\")   \n","res = model.predict(seg_token, seg_token_len)[\"pred\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-heP7xgNSQ_"},"outputs":[],"source":["def translateResult(sentences,results,idtarget2map,tokenizer):\n","    converted_sentences=[]\n","    for i in sentences:\n","        lst = []\n","        for word in i.split(\" \"):\n","            bpes = tokenizer.tokenize(word, add_prefix_space=True)\n","            lst.extend(bpes)\n","        converted_sentences.append(\" \".join(lst))\n","    output_mold = []\n","    len_lst = []\n","    cap = len(idtarget2map)+2\n","    def translateResultChunk(block,sentence):\n","        output = []\n","        prev = None\n","        lst = sentence.split(\" \")\n","        for i in block:\n","            if i<cap:\n","                output.append(idtarget2map[i-2])\n","                continue\n","            if prev is not None:\n","                chunk = lst[prev:i-cap]\n","                text = \"\"\n","                for i in chunk:\n","                    if i[0]==\"Ġ\":\n","                        text =text +\" \"+i[1:]\n","                    else:\n","                        text =text +i\n","                output.append(text.strip())\n","                prev = None\n","            else:\n","                prev = i-cap-1\n","        return output\n","    for i in range(0,len(converted_sentences)):\n","        blocks=[]\n","        count=0\n","        cur_pair = []\n","        for o in results[i]:\n","            k = int(o)\n","            if k==0 or k==1:\n","                continue\n","            \n","            if k<cap:\n","                cur_pair.append(k)\n","\n","                if count<2:\n","                    count+=1\n","                else:\n","                    if not(len(cur_pair) != 7 or cur_pair[0] > cur_pair[1] or cur_pair[4] > cur_pair[5]):\n","                        blocks.append(translateResultChunk(cur_pair,\"NULL \"+converted_sentences[i]).copy())\n","                    cur_pair = []\n","                    count=0\n","            else:\n","                cur_pair.append(k)\n","        output_mold.append(blocks.copy())\n","    return output_mold"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1654524372022,"user":{"displayName":"Duy Hoang","userId":"17457647490907439684"},"user_tz":-420},"id":"UbHuOz8kUMwK","outputId":"6ec9e02d-9ee6-4e88-a432-3565bf460e48"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[['NULL', 'LAPTOP', 'GENERAL', 'broken', 'NEG']]]"]},"metadata":{},"execution_count":29}],"source":["translateResult(sentences,res,idtarget2map,tokenizer)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["MQwocvOY5EEi","BWeNjY13jF5C","dCtAWimziVXR","HcuQh7sVUPq5","VEWncIejgMQ9","dVLiv9MiD_70","PE8Y8i_gt9rg","Zd3VqYYYuLc3","cS81fe29pPeQ","0pJJk3AIpTK-","g76Vh4BDqFNb"],"name":"Transformer_sentiment_attention_t5.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c20a73fba3b24a43be44f2850fab7649":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6457a12f852c44b08f3036602c63f16a","IPY_MODEL_4136225689e5409884824fc429f5e7f3","IPY_MODEL_9fb40918075e4e348a58d7085a6a63a8"],"layout":"IPY_MODEL_6b36e5eca83c4a2690db9e6b6f2957de"}},"6457a12f852c44b08f3036602c63f16a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0717b9587d6147ff92c5ca6f8c76b587","placeholder":"​","style":"IPY_MODEL_ff6ef87977f14bf2b304e4aee56cf23b","value":"Pre. tgt. for `test`: 100%"}},"4136225689e5409884824fc429f5e7f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_744b54834b054ee59f87efa48fb41125","max":64,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78db39eb97f74af5975905b9d1eb73ae","value":64}},"9fb40918075e4e348a58d7085a6a63a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76966a47b81b4ed5a3776813ab08245b","placeholder":"​","style":"IPY_MODEL_dfd20a5297a3459187b6ac3a748d213a","value":" 64/64 [00:00&lt;00:00, 238.01it/s]"}},"6b36e5eca83c4a2690db9e6b6f2957de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"0717b9587d6147ff92c5ca6f8c76b587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff6ef87977f14bf2b304e4aee56cf23b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"744b54834b054ee59f87efa48fb41125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78db39eb97f74af5975905b9d1eb73ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"76966a47b81b4ed5a3776813ab08245b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfd20a5297a3459187b6ac3a748d213a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41f4e6d4b55d4f90b0e66789020d5d7d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5bcb4ae6fb214f0fa10caca0fa511e62","IPY_MODEL_b597d114bbfe46e3afac52e69f7c882c","IPY_MODEL_d4b88231b8e44643a59cf46d02d183a7"],"layout":"IPY_MODEL_4a3bd9546645452ca31b0c123919ed1c"}},"5bcb4ae6fb214f0fa10caca0fa511e62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_209cd6349b91451290ad043ec3cb0569","placeholder":"​","style":"IPY_MODEL_98930f6252d940459f34ff8d24f4e292","value":"Pre. tgt. for `train`:  97%"}},"b597d114bbfe46e3afac52e69f7c882c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d6a7f3c2bd842c0a4a19a55342a519b","max":467,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96edc6f0003741d39fa4a2e0caeb0526","value":467}},"d4b88231b8e44643a59cf46d02d183a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e8d2faa05754e929dc6cb255e53e16e","placeholder":"​","style":"IPY_MODEL_44f4b531a0f044d1aa15a97c5b202a8d","value":" 455/467 [00:02&lt;00:00, 210.13it/s]"}},"4a3bd9546645452ca31b0c123919ed1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"209cd6349b91451290ad043ec3cb0569":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98930f6252d940459f34ff8d24f4e292":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d6a7f3c2bd842c0a4a19a55342a519b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96edc6f0003741d39fa4a2e0caeb0526":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e8d2faa05754e929dc6cb255e53e16e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f4b531a0f044d1aa15a97c5b202a8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba5b6487ccc24e59b84f97e48265f8e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8af9caf917a441549b296fe08f4f0cff","IPY_MODEL_a379af210f3443bc8bf16d9c6b9c441f","IPY_MODEL_4fb8ff5a7fd047c581cb979b19c8bde2"],"layout":"IPY_MODEL_e5c3650bc35d43f6a5ae442aada70996"}},"8af9caf917a441549b296fe08f4f0cff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecb179a2fbf548759c1cb675f99654f6","placeholder":"​","style":"IPY_MODEL_1473c3dc47104a2e9c1d60a225ded991","value":"Test: 100%"}},"a379af210f3443bc8bf16d9c6b9c441f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a952c78440445ddb3a045287b8d94d5","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e05f09c5fd4549d099ed8c873da407b4","value":13}},"4fb8ff5a7fd047c581cb979b19c8bde2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af5e3130f0024bfd93bf9dc105fc61a5","placeholder":"​","style":"IPY_MODEL_5ada919ba49f4affac3a46037800edb9","value":" 13/13 [00:16&lt;00:00,  1.07it/s]"}},"e5c3650bc35d43f6a5ae442aada70996":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"ecb179a2fbf548759c1cb675f99654f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1473c3dc47104a2e9c1d60a225ded991":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a952c78440445ddb3a045287b8d94d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e05f09c5fd4549d099ed8c873da407b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af5e3130f0024bfd93bf9dc105fc61a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ada919ba49f4affac3a46037800edb9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f00d63532aa47a2bbfd3175eb6289c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d83758beab174ae6b242dfab4f5d4e58","IPY_MODEL_d18a0849c9834eee8931fc8ab566bb0a","IPY_MODEL_1e9c6ea2d3d246b4b4f343d18f241671"],"layout":"IPY_MODEL_5d04210ff8bb4a3088e624059b5fa43c"}},"d83758beab174ae6b242dfab4f5d4e58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e6fc2321fa7453598f2816a27976d5b","placeholder":"​","style":"IPY_MODEL_1fe4a50c9b02422d945ffa0095414cec","value":"Test: 100%"}},"d18a0849c9834eee8931fc8ab566bb0a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a5d01d81680469b91057465c7921a67","max":94,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ef6d1941d9944ef9270e438c4f48ac6","value":94}},"1e9c6ea2d3d246b4b4f343d18f241671":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c79d553517941dcb4b54ca31c558ea2","placeholder":"​","style":"IPY_MODEL_cb6dfa452eeb4551a117bf9bf3a16a57","value":" 94/94 [01:54&lt;00:00,  1.03it/s]"}},"5d04210ff8bb4a3088e624059b5fa43c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"1e6fc2321fa7453598f2816a27976d5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fe4a50c9b02422d945ffa0095414cec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a5d01d81680469b91057465c7921a67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ef6d1941d9944ef9270e438c4f48ac6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c79d553517941dcb4b54ca31c558ea2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb6dfa452eeb4551a117bf9bf3a16a57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d7a21ece00640a7a217f121b61af8d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67e1c05200524ce0be5d0269ebc41de8","IPY_MODEL_65d19bd7586c4169896f1482400f4351","IPY_MODEL_11569ca5ca8c4e7d989151571b182faa"],"layout":"IPY_MODEL_42625e8e3bf0424e9bba5e906740d5fd"}},"67e1c05200524ce0be5d0269ebc41de8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_619ef086d9b245c5bf476af4fa1068d2","placeholder":"​","style":"IPY_MODEL_41787032210a4beeb7c7ed3e03190c63","value":"Test: 100%"}},"65d19bd7586c4169896f1482400f4351":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a555f9ce52204e1eba737cccda0c0216","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b26e52a7360e447fa3a9b302fd74ae56","value":26}},"11569ca5ca8c4e7d989151571b182faa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e0ac7d933ac43b7995e804421c71409","placeholder":"​","style":"IPY_MODEL_2aac18cf00b84dccbe61354ac8aafca6","value":" 26/26 [00:27&lt;00:00,  1.03it/s]"}},"42625e8e3bf0424e9bba5e906740d5fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"619ef086d9b245c5bf476af4fa1068d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41787032210a4beeb7c7ed3e03190c63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a555f9ce52204e1eba737cccda0c0216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b26e52a7360e447fa3a9b302fd74ae56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e0ac7d933ac43b7995e804421c71409":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aac18cf00b84dccbe61354ac8aafca6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85873f08bfea4900ba4fcba519f182b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff16507299e84d8298c6060afbe56b3d","IPY_MODEL_d8e7882f57dd41f7beb6e17a44741d7b","IPY_MODEL_1660b201b9d94ca2b11c5bb23ba79466"],"layout":"IPY_MODEL_dc98ba02df374ad1b7a0cb45db6c5b86"}},"ff16507299e84d8298c6060afbe56b3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e4c3d1864514bb697be62524962cc6a","placeholder":"​","style":"IPY_MODEL_3607437e511843b5aecff7b8697ee530","value":"Test: 100%"}},"d8e7882f57dd41f7beb6e17a44741d7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c820ff2556469796f2f47dce677537","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62b39d95f4924e67805d82407c029134","value":164}},"1660b201b9d94ca2b11c5bb23ba79466":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e25b8f5ce6944de9a38d44a10001b811","placeholder":"​","style":"IPY_MODEL_72982d228826466bbb979f8ff40afc11","value":" 164/164 [03:26&lt;00:00,  1.02s/it]"}},"dc98ba02df374ad1b7a0cb45db6c5b86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"5e4c3d1864514bb697be62524962cc6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3607437e511843b5aecff7b8697ee530":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37c820ff2556469796f2f47dce677537":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62b39d95f4924e67805d82407c029134":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e25b8f5ce6944de9a38d44a10001b811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72982d228826466bbb979f8ff40afc11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a16d15e65034125abe16dbc35931f2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff4c617ea47048daa597ee6a867fdff7","IPY_MODEL_09b4a53201b240cbb7a604cfa4846465","IPY_MODEL_0fe6665e4328452ea5d210dcbb1a167c"],"layout":"IPY_MODEL_a3ea6eb801aa43a8a0e8cf7b392b5d64"}},"ff4c617ea47048daa597ee6a867fdff7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb29e58344c6433e9ff7d3d7aa8f1781","placeholder":"​","style":"IPY_MODEL_40f659454b8041d5815a2f74c9993f88","value":"Epoch 1/50:   2%"}},"09b4a53201b240cbb7a604cfa4846465":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_8085cc95d9fb40ce8a568ea4e2044333","max":36700,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d326b8c28d10484ba7f8ad8b40fc99a0","value":734}},"0fe6665e4328452ea5d210dcbb1a167c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07ba7233c4164972b9066365cb4c9beb","placeholder":"​","style":"IPY_MODEL_a990ebb72b2547c7acf859f50dc55f54","value":" 734/36700 [02:32&lt;1:41:48,  5.89it/s, loss:1.63635]"}},"a3ea6eb801aa43a8a0e8cf7b392b5d64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"eb29e58344c6433e9ff7d3d7aa8f1781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40f659454b8041d5815a2f74c9993f88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8085cc95d9fb40ce8a568ea4e2044333":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d326b8c28d10484ba7f8ad8b40fc99a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07ba7233c4164972b9066365cb4c9beb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a990ebb72b2547c7acf859f50dc55f54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}